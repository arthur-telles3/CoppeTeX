%%
%% This is file `example.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% coppe.dtx  (with options: `example')
%% 
%% This is a sample monograph which illustrates the use of `coppe' document
%% class and `coppe-unsrt' BibTeX style.
%% 
%% \CheckSum{1613}
%% \CharacterTable
%%  {Upper-case    \A\B\C\D\E\F\G\H\I\J\K\L\M\N\O\P\Q\R\S\T\U\V\W\X\Y\Z
%%   Lower-case    \a\b\c\d\e\f\g\h\i\j\k\l\m\n\o\p\q\r\s\t\u\v\w\x\y\z
%%   Digits        \0\1\2\3\4\5\6\7\8\9
%%   Exclamation   \!     Double quote  \"     Hash (number) \#
%%   Dollar        \$     Percent       \%     Ampersand     \&
%%   Acute accent  \'     Left paren    \(     Right paren   \)
%%   Asterisk      \*     Plus          \+     Comma         \,
%%   Minus         \-     Point         \.     Solidus       \/
%%   Colon         \:     Semicolon     \;     Less than     \<
%%   Equals        \=     Greater than  \>     Question mark \?
%%   Commercial at \@     Left bracket  \[     Backslash     \\
%%   Right bracket \]     Circumflex    \^     Underscore    \_
%%   Grave accent  \`     Left brace    \{     Vertical bar  \|
%%   Right brace   \}     Tilde         \~}
%%
% useful link: https://apgita.org.br/academico/teses-e-latex/
\documentclass[msc,numbers,english]{coppe} % maybe mscexam?
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage[table]{xcolor}
\usepackage{array}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{algorithm}
\usepackage{pythonhighlight}
\usepackage[noend]{algpseudocode} 
\usepackage{multirow}
\usepackage{nicefrac}
\usepackage{multicol}
\usepackage{dirtytalk}
\usepackage{float}
\newcolumntype{P}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}
\interfootnotelinepenalty=10000

\makelosymbols
\makeloabbreviations

\begin{document}
% \title{Tradução automática através de redes neurais em domínios de baixo recurso: uma análise ponta a ponta do caso do português brasileiro}
% \foreigntitle{TACKLING LOW-RESOURCE NEURAL MACHINE TRANSLATION: ANEND-TO-END ANALYSIS FOR A BRAZILIAN PORTUGUESE CASE STUDY}
\title{Análise da tradução automática neural em domínios de baixo recurso através da mineração de dados: um caso do português brasileiro}
\foreigntitle{A data mining approach to analyse neural machine translation under low-resource settings: a Portuguese case study}
\author{Arthur}{Telles Estrella}
\advisor{Prof.}{João}{Baptista de Oliveira e Souza Filho}{D.Sc.}
% \advisor{Prof.}{Nome do Segundo Orientador}{Sobrenome}{Ph.D.}
% \advisor{Prof.}{Nome do Terceiro Orientador}{Sobrenome}{D.Sc.}

\examiner{Prof.}{Nome do Primeiro Examinador Sobrenome}{D.Sc.}
\examiner{Prof.}{Nome do Segundo Examinador Sobrenome}{Ph.D.}
\examiner{Prof.}{Nome do Terceiro Examinador Sobrenome}{D.Sc.}
\examiner{Prof.}{Nome do Quarto Examinador Sobrenome}{Ph.D.}
\examiner{Prof.}{Nome do Quinto Examinador Sobrenome}{Ph.D.}
\department{PEE}
\date{02}{2021}

\keyword{Primeira palavra-chave}
\keyword{Segunda palavra-chave}
\keyword{Terceira palavra-chave}

\maketitle

\frontmatter
\dedication{A algu\'em cujo valor \'e digno desta dedicat\'oria.}

\chapter*{Agradecimentos}



\begin{abstract}

Apresenta-se nesta dissertação uma abordagem qualitativa inspirada em data mining para analisar os resultados de um modelo de tradução automática neural, submetido a baixa disponibilidade de dados e treinado em uma única GPU. O estudo foca especificamente no par português-inglês. BLEU é usado com métrica referência para avaliar técnicas como subword embeddings, pre-trained word embeddings e data augmentation através de back translation. Essas soluções podem potencialmente endereçar alguns dos desafios encontrados em condições experimentais de baixo recurso, mas o viés que inserem na qualidade da tradução ainda é pouco compreensível. A análise qualitativa 
é conduzida em frases de diferentes níveis de complexidade para entender tais efeitos. Todas as variantes do Transformer serão discutidas em relação a um referencial (Google Translate), focando em delimitar o equilíbrio entre eficiência de recursos e performance em cada caso. O melhor modelo de baixo recurso desenvolvido é capaz de atingir 40.26 BLEU, 77.1\% do desempenhado pelo Google Translate (em janeiro de 2022).

\end{abstract}

\begin{foreignabstract}

In this work, a data mining qualitative approach is taken to analyse the results of a neural machine translation model, subjected to low data availability and using a single GPU. The study focuses specifically on the Portuguese-English pair. BLEU is used as a benchmark metric to evaluate techniques such as pre-trained word embeddings, subword embeddings, and data augmentation via back translation. These solutions can potentially address some issues faced in low-resource experimental settings, but their impact on biasing the quality of translations is unclear. The qualitative analysis is conducted in sentences with a complexity drill down to understand such effects. All the proposed Transformer-based variants are discussed in comparison to a benchmark model (Google Translate), focusing on explaining the balance between resource efficiency and performance in every case. The best model built is capable of reaching 40.26 BLEU, 77.1\% of what the Google Translate benchmark achieved (in january 2022).

\end{foreignabstract}

\tableofcontents
\listoffigures
\listofalgorithms
\listoftables
% --- UNDERSTAND HOW TO USE THIS ---
\printlosymbols
\printloabbreviations
% \listabbreviationname
% ----------------------------------
\mainmatter
\chapter{Introduction}
\section{The reenactment of Machine Translation}

Machine translation \abbrev{Machine Translation}{MT} is a research field that until 2013 has mainly invested in statistical based models, but the breakthrough promoted by sequence to sequence algorithms followed by the use of Transformer models has significantly changed the focus of the field. Before neural networks, Machine Translation systems were rules-based, syntax-based, phrase-based or a blend between more than one of these techniques. Probabilistic models were considered state of the art before the first sequence to sequence paper appeared. The increase in performance promoted by the sequence to sequence and Transformers quickly received some attention, and soon other variants were developed.

Despite being constrained by computational power in many stages since its beginning, one of the most relevant contributions to the translation task were the Transformers, which made possible to perform the computation in a truly parallel schema. With this new architecture, the operations performed during training are not totally dependent, allowing them to become parallelizable in the GPU. By removing the constraint of some operations having to wait for others to finish, NMT models were enabled to scale and reach even higher quality translations. \abbrev{NMT}{Neural Machine Translation}

In 3 years, NMT became the dominant approach to Machine Translation, inducing a major transition from statistical to neural models. Neural models already had a broad set of parameters and architectures present in the neural networks literature, soon researchers started to explore these possibilities seeking to boost translation quality and become the state-of-the-art benchmark.

\section{Challenges for the Portuguese language}
\label{portuguese_challenges}

Traditionally, the Machine Translation datasets and conferences usually focus on a subset of languages from countries that are actively investing on NLP, which biases and narrows the potential that the algorithms have towards a specific domain. Unfortunately, Portuguese is a language that does not ostentate supervised translation data in diversity and quantity, an issue that increases the struggle to build a model that can successfully translate it to other languages. Another obstacle is that Portuguese has european, brazilian and african variants, this provides a challenge for a model since generalization is harder if several sentences with different dialects can have the same meaning.

The branch of NMT inside natural language processing is also a field with few papers and academic works among Brazilian universities, this can be partially explained by the challenge that this environment presents: most models require cutting edge GPUs and usually only one GPU is not enough for medium sized model on an average WMT competition dataset. The scarcity of these resources for research purposes require students to innovate in a limited domain and search for cloud solutions without sponsorship.

Finally, Portuguese is a complex language that uses accents which can change their meaning (i.e. "e" and "é"), has different pronoun organizations (i.e. "realizar-se-á" equals "se realizará") and irregular verb inflections (i.e. the "pôr" and "haver" verbs) so text preprocessing and tokenization plays an important role. Disregarding these details by applying some generic preprocessing steps that eliminates accents for instance can lead to worse model performance. On the other hand, having some domain knowledge and apply this to the NLP pipeline can help the model better translate or classify, depending on the desired task.

\section{Contributions of this dissertation}

Many previous contributions for solving the NMT task in the community are focused on surpassing the state-of-the-art, disregarding the associated computational burden for achieving this goal. Research under this scope is usually driven by increases in scores such as BLEU, that have limitations in defining the quality of the translations produced. This dissertation diverges from that approach, in the sense that the focus is to generate ways of understanding quality biases and being able to associate them with the use of NLP techniques, while considering resource efficiency under low-resource settings.

The motivation for the concern regarding low-resource arises from the reality of most researchers: being able to develop a competitive translation model without a robust infrastructure is becoming increasingly hard over time. As the access to such machinery is restricted to selected group of researchers, the community is investing on accessible AI \abbrev{AI}{Artificial Intelligence}, thus one priority is that the experiments performed here could be reproduced using low-cost GPUs.

% In a scenario where performance is a matter of ultimate importance, the quality issues derived from model guesses and how specific techniques help to address some quality aspects are overlooked. Also, many researchers cannot take advantage of cutting-edge infrastructure or abundant data, so understanding efficient approaches that maintain a reasonable cost-benefit balance is key to succeed.

As a result of the effort put on defining ways to assess quality, one product of this dissertation can be found at \citet{portuguese-nmt}, an article presented at Symposium in Information and Human Language Technology (STIL)\footnote{The full paper is publicly available at \url{https://sol.sbc.org.br/index.php/stil/article/view/17807}.}. It started the exploration of resource efficient Natural Language Processing (NLP) \abbrev{NLP}{Natural Language Processing} techniques that can leverage translation performance and quality under resource constrained environments and analyses how they influence translation error bias. 

In this work, Transformers \cite{DBLP:journals/corr/VaswaniSPUJGKP17} are used to experimentally evaluate to what extent techniques such as transfer learning, modelling subsets of words (subword embedding), and augmenting a dataset with artificial sentences (back translation) can help a NMT model excel in a low-resource environment. The study considers only one average size GPU and small to medium-sized datasets, focusing on the English-to-Portuguese pair. In a quantitative sense, the challenge of low-resource is evaluated using a range of dataset sizes. The goal is to understand how it influences performance and the effectiveness of the aforementioned techniques under such conditions. Regarding the qualitative analysis, clusters of errors are defined with the help of a Brazilian English translator, that also helps performing a complexity drill down. Sentence complexity and error patterns are subjected to 3 experiments that reveal the multidimensional associations between model variants and these factors, where we conclude that the techniques associated with the model indeed have an influence over error biases.

For comparison purposes, the model with the most promising score is compared with the output of Google Translate generated over the test set of the main dataset used. Despite the unknown amount of resources required to train the external algorithm, we show that a low-resource model reaches up to 77.1\% of its BLEU score.

This work is not only an attempt to explain qualitatively and quantitatively the pros and cons of applying a specific technique in low-resource environments. We also hope to stimulate the community to dive deeper into error biases of specific techniques in NMT, as our belief is that this process may guide the community towards producing clearer, more fluid and grammatically correct translations.

\section{Chapter Organization}

Fundamentals of NLP that are the pillars for NMT are presented in chapter 2. It explains different types of word representations, how the task of Machine Translation works and its main challenges. Chapter 3 introduces 2 Sequence to Sequence architectures: the Transformer and Recurrent Neural Network (RNN) \abbrev{RNN}{Recurrent Neural Network}, along with some interesting properties that helped them to learn richer word representations and output more coherent translations.

In chapter 4 aspects of low-resource domains and its contraints are outlined, along with a review of NLP techniques that can potentially help to reduce those issues. A description of the datasets used in this work is provided in chapter 5, which also depicts quantitative and qualitative experiments and their results for the strategies considered. Finally, in chapter 6 this work is concluded and further improvements and study directions are outlined. 

\chapter{A brief introduction to Natural Language Processing}

Natural language was considered by the Machine Learning community as one of the most complex data types to represent. Practical applications with meaningful feature representations took time to evolve, even longer than computer vision related tasks, which also require customized representations. In the sections below, a brief introduction to different kinds of features used for word representations by will be given to contextualize the reader of some of the historical challenges and evolution of this field.

\section{Feature representation}

In order to keep the unique and original meaning of each word, one of the oldest and most popular ways to represent natural language is through one-hot encoding. Let $V$ be the number of distinct words presented in a vocabulary, when this set of words gets converted into features, each word will be represented by an unique dimension inside a binary vector of size $V$. That means that when representing only one word, the vector contains 1 for the dimension of that word and all the other positions are filled with zeroes. Since two different words would be represented by 2 equal sized vectors containing 1 for distinct positions and 0 for the others, these vectors are considered to be orthogonal. Being orthogonal means that their dot product is 0, hence having no measure of similarity between them. One-hot encoding is also referred to as a sparse representation, as it has only one position greater than zero. In addition, it generates a high number of dimensions to represent the vocabulary, which may provide limitations for algorithms that cannot properly handle high dimensional inputs.

Another possibility to represent a set of input words is using fixed-length real vectors, where the number of dimensions is predetermined, regardless of the size of the vocabulary, and the vector is not binary anymore. Such real numbers may vary in range depending on how there are generated, and there are a number of techniques that can provide them. For instance, they can be randomly initialized using uniform distributions and then adjusted using an optimization algorithm based on word co-occurrence statistics. In neural algorithms, these real number vectors are usually extracted from the weight matrix of a Neural Network, thus can be referred to as weight vectors or neural embedding. This word encoding strategy that doesn't assign unique ids to words is known as dense representation.

There are pros and cons for using either sparse or dense vectors to represent words, and a summary of them can be found on Table \ref{feature-representations}.

\begin{table}[h]
\caption{Pros and Cons per Feature Representation}
\fontsize{10}{12}\fontfamily{phv}\selectfont
\label{feature-representations}
\centering
  {\footnotesize
  \begin{tabular}{| p{0.15\linewidth} | P{0.12\linewidth} | P{0.09\linewidth} | P{0.18\linewidth} | P{0.32\linewidth} |}
    \hline
    Feature representation & Encoding Type & Dimensionality & Information sharing & Curse of dimensionality \\
    \hline
    One-hot encoding & Sparse & High & None, as dimensions are indepedent & Sparse and high dimensional vectors may hinder model performance \\
    \hline
    Neural embedding & Dense & Fixed-Length & Similar vectors for related words & Dimensionality can be experimentally tuned to a sweet spot \\
    % \hline
    % BT (50\% of News synthetic examples) & 34 & 33 & 21.80 & 51.34\\
    \hline
\end{tabular}}
\end{table}
  
One important aspect that also distinguishes them is that if the dimension is too high for one-hot encoding, dimensionality reduction may be required, and depending on the dataset and algorithms involved, that may be contraindicated due to computational requirements and potential quality issues that may arise. On the other hand, weight encoding lets you tune the dimension, but several experiments are required to achieve the dimension that yields optimal performance, which also depends on picking the right algorithm to solve the proposed task. 
  

\section{Neural word embeddings}
\label{neural_embedding}

Natural language representation using neural word embeddings was responsible to scale Machine Learning algorithms to a range of applications, therefore they compose a watershed for the research community. Neural networks can also be recognized as intrinsic dense vector learners due to the learning process of forward and backpropagation.

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.4]{chapter2_images/word2vec_didatic.png}
\caption{Architecture diagram for calculating word probabilities using a 2-layer Neural Network.}
\label{word2vec_didatic_architecture}
\end{figure*}

The properties of each feature representation cited in the previously shown Table \ref{feature-representations} are clear, but one question remains: How dense vectors are able to capture similarity when representing the meaning of words? One quote by John Firth from 1952 may shed light on the potential answer:\newline

\say{You shall know a word by the company it keeps.} \newline

Basically, the meaning of a word can be defined by the context around it, and usually this context is extracted using a fixed-size window, whose size is a hyperparameter of the algorithm $c$. To gradually understand how words relate to their context, lets first consider the association between 2 given words in a text.

\subsection{Single-word context}

One popular technique to obtain word weights for instance is the two-layer neural network architecture illustrated at Figure \ref{word2vec_didatic_architecture}. It is a simplified representation of a neural word embedding, which takes an one-hot encoded vector of dimension $v$ as input and outputs a dense vector. In the one-hot encoded vector, only one out of $v$ elements will be 1, and all the others will be 0. The weights between the input vector and the output layer can be represented by a $v*n$ matrix $W$, where $n$ is the hidden layer size. In $W$, each row is a n-dimension vector representation $\textbf{v}_{w}$ of the respective word in the input. In practice, row $\textbf{x}$ of $W$ corresponds to $\textbf{v}^{T}_{w}$. This enables the hidden layer to become analogous to a lookup table, where each row represents a different word.

In Figure \ref{word2vec_didatic_architecture}, the hidden layer is represented by $\textbf{h}=W^{T}\textbf{x}$. From the hidden layer to the output layer, there is another weight matrix $W^{'}=\textbf{w}^{'}_{Cj}$, which is a $n \times v$ matrix. Weights from both matrices can be used to compute scores $\textbf{u}_{j^{'}}$ and $\textbf{u}_{j}$ for a given word:

\begin{equation}
\textbf{u}_{j^{'}} = \textbf{v}^{'T}_{w_{j}}h
\label{input_hidden}
\end{equation}
and
\begin{equation}
\textbf{u}_{j} = \textbf{v}^{T}_{w_{j}}h,
\label{hidden_output}
\end{equation}
where $\textbf{v}^{'T}_{w_{j}}$ and $\textbf{v}^{T}_{w_{j}}$ are the j-th columns of the matrix $W^{'}$ and $W$, respectively. Consider $o$ fixed for the purpose of evaluating only one word association. The score of \ref{hidden_output} is then passed through a softmax layer, that calculates the probability of a set of words that could be fit into that context.

The output layer is responsible for determining the association of a word with its context, or vice versa, depending on the approach used. Considering the calculation of the probability of a single word, the softmax output then becomes the result of \ref{softmax_output} 

\begin{equation}
\label{softmax_output}
p(w_{o}|w_{i}) = \frac{exp(\textbf{u}_{j})}{\sum^{v}_{j^{'}=1}exp(\textbf{u}_{j^{'}})},
\end{equation}
that is connected to the vectors $\textbf{u}_{j^{'}}$ and $\textbf{u}_{j}$ through the equations \ref{input_hidden} and \ref{hidden_output}, respectively. In such equations, $\textbf{v}_{w}$ is the weight representation between the input and hidden layer and $\textbf{v}^{'}_{w_{i}}$ represents the vector between the hidden and the output layer.

\subsection{Multi-word context}

Examples of windows with size 2 and 3 are shown at Figure \ref{word_window}. Consider that the central word is immersed in a context of $C$ other words, where $c=\frac{C}{2}$. As the output word $o$ lies within the target word's context, it can also be denoted by $i+o$, with $o$ being the number of positions ahead or behind the input inside the text and $i$ is the position of the center word. 

\begin{figure*}[b]
\centering 
\includegraphics[keepaspectratio,scale=0.6]{chapter2_images/word_window.png}
\caption{Word context window of sizes 2 and 3}
\label{word_window}
\end{figure*}

The weights of the word embedding are randomly initialized and updated by an operation based on interactions with nearby words, which is calculated through an optimization algorithm. Such interactions with nearby words are pairwise associations, where the goal is to calculate the likelihood that both words will co-occur. The model's task is to calculate the probability of the surrounding word given the target word $p(w_{i+o}|w_{i})$. Once all the words in the context of a center word are defined by a fixed window, the goal is to maximize the likelihood of the context words given the center word. This basically means that the probability of that context word will be maximized through Equation \ref{maximize_likelihood}: 

\begin{equation}
\label{maximize_likelihood}
L(\theta) = \prod^{C}_{i=1} \prod_{\substack{-c \leq o \leq c \\ o \neq 0}} P(w_{i+o} | w_{i}).
\end{equation}

The same equations previously presented also apply for the multi-word case. The difference is that now \ref{input_hidden} and \ref{hidden_output} are calculated for each word, and equation \ref{softmax_output} is replicated for the whole sentence, thus being calculated C times for each sentence.

% words vary in -c<j<c
% units in vector very in 0<v<V
% https://medium.com/analytics-vidhya/maths-behind-word2vec-explained-38d74f32726b

\subsection{\mbox{Word2Vec}: CBOW and Skipgram}

The explanation so far constitutes a simplified version of the architecture proposed in the seminal work of \citet{mikolov2013efficient}, popularly known by the alias of \mbox{Word2Vec}. In the original proposal, it operates in a multi-word setting, which in turn has 2 variations in the original paper: Continous Bag of Words (CBOW) \abbrev{CBOW}{Continous Bag of Words}, which is in the left side of Figure \ref{cbow_skipgram}, and Skipgram, on the right. The main difference between them is that in CBOW, the task being performed is to predict the target word based in the context, while in Skipgram the context words are predicted based on the target word. A more detailed explanation will be provided for skipgram, since the embeddings in this work are based on it, and it has consolidated itself in the literature as a more effective approach for a wide range of experiments.

% One aspect of the CBOW variant model that changes when compared to the simplified architecture is that it takes the average of the vectors of the input context words, it takes the product of the input and hidden weight matrix, then calculates the average vector as the output.

% \textcolor{red}{Question: should I add the reference for the equations? Even if I changed the logical order and some of the variables used to present them?}
% its from word2vec parameter learning explained


Remember that our previous modelling approach constitutes of calculating the probability of the co-occurring the output word $o$ with the center word $i$, considering $-c \leq o \leq c$. In order to obtain the parameter update equations for this model, we need to maximize \ref{softmax_output}, the conditional probability of observing an output word $w_{o}$ given an input word $w_{i}$. The loss function $E$ that results from maximizing the correspondent set of words in skipgram is derived in the following equations


\begin{equation}
\label{skipgram_loss}
\begin{aligned}
E = & max -\log p (w_{i+c}, w_{i+c-1}, \dots, w_{i+1}|w_{i}) \\
& = -\log \prod^{C}_{o=1} \frac{exp(u_{cj^{*}_{c}})}{\sum^{V}_{j^{'}=1}exp(u_{j^{'}})} \\
& = -\sum^{C}_{o=1}u_{o^{*}_{c}} + C\times\log\sum^{V}_{j^{*}=1}\exp{u_{j^{'}}},
\end{aligned}
\end{equation}
where $o^{*}_{c}$ represents the index of the current c-th output context word in the vocabulary. This algorithm updates the word vectors for each element in the context, but the final representation of the word reflects its neighbors, since it is influenced by all of them during the optimization process. If interested in further details on how \mbox{Word2Vec} equations are derived, the reader is referred to \citet{DBLP:journals/corr/Rong14}. Considering the output word as $i+o$ for simplicity, the output of the softmax function is now reproduced by a similar equation than the one previously shown at \ref{softmax_output}, thus assuming the form

\begin{equation}
\label{skipgram_equation}
p(w_{i+o}|w_{i}) = \frac{exp(v^{'T}_{w_{j}}v_{w_{i}})}{\sum^{V}_{j^{'}=1}exp(v^{'T}_{w_{j^{'}}}v_{w_{i}})}
\end{equation}
for each word. This is the pure form of the Skipgram embedding. An interesting analogy that helps to grasp the way this multi-word mechanism works is to think that the previously mentioned logistic regression neurons are trying to distinguish what a "true context" word is of what can potentially be "noise", an unrelated word in the text.

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.35]{chapter2_images/cbow_skipgram.png}
\caption{Multi-word mechanisms CBOW and Skipgram}
\label{cbow_skipgram}
\end{figure*}

In addition to the pure form, there are also some optimization tricks applied to improve its scalability: hierarchical softmax and negative sampling. The hierarchical version of softmax replaces the expensive operation with an approximation of itself using binary trees. In such trees, the leaves represent words of that tree and the calculation of a word probability is decomposed into a sequence of probabaility calculations. This saves the algorithm from having to calculate the expensive normalization over all words.

Another feature inserted in the implementation is negative sampling. It is a way to sample only a subset of the words that need to be updated per iteration during optimization. The output word (the most probable word) is kept in the sample and gets updated, and a sample of some other words are used as negative samples, following a probabilistic distribution that can be arbitrarily chosen. This distribution is called noise distribution, and in order to derive a good sample from it, an empirical process is applied. The authors define a simplified training objective that is capable of producing high-quality embeddings in their original paper \cite{mikolov2013efficient}. In practice, negative sampling reduces the number of word samples to perform the update equations, based on a posterior multinomial distribution \cite{DBLP:journals/corr/Rong14}. These features were crucial to enable a huge number of practical applications using this technique, also attracting the attention of other Machine Learning branches. 
% https://ruder.io/word-embeddings-softmax/index.html#hierarchicalsoftmax

\section{Machine Translation and its challenges}
\label{machine_traslation_challenges}
% taken from chapter 10 of https://web.stanford.edu/~jurafsky/slp3/

Some aspects of the human language seem to be universal, or statistically universal, since we know only a subset of the languages that were created in the whole history of mankind in the current year of 2021. When speaking of language diversity, only widely spoken and recently created languages are considered. Many of these languages employ the use of verbs and nouns, words to refer to animals, emotions,  attitudes and other aspects. Yet, there are a number of differences between them. They can be idiosyncratic or systematic for instance. Idiosyncratic accounts for which words are usually combined and how common they sound together, since different languages are usually biased to distinct sets of words. Systematic represents the word order chosen, for example if the verb is usually put before the direct object or not. The study of systematic differences is called linguistic typology. These and other challenges imposed by language nuances discussed in this section are inspired by the Neural Machine Translation chapter of the unfinished book of \citet{nmt_nlp_speech_draft_book}, shared as a draft.

\subsection{Word order typology}

Languages differ in the way they order verbs, subjects and objects in simple declarative clauses. Some are considered SVO (subject-verb-object) \abbrev{SVO}{subject-verb-object}, their sentences start with the subject, the verb is put in the middle and ends with an object. Portuguese, Chinese and English are SVO languages, whereas German and Japanese are SOV (subject-object-verb) \abbrev{SOV}{subject-object-verb}. There are also VSO (verb-subject-object) \abbrev{SOV}{verb-subject-object} languages, such as Arabic and Irish.

One of the most challenging environments for a NMT model to execute the translation task happens when it has to translate from a language of one typology type to another, which luckily isn't the case in this work, as both languages are SVO.

\subsection{Morphological typology}

Regarding typology, languages can be categorized in 2 dimensions. The first is the number of morphemes per word: isolating languages like Vietnamese usually contain one morpheme (i.e. one enclosed meaning or reference to something) per word. They are simpler to deal with than polysynthetic languages, which may contain a variable number of morphemes per word. This classification isn't binary, though. Portuguese and English are somewhere in between both, but closer to isolating rather than polysynthetic. Both languages can add suffixes or prefixes that change the meaning: for instance happy is an adjective, but when united with "ness" becomes a noun "happiness".

The second dimension is to which degree the morphemes are segmentable. Languages can be agglutinative (clear boundaries between morphemes) such as Turkish; or fusional, such as Spanish; where the boundaries between morphemes are unclear or lost. One example for this is the verbal inflection "comí" (I ate), where the morphemes "comer" and "yo" have lost their boundaries between each other. Portuguese and English are considered to be fusional, but not as much as other fusional benchmarks such as Classical Hebrew.

\subsection{Word alignment}
\label{word_alignment}

Alignment in the context of translating means how word correspondences are made between the reference (source language) and its translation (target language). The aforementioned typological properties substantially increase the complexity of alignment, specially for situations where distinct typologies must be matched. 

Not all words in the source or target language necessarily need to be aligned, sometimes the role of one word in a translation pair is just to maintain fluency, contributing to a correct sentence in terms of grammar. Figure \ref{alignment_types} contains 3 alignment examples, the source language is on the left and the target is on top. From left to right, the first rectangle represents a simple one-to-one scenario, the second shows a many-to-many relation and the third, one-to-many, all of them using the Portuguese-English pair. Note that the last scenario has 3 spurious words in the source language which had no alignment with the target translation.

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.22]{chapter2_images/alignment_types.png}
\caption{One-to-one, many-to-many and one-to-many alignments using Portuguese, English and French sentences}
\label{alignment_types}
\end{figure*}

\subsection{Lexical divergences}

Sometimes, languages may contain a specific expression or word that cannot be accurately translated to another language without losing some of its meaning. Whenever this happens, the word constitutes a lexical gap between such languages. This is pretty common with slang words, which have a very specific use case, making it harder to express the same meaning in another language, even when using several words.

Another characteristic of languages that may increase the chance of happening a lexical gap is the fact that some of them are verb-framed. This means that they mark the path of motion in the verb and leave the satellites to express the manner of motion. The path of motion for instance means the direction of the movement, such as move into, out of or across, while the manner of motion refers to a specific type of motion, such as running, walking, or crawling. 

On the opposite side of verb-framed, there are languages that are satellite-framed, which means that the manner of motion is expressed in the verb while the path of motion is expressed in the satellite. As happened with other language characteristics, these classifications don't mean that a given language cannot mix verb-framed and satellite-framed expressions. English and Portuguese for instance can generate expressions using both styles. 

Consider the translation of language $A$ that mainly uses verb framing to language $B$, which mainly uses satellite framing. Having a difference in the framing pattern of both $A$ and $B$ languages does not necessarily mean that the chance of lexical gaps increases. It means that $A$ may have a range of specificities by aggregating verbs that may be complex to reproduce with a correspondent aggregation of satellites in $B$. This diversity of patterns to generate distinct meaning might pose a challenge to generate an accurate translation, and may result in a partial loss of the original meaning.

\subsection{Qualitative aspects to evaluate translations}

Defining qualitative and quantitative characteristics for accurately evaluating how successful a translation was is hard. The quantitative matter will be further discussed in another section, but the qualitative side is usually broken into 2 dimensions, namely adequacy and fluency.

Adequacy represents how well the meaning of the source sentence was captured, and is sometimes referred to as fidelity. Synonyms hardly ever affect this dimension, and the complexity of correctly evaluating this dimension is what mostly hinders quantitative metrics. Fluency means how correct and fluid the translation is in the target language, it evaluates sentences in categories such as grammaticality, readability and naturality of words of the words chosen.

\subsection{Ambiguity}

The challenge of identifying and properly representing ambiguity is one of the most complex ones for natural language. This happens because the model heavily depends on the diversity and quality of the dataset presented, and it should be able to assign distinct features to different meanings. Ambiguity can be found at the word-level, for instance the word sink can be a noun or a verb, which is known as lexical ambiguity. It can also occur for a whole sentence or a clause, then it is called structural ambiguity. Models inherently do not possess knowledge of the world to identify in which situations ambiguity may occur. 
If one could provide such knowledge to a model beforehand, than it should be capable of handling such situations, but no solution so far proved to be robust enough to achieve this.

A plausible critic for the effectiveness of the vector structure derived from \citet{mikolov2013efficient} is that it creates a single vector representation for words that may contain multiple meanings, hence capturing only the most frequent cases in the training set. It lacks flexibility to deal with occurrences of polysemy or homonymy in a given context. The word pike is a classic case of homonymy, also used to explain ambiguity in the course material of \citet{cs224n_stanford_l2}, since it can mean any of the following:

\begin{enumerate}
    \item A sharp edge or staff
    \item To piece or kill with a pike
    \item A type of elongated fish
    \item A railroad line or system
    \item One type of road
    \item To make one's way (pike along)
\end{enumerate}

There also other meanings this word can have based on your location in an English speaking country. Polysemy accounts for the coexistence of many possible meanings for a word or phrase, a phenomenon that happens in either Portuguese or English. In addition to the challenges of polysemy and homonymy, Portuguese provides some specific challenges such as distinct pronoun organizations that may carry the same meaning, and some others that were already mentioned in a previous section (\ref{portuguese_challenges}).

\subsection{The challenge of open vocabulary}
\label{open_vocabulary}

NMT models typically learn from supervised data, hence they are constrained to a fixed vocabulary. When dealing with an unseen word in some test scenario, traditional word-level models produce an $<unk>$ (unknown) token. This phenomenon is referred to as out-of-vocabulary (OOV) \abbrev{out-of-vocabulary}{OOV}. 

There are a number of ways to deal with the open vocabulary issue. Speaking of word-level models, the only way to reduce the chance for the model to produce $<unk>$ tokens is to guarantee that the training set has enough unique words to cover whatever words that appear for evaluation. One conclusion can be derived from this statement: the bigger the number of unique words a dataset has, the bigger the challenge of addressing open vocabulary becomes. Another observation to outline is that an increase in complexity and diversity of domains presented in the text for instance can be correlated with an increase of unique words. Some of the datasets used in this work corroborate with this hypothesis.

If the word-level restriction is removed, then subword models can increase the occurrence of smaller tokens, since they are capable of breaking the words into smaller units. These units can be characters (at the character-level), syllables or prefixes (at subword-level). By acting upon smaller pieces of the word, the model incorporates the ability of generating custom tokens, since any combination of these pieces become possible when generating translations.

\subsection{Addressing MT challenges}

This chapter performed an overview of the qualitative theory that is intrinsic to the study of natural language, which is relevant to the context of Machine Translation. It sheds a light on the practical differences that one language may have when compared to another, and helps to understand some of the challenges that MT researchers faced when creating systems based on expert knowledge. Modelling all these nuances was a complex task, since many of the first MT systems were rule-based or phrase-based, belonging to a MT branch calling statistical Machine Translation. Specific language phenomena were compiled in a set of rules that was unique to each language pair, like tables of equivalent expressions, requiring a manual effort to maintain and update such systems. Alignment, for instance, was treated like a latent variable. This increased the complexity of the translation task, since the goal switched from predicting $P(x|y)$ to $P(x, a|y)$, where $a$ is the alignment variable.

Neural systems represented a breakthrough as they removed the manual work of modelling language nuances, by learning such nuances on their own. This came along with the flexibility to adapt their architecture given the available resources and performance target. Among other advantages of NMT are the better use of context and phrase similarities, enhanced fluency of the translations and the necessity to optimize only a single network end-to-end. NMT also has some disadvantages: neural networks are known for being a black-box algorithm which is very hard to interpret and debug. It is also difficult to control neural biases (which can be gender or race related) that depend on the dataset used and the process of learning as a whole.

Despite the disadvantages, the solid benefits motivated an investment in this research branch, increasing the number of publications of the field since 2014. Further technical details and practical behaviour of neural networks will be discussed in the following chapters.

\chapter{Neural networks and Machine Translation}

Since their ideation in 1958, neural networks have seen peaks and valleys of research interest in the Machine Learning field. They have gone out of the tar pit (term borrowed \cite{Moseley2006OutOT} that originally mentions learnings of "dark times" in software engineering) in the 1980s with feedforward and recurrent variants and evolved with contributions such as convolutions and LSTM variants in 1990s. After such progress, they started to leverage promising results when applied to numerical and categorical data. Specially after 2013 neural networks were growing at an unseen rate, driven by the use of deep networks. Applications to image processing and natural language processing were massively explored and soon consolidated themselves as state of the art algorithms. 

Before going into further detail of the pioneer research that applied neural networks to solve Machine Translation, the task of translating from one language to another will be formally explained. Even though are a few approaches to reach the same result, we will focus the core of the explanations on the neural alternative.

\section{Machine translation as a task}

The ability to use an automated procedure to translate from a source language to a target language had precursors since the 1930s, with the use of mechanical dictionaries, later coming up with pioneer applications on the 1950s \cite{Hutchins1995MachineTA}. The first applications were heavily funded by military and focused on Russian-English translation or the other way around, mostly motivated by the war between the United States and Soviet Union. The translation task can be formulated as an optimization problem:

\begin{equation}
\label{translation_equation}
\hat{y} = argmax_{y} P (y | x),
\end{equation}
where $x$ is the representation of a sentence in the source language, $y$ represents another sentence in the target language and $P$ is a probability function. $P$ retains the parameters trained using supervised learning, which represent a vocabulary previously seen, which in turn will generate the model guess $\hat{y}$ for the sentence $x$. After applying the Bayes rule to equation \ref{translation_equation}, it can be separated into 2 components:

\begin{equation}
\label{translation_model}
P(x|y)
\end{equation}
and
\begin{equation}
\label{language_model}
P(y).
\end{equation}

Equation \ref{translation_model} represents a translation model, the benchmark reference of which translation matches which source sentence. Accordingly, \ref{language_model} can be interpreted a language model, which basically defines how fluently some meaning is defined using a set of words. To accurately reach a high probability for \ref{translation_model}, a large amount of high quality parallel data is required, while \ref{language_model} requires only proficiency under the scope of a single language.

Another approach to learn equation \ref{translation_model} is to narrow the scope of the translation task by breaking into more steps: achieved by introducing a latent variable $a$ that connects source and target clauses and words. This variable is the mathematical representation of the alignment model explored in a previous section \ref{word_alignment}, and it isn't explicitly specified in a dataset apriori. The alignment model basically connects particular words and positions between both languages.

The process of obtaining an efficient representation of $x$ is performed with a decoding algorithm. Decoding isn't a trivial task, since human translators can reword with synonyms, reorder and rearrange words, replace single words with multi-word phrases, and vice versa. Decoding has also been proven as a NP-hard problem, even in relatively simple translation models \cite{knight-1999-decoding}. The task of reaching a degree of correspondence between source and target sentences is aggravated by the aforementioned Machine Translation challenges \ref{machine_traslation_challenges}.

When decoding the source sentence, there are some special tokens used to represent the start of sentence and the end of sentence ($<$sos$>$ and $<$eos$>$). The decoding process always starts with the $<$sos$>$ token, then each word from the source sentence is sequentially read and the model tries to generate its correspondent word in the target language. This process only ends when the $<$eos$>$ has been produced or the maximum sentence length in words has been reached. When the model doesn't have a clear translation for a source word based on its current knowledge, it may output the token $<$unk$>$, which stands for unknown. 

With the approach used by Machine Translation defined and the qualitative challenges related to the quantitative theory, the reader has a clearer picture of the challenges faced by the first neural algorithms applied to solve this task. The sections below will present how the first algorithms worked and what were their flaws and strengths.

\section{The rise of Neural Machine Translation}

For a long time, neural networks haven't seen any applications to solve translation tasks, until 2013 when \citet{cho-etal-2014-learning} came up with a RNN Encoder-Decoder architecture. In a time where statistical models were the main alternative for Machine Translation, the adoption and later maturity of RNN algorithms gradually started to prevail in published papers.

There are some interesting properties in recurrent neurons that justify their application to solve the Machine Translation task. To clarify them to the reader, equations and properties of this neuron will be shown in a dedicated section.

\subsection{A brief introduction to the recurrent neuron}

Traditional feedforward neurons treat the data points they are trained on independently, hence switching the order of data points within a batch does not affect the final result. Recurrent neurons contain an interesting property that enabled neural networks in the past to succeed under a wider range of applications: they consider previously presented samples, therefore switching the order leads to a distinct result. 

Basically, a RNN contains as output the last hidden state and the concatenation of all hidden states after going through an activation function. The computational graph of a RNN is illustrated at Figure \ref{RNN_architecture}. In this graph, $W_{hh}$ is the weight matrix that maps the previous into the current hidden state, $W_{xh}$ is the matrix that maps the input $x_{t}$ to the current hidden state $h_{t}$ and $W_{hy}$ follows the same logic. In each addition operation between signals, a correspondent bias vector is summed to the final outcome.

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.4]{chapter2_images/RNN_architecture.png}
\caption{Computational graph of a RNN}
\label{RNN_architecture}
\end{figure*}

This schema represents the concept of "memory" to RNNs, and this feedback loop enables reccurrent layers to "unroll" (or unfold) the input of the current timestamp into a computational graph of its previous inputs, that consider recent calculations. In order to perform backpropagation, we set a depth to stop unrolling the network and propagate the gradients from there. In this maximum depth, the unrolled network ends as a feedforward layer, since previous inputs or hidden states are not considered anymore.

Observe that this property is desired for sentence representations, as presented in a previous section \ref{neural_embedding}, the neighbouring words do affect the meaning representation of a word in a sentence. This algorithm matched perfectly with the purpose of the translation task.

\subsection{The first RNN-based machine translator}
\label{learning_phrase_representations}

In Cho et al's approach, one RNN sequentially reads each symbol of an input sequence $\textbf{x}$ and encodes this sequence of symbols into a fixed-length vector representation, the encoder hidden state. The equation for the encoder hidden state at a given timestamp $t$ is defined by

\begin{equation}
h^{e}_{t} = f(x_{t}, h^{e}_{t-1}),
\end{equation}
where $f$ represents the nonlinear function inserted by the RNN. Let \textbf{x} be the vector that represents a variable-length sequence $\textbf{x}=(x_{1},\dotsc,x_{T_{x}})$. The information delivered from the encoder to the decoder comprises of the last hidden state of that equation, which in turn is called context vector $\textbf{c}$.

Another RNN decodes this representation into a sequence of symbols by predicting the next symbol $y_{t}$ given the context vector $\textbf{c}$ and all the previously predicted words $\{y_{1},\dotsc,y_{t-1}\}$. When calculating the probability over the translation $y$, the decoder decomposes the following joint probability:

\begin{equation}
p(y) = \prod^{T}_{t=1} p(y_{t} | \{y_{1}, \dots, y_{t-1}\}, \textbf{c}),
\end{equation}
where $y=(y_{1}, ..., y_{T_{y}})$ (the whole range of target words) and c is the previously mentioned context vector. The equation for the decoder's hidden state at time $t$ resembles the encoder 

\begin{equation}
h^{d}_{t} = g(y_{t-1}, h^{d}_{t-1}, \textbf{c}),
\end{equation}
with g being a simplification of the RNN function generated upon the input. The translated word which is generated based on $h^{d}_{t}$ can be calculated by

\begin{equation}
y_{t} = softmax(f(h^{d}_{t})).
\end{equation}

These equations provide a holistic view of the first RNN-based translator. In this setup, the encoder and decoder of the proposed model are jointly trained to maximize the conditional log-likelihood of a target word given a source sequence of words and the previous translated words. 

The drawback of using this architecture is that some of the information provided by the encoder RNN is lost, since only the last hidden state is used, and the other RNN output is discarded. Another issue is that the neural network has to compress all the necessary information of a source sentence into a fixed-length vector. This increases the challenge of dealing with long sentences, which is already complex for the RNN neuron that suffers with vanishing gradients.

\section{NMT by jointly learning to align and translate}
\label{align_and_translate}

To address the fixed-length and "information compression loss" issues identified in the previous implementation, the authors of the first paper proposed another architecture that changes how the encoder is connected to the decoder in \citet{bahdanau2016neural}. The context vector provided to the decoder $\textbf{c}=\{h^{e}_{1},\dotsc,h^{e}_{T_{x}}\}$ now goes under an activation function and has variable-length, allowing the decoder to get information from all the hidden states of the encoder, not just the last one. 

In the following topics there will be 2 distinct explanations for the main contribution of this paper: the attention mechanism. The first will split the process into illustrated steps to help the reader grasp the intuition, then the equations that allow such properties will be defined.

\subsection{Intuition for the attention mechanism}

These attention mechanism is purely a algorithm to assign weights to words, defining the alignment model presented before in section \ref{word_alignment}. These weights define how much attention will the decoder pay for a specific input word when guessing the next word of the translated sentence. 

Consider the translation of a Portuguese sentence "Vou às dez." to English. In Figure \ref{attention_first_steps} at step 1, the rectangles that resemble a traffic light represent the weight matrices of a RNN layer (see \ref{RNN_architecture} to remind them). Also, the neurons connected are reccurrent: hence, they access the hidden state of the previous one plus the input. The attention scores are derived from a dot product between the "$<$sos$>$" vector and the words of the input sentence, creating one distinct weight for each one of them at each step. The attention weights are recalculated at every step of the translation process, this is repeated for every next word generated in the output sentence. After calculating attention scores, a softmax layer within attention will attribute a weight for every input word, represented by the green bars on the top right of Figure \ref{attention_first_steps} at step 2.

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.331]{chapter3_and_4_images/attention_first_3_steps.png}
\caption{The first steps of how attention acts when translating a sentence in NMT, adapted from \citet{cs224n_stanford_l7}}
\label{attention_first_steps}
\end{figure*}

The model has already been trained on a supervised dataset, so it knows that the word "Vou" is usually translated to a set of words in English. The output of the attention function, which is calculated for the current decoder word based on previous encoder words, is as a weighted sum of the encoder word vectors, where the weights are represented by the green bars. Their module represent the importance of a word in such sentence. In the case of "Vou", starting from the just "$<$sos$>$" token, the subject of the verb in Portuguese is hidden and the characterization of the verb is resolved with the subjective personal pronoun "I". 

The translation task then keeps producing next words based on the input, generating the word "will" after "I" to complement the meaning of the original word "Vou" at step 3. The goal is to match the meaning of an action that will happen at the mentioned time. 

The attention weight distribution changes for each new word in each step. In Figure \ref{attention_last_steps} at step 4, the meaning of "às" becomes the focus, with some weights assigned to "dez" as well. Therefore, the attention distribution changes accordingly.
 
\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.332]{chapter3_and_4_images/attention_last_2_steps.png}
\caption{The last 2 steps of attention during the translation of a sentence, adapted from \citet{cs224n_stanford_l7}}
\label{attention_last_steps}
\end{figure*}

In step 5, the other tokens "ten" and "." were skipped to illustrate how the algorithm ends. Usually when the weights are considered to be low in absolute value with respect to the inputs, it may be a signal that the algorithm has converged. The translation process ends when the $<$eos$>$ token is produced. 

Now that the interaction and calculation of attention weights during translation is clearer, a mathematical walkthrough will be perfomed on the attention layer.

\subsection{Attention: Calculus background}

Consider the new variable-length context vector, that is now composed of several hidden states as input as opposed to only the last one in the first RNN-based translator model. A represenation of the context vector with respect to every encoder input word is now available. Such words can also be interpreted as a set of annotations $\{h^{e}_{1},\dotsc,h^{e}_{T_{x}}\}$. The attention output is then computed by a weighted sum of such annotations $h^{e}_{i}$ at a given time $i$:

\begin{equation}
\mathbf{c}_{i}=\sum^{T_{x}}_{j=1}\alpha_{ij}\mathbf{h}^{e}_{j}.
\end{equation}

Each annotation receives a weight $\alpha_{ij}$ in the vector $\textbf{c}_{i}$, the absolute value of these multipliers comprise a mechanism that can be interpreted as how relevant that annotation is in the input sentence, hence defining its importance for the decoder. Those weights are also known as attention coefficients, which represent a probability that a target word $y_{i}$ is aligned to, or translated from a source word $x_{j}$. This contribution for the translation process enables the network to be more interpretable since each translation guess becomes a weighted sum of source words, solving the challenge stated at \ref{word_alignment}. The attention coefficients are defined by

% \textcolor{red}{a explicação para $h_{i-1}$ e nao $h_{i}$ está no paper}
\begin{equation}
\alpha_{ij}=\frac{\exp{(a(h^{d}_{i-1}, h^{e}_{j}))}}{\sum^{T_{x}}_{k=1}\exp{(a(h^{d}_{i-1}, h^{e}_{k}))}},
\label{attention_weights}
\end{equation}
where $a$ is an alignment model that relates the embeddings $\mathbf{h}_{j}^{e}$ in the position $j$ with the embeddings of $\mathbf{h}_{i}^{d}$ at the position $i$. The variable $k$ iterates over the entire range of the input sentence $T_{x}$. The alignment scores can also be interpreted as a way to distribute attention over the input, and $a$ is parameterized as a feedforward neural network that is jointly trained with the other components of the system. The authors also mentioned that this attention mechanism relieves the encoder from the burden of having to encode all the information of the source sentence, hence contributing to its resilience when dealing with long sentences.

Another improvement that contributed to the success of this approach is the use of bidirectional RNNs to encode the input sequence. The key difference when compared to a unidirectional RNN is that the vector that flows from the encoder to the decoder now contains a representation not only of the preceding words, but also of the following words, allowing a more efficient attention weight distribution. This vector is simply a concatenation of the representations of the forward and the backward hidden states. Despite these improvements, the attention mechanism is still rather simple, since it considers only a weighted sum of the input, and does not accomplish the importance attribution task as well as its succeeding variants, implemented in other papers later on.

One detail of the newly proposed NMT system which contributed to its performance but is not clearly highlighted in the paper (only mentioned) is the use of a beam search decoding algorithm. This algorithm appeared first as a generic sequence transducer \cite{DBLP:journals/corr/abs-1211-3711}, but a few years later was applied to the context of MT \cite{DBLP:journals/corr/SutskeverVL14}. The main benefit brought by using it is the execution of a more efficient search for the most probable next word given the preceding words. It reduces the chance to get trapped in locally optimal but globally inefficient solutions such as the Greedy Search algorithm, allowing the NMT model to explore more translation variants, hence converging to a more realistic translation. 

\subsection{Search algorithms for NMT}

After converging to a local optimum and learning word vector representations that reflect semantical and syntactical attributes of the words, the model can be tested and generate translations. When accomplishing the task of generating translations, the quality of the search performed and number of possibilities considered by the algorithm make a difference on the final result. The traditional approach and a more clever one will be discussed here: greedy search and beam search, respectively.

\subsubsection{Greedy search}

This was the approach used for the first RNN-based translator in section \ref{learning_phrase_representations}. It is considered as the simplest way to generate a translation given the context, as it calculates which word maximizes the probability of the next occurrence given the set of previous words. This search is rather simple and is used since the beginning of the statistical Machine Translation field. The goal is to perform the calculation of

\begin{equation}
\hat{y}_{t} = argmax_{y}P(x_{t}|y_{t})P(y_{t})
\end{equation}
at each timestep $t$ to define the probability of the next words, where $x$ is the representation of a sentence in the source language and $y$ is the current composition of the target sentence built at timestamp $t$. This algorithm has the drawback of being impossible to go back after generating an unnatural or sub-optimal sentence. Basically, a token that looks good to the decoder in a given moment might turn out later to have been a wrong choice. When this happens, the model gets limited to generating new words on top of the current sub-optimal sentence, hence the translation loses fluency. Nonetheless, it was the top of mind algorithm for the MT community for a long time.

Ideally, the only method that guarantees to find the best translation is the exhaustive search. Given a vocabulary of size $V$, exhaustive search would be performed by computing every one of the $V^{T}$, with $T$ being the length of the translation in words. Going over such a large word search is obviously too expensive. Hopefully, a more cost-effective search variant that considers more words than the greedy variant and that is less complex than the exhaustive one was created. 

\subsubsection{Beam Search}

In this algorithm, instead of choosing the best token at each timestep, $k$ possible tokens are kept at each step. This fixed-size parameter is called beam width. It works by assigning a probability to each word, for each of the $k$ available candidates, and this probability is obtained through a softmax calculated over the entire vocabulary. These numbers are then sorted from the most probable to the least, and $k$ best possible tokens are considered in each step. Such candidate tokens are called hypotheses, and this process of calculating probabilities, sorting the sentences and progressively adding words at the end of the sentence at each step means that the algorithm is simultaneously evaluating a set of $k$ different sentences.

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.7]{chapter3_and_4_images/beam_search.png}
\caption{Example of beam search with beam width of size 2, taken from \citet{nmt_nlp_speech_draft_book}}
\label{beam_search}
\end{figure*}

A hypothesis only stops being evaluated if it ends with an $<$eos$>$ token, or if it is not within the top $k$ options. If an $<eos>$ token is reached, the sentence is placed aside and other hypothesis continue to be explored. This process is repeated until all the hypothesis reach an $<eos>$ token or the maximum predetermined sentence length (in words). Figure \ref{beam_search} shows an example for a small sentence using beam width of size 2, where "arrived" and "the" are the first guesses, later producing 4 words where only the 2 most probable were selected. After picking "green" and "witch" by sorting probabilities in a descending order, another 4 words are generated, and the process is repeated until convergence.

Besides the practical gains regarding fluency, this algorithm has the drawback of penalizing longer sentences with lower scores. Moreover, this issue can be partially addressed by a length normalization factor, but not totally mitigated. Beam search has become the standard approach in NMT, with a good starting point being the use of a beam width of size 3, the same adopted in our experiments.

\subsection{Exploring RNN-based sequence to sequence architectures}

An interesting work that illustrates the potential of the previously presented RNN models is due to \citet{britz2017massive}. They use \cite{cho-etal-2014-learning} as a base model and evaluated the tuning of several hyperparameters, like embedding size, RNN cell variant (LSTM and GRU), encoder and decoder depth, unidirectional vs. bidirectional RNN encoders, attention mechanism and beam width parameters. One of the claims of this work is that careful hyperparameter tuning can yield better results than exploring architecture variations.

Here are some interesting findings reported by \citet{britz2017massive}: (1) larger embeddings consistently outperforms smaller ones by a thin margin, (2) LSTM cells consistently outperformes GRU cells, (3) deeper decoders tend to lead to performance increases, (4) additive attention achieves slightly better results than multiplicative attention and (5) beam searches tuned to a "sweet spot" can increase model performance up to 5\%.

\section{Transformer models}

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.7]{chapter3_and_4_images/Transformer_architecture.png}
\caption{Transformer model architecture, taken from the original paper \cite{DBLP:journals/corr/VaswaniSPUJGKP17}}
\label{Transformer_architecture}
\end{figure*}

One contribution switched the focus on recurrent and convolution-based neural network models for NMT in 2017: the Transformer architecture. Introduced in the seminal work of \citet{DBLP:journals/corr/VaswaniSPUJGKP17}, this model is not constrained by the same condition as RNNs. Due to their sequential pattern, RNNs have to wait for an operation to finish to begin another one, which isn't the case of Transformers. This new approach gained traction and adherence from the community, since parallelization became one of its strengths. Also, along with it, came the potential to explore other architecture variations.

In the original implementation, the encoder and the decoder contain 6 stacked Transformer layers. Regarding the encoder, each layer contains a multi-head self-attention mechanism, followed by layer normalization and a position-wise feedforward fully connected layer, represented by the "Add \& Norm" block in Figure \ref{Transformer_architecture}. A residual connection is also employed around each of the 2 blocks inside a Transformer layer, which is the arrow that goes into the "Add \& Norm" block. The structure is quite similar in the decoder, although an extra masked multi-head attention layer is added over the output of the encoder stack, before multi-head attention. This mask, combined with the offset in the target sentence (output embedding) by one position, ensures that predictions for a given position $i$ depend solely on the known inputs for positions less than $i$. Backpropagation operates end-to-end in this architecture.

\subsection{Unveiling self-attention}

The attention mechanism differs from previous implementations, but the goal is the same: to enable the model to distribute the influences of every input word on the output words generated. It can be described as mapping a query and a set of key-value pairs to an output, where the query (Q), keys (K) and values (V) are all matrices with dimensions defined by the respective hyperparameters, set to 64 in the original work. Attention's "keys and values" are hidden state representations in encoder network \cite{eisenstein2019introduction}, and "queries" are similar representations but belonging to the decoder. 

Despite the new attention matrices with different representations, the core of this new mechanism is still pretty much the same as the one used by \citet{bahdanau2016neural}. The attention weights are defined by a similar equation to (\ref{attention_weights}), now given by (\ref{selt_attention_weights}), where its numerator is summarized by (\ref{key_query_affinity}):

\begin{equation}
e_{ij} = Q^{T}_{i}K_{j}
\label{key_query_affinity}
\end{equation}
and
\begin{equation}
\alpha_{ij} = \frac{\exp{(e_{ij})}}{\sum_{j'} \exp{(e_{ij^{'}})}}.
\label{selt_attention_weights}
\end{equation}

% http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture09-Transformers.pdf

In such equations, $Q$ and $K$ are representations for the queries and values matrices respectively and $i$ and $j$ represent positions in the output and input. The equivalent weighted sum that produces the output of the self-attention layer $y$ for Transformers also shares similarities with RNNs:

\begin{equation}
y_{i} = \sum_{j}\alpha_{ij}v_{j}.
\label{Transformer_output}
\end{equation}

\subsubsection{Scaled dot-product attention}

The name scaled dot-product attention is due to the dot products of the query matrix computed with all keys, normalized (scaled) by $\sqrt{d_{k}}$ and applied through softmax. The result of this operation are the weights for the correspondent values. The illustration that constitutes this mechanism is provided in Figure \ref{scaled_dot_prod_attention}, where MatMul represents a matrix multiplication block and the mask is optional, used only in the decoder.

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.7]{chapter3_and_4_images/scaled_dot_prod_attention.png}
\caption{Scaled Dot-Product Attention, taken from \citet{DBLP:journals/corr/VaswaniSPUJGKP17}}
\label{scaled_dot_prod_attention}
\end{figure*}

The equation that reflects the output of this block diagram is given by

\begin{equation}
Attention(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V.
\label{attention_equation}
\end{equation}

One advantage of using dot-product attention here is that it is space-efficient. Also, it can be implemented using optimized matrix multiplications, benefitting from parallelization when compared to additive attention, for instance. The scaling factor is an addition to the original dot-product attention, as the authors claim that it helps reduce the magnitude of dot-products, which may push the softmax function to regions where is has extremely small gradients. 

\subsubsection{Multi-head attention}

This technique is basically the concatenation of scaled dot-product attention blocks. The assumption behind it is that different learned projections of the same $Q$, $K$, $V$ matrices yield representations that may complement each other, as an ensemble of attention heads. Figure \ref{multi_head_attention} shows how it is related with the previous algorithm. The result of the concatenated matrices goes through a linear feedforward layer on the top.

Multi-head self-attention also plays a role on the overall performance of the Transformer. The authors claim that it is beneficial to project the matrices Q, K and V in attention $h$ times, with different learned projections by each one of the heads. The projections of each head are concatenated and the attention weights are adjusted to different representation subspaces. In the original setting, $h=8$ heads are used. One important constraint which is usually followed as a recommendation is that the hidden size ($d_{model}$) must be divisible by the number of attention heads. 

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.7]{chapter3_and_4_images/multi_head_self_attention.png}
\caption{Multi Head Attention, taken from \citet{DBLP:journals/corr/VaswaniSPUJGKP17}}
\label{multi_head_attention}
\end{figure*}

At the time of writing, multi-head self-attention was thought to be consistently contributing to the Transformer performance. Years later after an analysis performed by \citet{DBLP:journals/corr/abs-1905-09418}, this hypothesis was rejected. The authors evaluated how the contribution of each attention head happened during the training process, and they find that only a small subset of heads are enough to sustain the Transformer translation scores. They analyse the importance distribution, a heatmap that correlates each output word with every input word, and discover that several heads learn similar dependency mappings. By using this map as a qualitative signal of feature importance per head, they propose to prune the heads that have similar dependency maps and conclude that there is no noticeable loss in translation quality by doing this.

This paper raises a relevant question to the community: are deep learning works usually biased towards increasing complexity and model parameters unnecessarily? Less complex models have a smaller carbon footprint and energy waste, which can lower the impact on nature, which could bring the NMT community closer to the status of Green AI.

\section{Attention variants for sequence to sequence learning}

Not only architecture variants can lead the Transformer to better quality translations, the attention mechanism has also received some contributions. In the approach followed by \citet{DBLP:journals/corr/abs-2006-10270}, multi-head attention is scaled to compose a new mechanism called multi-branch attention, where each branch is an independent multi-head attention mechanism. 

Despite the evidence provided that increasing the number of heads in attention mechanisms may generate redundancy, this work has reported better results than the standard Transformer baseline, which goes against the analysis previously stated. In this paper, other techniques such as a drop branch mechanism (similar to dropout \cite{JMLR:v15:srivastava14a}, but applied to Transformer's branches) and a specific initialization recipe for each branch may have an effect on the final outcome. The composition of techniques makes unclear whether the benefits come from scaling the attention complexity or from those architectural alternatives.

Going back to the literature, variants applied on the set of words that attention sees and on the weight distribution function are among the most common variants explored. In the paper published by \citet{luong-etal-2015-effective}, the attention mechanism is split into 2 classes: global and local attention. The former always attends to all source words of a given sentence, while the latter only looks at a subset of words at a given time. 

Local attention is explored with different alignments: monotonic and predictive. In the first case, they just assume that source and target sequences are roughly monotonically aligned, and concatenate the current hidden state with the source hidden state. In the predictive variant, they use a Gaussian distribution centered around the word to be predicted and define alignment weights based on the module of the gaussian, so nearby words are more important than distant words. Following this approach, the attention weights are biased to follow a normal distribution.

The main findings of this paper are: (1) attention-based models usually outperform non-attentional ones and (2) properly tuning the alignment model can yield results. A small increase in performance was be observed by adding a custom weight function (Gaussian-based in this case). They add that using an ensemble of the previous attention architectures may be also effective, as they claimed new state of the art results. However, further details of how this ensemble was implemented are not provided.

Integrating attention mechanisms with different attention perspectives is another exploited approach, which has been followed by \citet{calixto-etal-2017-doubly} and \citet{cui-etal-2019-mixed}. In \cite{calixto-etal-2017-doubly}, a single layer feedforward network is used to compute the expected alignment between each annotation vector in both mechanisms, but this increases the amount of parameters to train the model. In \cite{cui-etal-2019-mixed}, the concept of forward and backward attention is introduced, where specialized masks help the Transformer to model word order information, in a slightly different schema of the positional encoder used in the original paper. Both mechanisms are concatenated with the standard global and local attention. Even though this approach leads to a better BLEU score, it also increases model parameters.

There is no strong evidence that tuning the attention mechanism would provide relevant benefits in low-resource settings. Furthermore, all variations usually increase model parameters, which increases the complexity and amount of data required to learn, along with the amount of resources required to be trained (GPU memory). In a single GPU scenario, a larger and more robust GPU would be necessary to train. Increasing the number of GPUs unfortunately wasn't an option in the scope and budget of this work. Adding this to the fact that data augmentation and embedding techniques have better potential in low-resource availability environments, the focus of this work was deviated from tuning attention mechanisms.

\subsection{Positional encoding}

To insert word order information even without recurrence or convolutions, the encoder augments the base layer of the network with positional encodings of the indices of each word in the source sentence. Such encodings are vectors for each position $i\in\{1,2,\dotsc,I\}$. These encodings are set equal to a set of sinusoidal functions of $i$ positions:

\begin{equation}
PE(i, 2d) = \sin{\frac{i}{10000^{\frac{2d}{d_{model}}}}},
\end{equation}
and
\begin{equation}
PE(i, 2d + 1) = \cos{\frac{i}{10000^{\frac{2d}{d_{model}}}}},
\end{equation}
where $d$ is the dimension or level within the representation, the amount of real numbers used to represent a word. Positional encodings provide an interesting property when they reach the attention mechanism, it can be seen after expanding the 4 terms that appear after the expansion of attention weights. Consider Equation \ref{posenc_attention}, which is an expansion of \ref{attention_equation} before applying softmax, with the addition of word position information:

\begin{equation}
\begin{aligned}
\label{posenc_attention}
\alpha_{ij} ={} & \frac{(w_{i}*W^{Q,1})(w_{j}*W^{K,1})^{T}}{\sqrt{d}} + \frac{(w_{i}*W^{Q,1})(p_{j}*W^{K,1})^{T}}{\sqrt{d}} + \\ 
& \frac{(p_{i}*W^{Q,1})(w_{j}*W^{K,1})^{T}}{\sqrt{d}} + \frac{(p_{i}*W^{Q,1})(p_{j}*W^{K,1})^{T}}{\sqrt{d}}.
\end{aligned}
\end{equation}

In this equation, $\{K, Q\}$ represent the Key and Query matrices, $\sqrt{d}$ is the normalization factor used in \citet{DBLP:journals/corr/VaswaniSPUJGKP17}, $p$ is a learnable real-valued vector assigned to each position $i$ or $j$ (the positional encoding factor) and $w$ is the actual corresponding embedding vector in the embedding layer. The 4 terms of equation \ref{posenc_attention} represent interactions between word-to-word, word-to-position, position-to-word and position-to-position, respectively \cite{DBLP:journals/corr/abs-2006-15595}. The absolute positional encodings that are derived from the traditional Transformer simply adds the positional embeddings to the original word embeddings. The result of such combination is the ability to grasp word position and benefit from it during training. 

The drawback of using absolute positional encodings is that this information is summed to the original word embeddings, but word position and word meaning are unrelated information. This brings mixed correlations between the word semantics and the position information itself, adding unwanted noise to the learning process and limiting the expressiveness of the model \citet{DBLP:journals/corr/abs-2006-15595}.

\section{Relevant Transformer variations}

Since their publication, Transformers were customized in many ways by researchers aiming to become state of the art. Not all the variations are worth a story to tell, but some of them have exceeded expectations and deserve some space in this discussion. Beginning with BERT (Bidirectional Encoder Representations from Transformers)\abbrev{Bidirectional Encoder Representations from Transformers}{BERT}, the Transformer idealized by \citet{DBLP:journals/corr/abs-1810-04805}, designed to train deep bidirectional representations from unlabeled text.

Two training tasks can be performed with BERT: the first can be compared to training a masked language model, where some percentage of the input tokens are hidden at random, and then those masked tokens are predicted. The second aims to extend the model's functionalities to other NLP tasks, such as question answering and natural language inference, and this is accomplished via next sentence prediction. The intriguing aspect of BERT is that it can be easily fine tuned to perform a wide range of tasks, while still keeping an architecture that isn't far from the original Transformer. This is achieved while still beating the best results available for range of tasks, such as natural language understanding, inference and question answering.

Some other variants of the traditional Transformer architecture were heavily focused on increasing complexity aiming to surpass the state of the art, and becoming able to solve a wider number of tasks. The variant presented by \citet{brown2020language} proves that it is possible to improve performance by simply increasing model size, and as a consequence, the dataset size and computational power used as well. 

GPT-3 comes in various sizes, with the smallest model having 125M parameters, 12 attention layers, each with 12 heads of dimension 64. The largest version has 175B parameters, and it uses 96 attention layers, each with 96 heads of dimension 128. This model also raised an environmental concern, as it used about 190,000 kWh to be trained \cite{quach_2020}. Some results are interesting to highlight: (1) text generation, where humans fail to separate artificially generated summaries from original ones in 24\% of the cases, (2) Machine Translation, where it beats the best available BLEU scores in language pairs with english as target and (3) other alternative tasks, such as arithmetic and word scrambling. 

Not all the Transformer variants focused on firepower, though. There is also some research aiming to come up with lightweight alternatives to the Transformer, with the goal of increasing performance but keeping the computational power under satisfatory standards. Both models proposed on \citet{DBLP:journals/corr/abs-1911-12385} and a year later on \citet{mehta2021delight} are able to improve the efficiency of the original Transformer as a language model on a range of tasks. The definition of efficiency here is achieving a high BLEU score (for NMT) or another score (for other tasks), considering the model parameters as a cost. 

Delight, the most recent work, changes the inner mechanism of the Transformer layer by using the delight transformation, which is composed by group linear transformation layers that are shallower and narrower near the input while deeper and wider near the output. The authors state that this transformation decouples attention dimensions from the depth and width, allowing representations to be learned efficiently using block-wise scaling. Delight also replaces multi-head attention for single-head attention, reducing the number of parameters needed to train. On the WikiText-103 dataset, they increase bleu by 3.3\%, while using 80\% of the standard Transformer parameters and requiring 23 hours to train, as opposed to 37 hours used by the benchmark.

\section{Alternative training objectives}

Many state-of-the-art models have reached their scores using the traditional cross-entropy loss. Most language models also focused on maximum likelihood learning objective, as stated in \citet{DBLP:journals/corr/abs-2012-15515}. Their work mostly focused on reviewing the contributions and the tools available for executing the NMT task, pointing out trends that happened through time. The equation for log-likelihood is defined as

\begin{equation}
L(\theta) = \sum^{S}_{s=1} \log{P(y^{(s)}|x^{(s)}; \theta)},
\end{equation}
where $x$ and $y$ are pairs of the training set $D=\{(x^{(s)},y^{(s)})\}^{S}_{s=1}$. The standard goal of NMT algorithms is to maximize the log-likelihood on the training set data, which in turn is calculated by 

\begin{equation}
\hat{\theta}_{MLE} = argmax(L(\theta)).
\end{equation}

Nevertheless, the work of \citet{ranzato-sequence} indicate two drawbacks of this approach. First, during training, NMT models are not exposed to their translation errors: this phenomenon is referred to as exposure bias. Finally, the MLE estimation is defined at word-level rather than sentence-level. So, theoretically, the optimization objective is not aligned with the final objective, which is generating a sentence that can match a human translation.

With this problem in mind, they introduce Mixed Incremental Cross Entropy Reinforce (MIXER), which switches the learning objective towards sentence-level training. This algorithm tries to handle the problem of backpropagating gradients on non-differentiable metrics such as BLEU with some ideas borrowed from reinforcement learning (RL) \abbrev{Reinforcement Learning}{RL}. 

Other works also have put effort on raising awareness to the learning objective issue. For instance, the technique proposed by \citet{shen-etal-2016-minimum} reported better results by following an alternative objective. They claim that MIXER samples only one candidate to calculate the reward, while their approach, Minimum Risk Training (MRT) \abbrev{Minimum Risk Training}{MRT}, generates multiple samples. This potentially increases MRT's capability of discrimination, pretty close to the effect of increasing beam search in a standard NMT setup.

Apart from the efforts put into iterating on the learning objective and the benefits claimed by these papers, some side effects of switching the training objective are also diagnosed by \citet{DBLP:journals/corr/abs-1907-01752}. They point out the weakness of RL-based approaches for optimization, claiming that some of the gains are not fully attributed to the techniques, and also discuss convergence issues related to the training objectives proposed by \citet{shen-etal-2016-minimum} and \citet{ranzato-sequence}. Based on the presented review of the literature, the potential of changing the learning objective with the goal of having better translations under low-resource is unclear. Thus, changing the learning objective in this work was not taken into consideration.

\chapter{Low-resource domain context}
\label{low_res_context}

The term "low-resource" recalls situations where any of the resources needed to commit to one task are lacking, and in NMT this phenomenon can manifest itself in various alternatives. Complex models that contain a high amount of parameters to be learned also demand an abundant amount of data, a phenomenon that is explained by the VC (Vapnik-Chervonenkis)\abbrev{Vapnik-Chervonenkis}{VC} dimension theory. If a supervised complex model has scarce data for a task and its training process is limited to that data, its potential to achieve relevant scores can be significantly hindered. This can even lead to other simpler algorithms outperforming that model in the same task. There are some situations where if a dataset is small enough, phrase-based Machine Translation models can also overcome neural-based ones, such as the case study reported in \citet{sennrich-zhang-2019-revisiting}.

Scarce data also has a remarkable contribution for increasing the open vocabulary challenge, mentioned in the Section \ref{open_vocabulary}. Solving the translation task under low data availability also means solving the open vocabulary challenge under adverse conditions. Increasing vocabulary complexity somewhat also increases the challenge, so a technique adopted to solve the issue must be cost efficient regarding computational resources. 

The second most important matter that is fundamental to the success of a translation model is the infrastructure used for the training setup, which usually requires one or more GPUs or TPUs (Tensor Processing Units)\abbrev{Tensor Processing Unit}{TPU}. Models that require a high batch size such as Transformers cannot be trained in practical terms with a CPU. One can accelerate the training process hundreds of times when switching to a GPU, so it can represent a difference from hours to days. 

To measure the memory footprint of a dataset and understand whether it fits in some infrastructure or not, its size must be taken into consideration. Two dimensions that directly influence the memory footprint of a dataset are the number of sentences and number of words per sentence. This number is relevant to determine the ideal batch size that will be fit in every epoch. 

Another factor that has an influence on the batch size choice is the amount of trainable parameters in a model, which increases linearly with how deep a neural network is. A common practice is to increase the batch size proportionally to the network depth: this phenomenon is also reflected in the parameter of GPT-3 variants in the work of \citet{brown2020language}. There is no theory that establishes a deterministic rule for how batch size should increase based on model parameters. However, it is considered a good practice to consider them directly proportional, with the goal of achieving better generalization during training.

There are also other resources that must be considered to the NMT task but rarely represent a bottleneck: those are disk storage, internet connection and a correctly conFigured python environment (or another language of choice). This happens because such resources are more accessible than cutting edge GPUs. When referring to low-resource domain in the following sections, only dataset size and GPU power constraints are considered, as mentioned above.

Techniques that allow the NMT models to be more resilient to open vocabulary and other issues mentioned in the previous paragraphs are available in the literature. They will be presented and discussed in the following sections, right after a brief overview of some challenges we had to understand and bypass to succeed to tackle Portuguese NMT under low-resource.

\section{Intrinsic challenges of this work}

Before officially focusing the analysis performed here on low-resource settings, the initial objective was the traditional approach of creating a model to reach the state of the art. However, as soon as the infrastructure has shown limitations we realized that it wasn't possible to compete in such level. Performing experiments on different lengths of datasets and tuning distinct sets of hyperparameters for Transformer variants can be quite challenging when using a 12GB GPU, which is the standard hardware when using low-cost or free cloud providers as of 2021. 

Transformer variants have shown terrific performance in a variety of practical applications, attracting multiple companies and having dedicated research groups seeking to reach the state-of-the-art status. Researchers from more resourceful countries or universities usually have multiple GPUs in the same cluster, that enable them to expand the total GPU memory available, but this can be quite expensive without sponsorship to get on your own. Given such limitations, the decision was to contribute to the low-resource branch of NMT given the overall practical limitations.

The process of understanding the constraints of your environment also involves a non-friendly trial-and-error approach. Even when setting your batches to begin bigger in length and dwindle in size, the out of memory error usually does not appear in the beginning of the first epoch or even in the first few epochs. One has to perform a number of attempts that take some time to fail to understand that the infrastructure is constraining the experiment to happen as desired. This trial-and-error process is repeated whenever an experiment changes the dataset used or the number of parameters (complexity) of the model being trained. This has contributed to a significant increase on the time spent to perform experiments on different datasets and using different techniques.

In the next sections the techniques exploited to deal with low-resource issues and some technical trade-offs of using them will be outlined.

\section{Subword embeddings}

The theory provided in previous chapters was mainly focused on word representations, with only a few mentions to some interesting properties of switching to the subword level. In this section, a more detailed explanation of how subword models operate will be provided. The process of obtaining the real number vectors is basically the same, the only change applied is to which token is being represented by a given dense vector. An algorithm is used to break conventional words into tokens, and the result of this operation influences the explainability (quality) of the final subword representations.

This type of embedding is more robust when facing the out of vocabulary (OOV) issue when compared to the traditional word-level ones \cite{eisenstein2019introduction}. When breaking the word into smaller tokens, the smaller tokens generated are usually more likely to appear in both training and test sets. This relieves part of the burden of having to be trained on a relevant amount of the same words that the model will be tested. Subword models can be designed to act in a character level or in an intermediate level between word and character (like phonemes, syllables and others). 

There are a number of subword embeddings available in the literature that could be used for low-resource purposes. The goal of this work is to evaluate the effect of a set of techniques, not explore their variants in detail. Therefore, an embedding called BPE was chosen due to its wide usage and success in a number of applications.

\subsection{BPE (Byte Pair Encoding)} \abbrev{Byte Pair Encoding}{BPE}

The pioneer work that introduced subword embeddings was presented by \citet{DBLP:journals/corr/SennrichHB15}. In their approach, segmentation is performed based on BPE - a compression algorithm, introduced to break the raw text into sub-parts. Roughly, BPE breaks the words in a corpus into smaller parts (the smallest BPE unit is a character). Some of these parts are subsequently merged, and the number of merge operations is a hyperparameter to be tuned. A sample code that details the steps of how segmentation occurs can be found at Algorithm \ref{bpe_algorithm}. 

In the original implementation, notice that the only required parameters is the number of merges, which is not much intuitive given that the vocabulary size is what concerns the NLP researcher the most. Thus, one of the drawbacks of the original implementation is that one cannot clearly define the maximum vocabulary size a priori, relying solely on the number of merge operations for that. This opens a margin for creating a huge vocabulary that has a low occurrence of some subwords, possibly hindering its performance. Despite the mentioned drawbacks and clear room for improvements, the adoption of this method rapidly became widespread because it is simple, cheap to run, easy to understand and effective.

\begin{algorithm}
\begin{algorithmic}[1]
\caption{Pseudocode for BPE, taken from \citet{bostrom-durrett-2020-byte}}
\label{bpe_algorithm}
\State Input: set of strings $D$, target vocab size $k$
\Procedure{BPE}{$D,k$}
     \State $V \gets$ all unique characters in $D$ (about 4,000 in English Wikipedia)
     \While{|V| < k do} \Comment{Merge tokens}
        \State $t_{L}, t_{R} \gets$ Most frequent bigram in $D$
        \State $t_{NEW} \gets t_{L} + t_{R}$ \Comment{Make new token}
        \State $V \gets V + [t_{NEW}]$
        \State Replace each occurrence of $t_{L}, t_{R}$ in $D$ with $t_{NEW}$
    \EndWhile
    \State \textbf{return} $V$
\State \textbf{end procedure}
\EndProcedure
\end{algorithmic}
\end{algorithm}

% \textcolor{red}{No paper original, eles não detalham o que é bi-gram segmentation. Das 7 segmentações, 3 são baseadas em bi-gram e eu nao tenho como saber como esse algoritmo funciona pra explicar aqui. Pesquisas no google revelam muitas variações dessa técnica e sem referencia clara sobre qual foi a primeira original. Acredito que não sou obrigado a explicar algo que o proprio paper nao detalhou direito.}
The process of breaking the words into smaller units inherently increases the number of samples per token. This grants to the algorithm the ability to cope better with the OOV issue. In the original paper \cite{DBLP:journals/corr/SennrichHB15}, they compare BPE against segmentation techniques commonly used in Statistical Machine Translation (SMT) \abbrev{Statistical Machine Translation}{SMT}, for instance bi-gram segmentation. Bi-gram stands for the combination of 2 tokens into a single words. For more details about such algorithms, please check the original paper. The tests performed against the segmentation benchmarks for evaluating translation scores and OOV coverage show that BPE outperforms most of them.

% https://aclanthology.org/2020.findings-emnlp.414.pdf

To illustrate better how BPE works, consider a corpus of 4 distinct words, each one with their own amount of occurrences: $\{"old</w>": 7, "older</w>": 3, "finest</w>": 9, "lowest</w>": 4\}$, where the token $</w>$ marks the end boundaries of words. The whole process of breaking words into sub-parts presented in Figure \ref{bpe_steps} is explained below, where each enumerated bullet represents the equivalent step in the figure:

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.37]{chapter3_and_4_images/bpe_explained.png}
\caption{Steps of the BPE algorithm applied to an example corpus.}
\label{bpe_steps}
\end{figure*}

\begin{enumerate}
    \item The first step of the algorithm simply counts the number of characters in the initial vocabulary (the presented corpus of 4 words), as a character-level algorithm would behave. At each step, the occurrences of token pairs are counted. The algorithm prioritizes the most frequent token pair first
    \item The most common token pair in the given vocabulary is $"es"$, which occurs $9 + 4 = 13$ times. The standalone characters of that paired token must be subtracted by its number of occurrences. This happens to distinguish the occurrences of the single character tokens $e$ and $s$ occurs and when $es$ happens, which are now separated. The step results in $s$ being eliminated (represented by the red line), as it only spawns within the $es$ token
    \item Another token pair that spawns with the same frequency as $es$ is $est$, which by coincidence also contemplates all the occurrences of $es$ and $t$, resulting in their elimination.
    \item In the next step, we merge $est$ with $</w>$ as the resulting token also has the highest number of occurrences ($13$). This reduces the frequency of $</w>$ to 10. 
    \item Another common token pair is $ol$, which has a frequency of $10$ in the given vocabulary. To represent this new token, the single characters $o$ and $l$ are subtracted by its number of occurrences, reducing their frequency to 4.
    \item In this step the token $old$ is formed, incorporating all the occurrences of $ol$ and removing the token $d$ from the distribution table.
    \item The final distribution resulted from the BPE algorithm is shown in Table \ref{final_distribution}. Observe that the final token count is 11, less than the previous starting point of $12$.
\end{enumerate}


The main difference between this method and the older ones is that it is capable of breaking words into interpretable units. Finding a clever way to split words was the main challenge of that time. On the other hand, as it is a greedy algorithm, there were still some cases in which the subword tokens do not represent a morpheme with enclosed meaning. However, the share of meaningful subword obtained was enough for it to stand out among the rest.

\begin{table}[h]
\caption{Final distribution of tokens (step 7)}
\fontsize{10}{12}\fontfamily{phv}\selectfont
\label{final_distribution}
\centering
  {\footnotesize
  \begin{tabular}{| P{0.1\linewidth} | P{0.1\linewidth} | P{0.2\linewidth} |}
    \hline
    Number & Token & Frequency \\
    \hline
    1 & </w> & 10 \\
    \hline
    2 & o & 4 \\
    \hline
    3 & l & 4 \\
    \hline
    4 & e & 3 \\
    \hline
    5 & r & 3 \\
    \hline
    6 & f & 9 \\
    \hline
    7 & i & 9 \\
    \hline
    8 & n & 9 \\
    \hline
    9 & w & 4 \\
    \hline
    10 & est</w> & 13 \\
    \hline
    11 & old & 10 \\
    \hline
\end{tabular}}
\end{table}
% https://amitness.com/2020/06/fasttext-embeddings/

An evidence of its effectiveness is given by the work published later by \citet{sennrich-zhang-2019-revisiting}, which reports the practical effect of changing word-level NMT to subword NMT using BPE. They report that the ultra-low resource setting is the one that benefited the most, going from 7.2 BLEU to 16.6 using the base model of \citet{bahdanau2016neural}. 

\label{subword_variations}
\subsection{Subword variations}

Some research was also performed towards using character-level RNNs only when word-level embeddings generate a OOV word in \citet{DBLP:journals/corr/LuongM16}. Switching to character-level also has its consequences, since splitting into characters result in longer sequences where each symbol contains less information, yielding computational and modelling challenges. The total memory required to represent a set of words in character-level increases substantially when compared to subword or word-level.

Sticking to an unique level of embedding was also one of the paradigms broken in the work of \citet{chen-etal-2018-combining}, where different levels of granularity (word, subword and character) are combined to augment the token representations. They extend the encoder with a character attention mechanism to learn better source-side representations and incorporate information of source-side characters into the decoder with multi-scale attention, allowing the character-level information to cooperate with word-level to optimize the translation. Results show that such models when compared to single granularity ones achieve higher scores, and also using multiple levels also benefits to address the OOV issue in one of their experiments.

Pre-trained word embeddings also followed the subword trend with the work published by \citet{DBLP:journals/corr/BojanowskiGJM16}, which became popular under the name of \mbox{Fast Text}. This algorithm is inspired on \mbox{Word2Vec} using the skipgram variant. It treats words as a bag of character n-grams and adds the special symbols ">" and "<" at the beginning and end of the words, to allow distinguish prefixes and suffixes from other character sequences. A visual distribution of the n-grams derived from the word "eating" is available at Table \ref{fast_text_ngrams}, where $n=6$.

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.6]{chapter3_and_4_images/fast_text_ngrams.png}
\caption{N-grams generated by \mbox{Fast Text} for the word "eating"}
\label{fast_text_ngrams}
\end{figure*}

Basically, each word is assigned a number of n-grams, and in practice only n-grams for $3 \leq n \leq 6$ are extracted for each word. Finally, a word is represented by the sum of the vector representations of its correspondent set of n-grams. When learned, that word is represented by an index and a respective set of n-grams, which are called subword tokens.

\section{Transfer Learning}

Transfer Learning is a common method in Machine Learning where a model developed for a specific task is reused as the starting point for a model in another related task, typically reducing the amount of required training. This technique was used by \citet{zoph-etal-2016-transfer}, where the weights of a high resource language pair are transferred to another unrelated low-resource language pair, both having English as the target language. By following this approach, they enable a NMT model to outperform the previous state-of-the-art Syntax-Based Machine Translation (SBMT) model. The metric used for evaluation was BLEU, and this happened in one of the four low-resource language tasks: the one applied to the Hausa language.

Another approach to transfer learning that optimizes the process of learning word vectors is the use of pre-trained word embeddings. This is achieved by applying language models to adjust and learn these vectors within a N dimensional space, represented by embedding dize. The learned representations (weights) are reused by the NMT model as a starting point, instead of the random initialization that usually happens when the model is trained from scratch.

The \mbox{Fast Text} algorithm presented in Section \ref{subword_variations} was chosen as the transfer learning candidate in this work. It was chosen because it carries the benefits of subword embeddings, analogous to the BPE features explained. Using such vectors as a warm start for the NMT model inherently enables it to be more robust against the OOV issue. The representations loaded also carry semantical and syntactical properties between words, which were learned by the pre-trained language model.

An interesting work that analyses the effect of using pre-trained word embeddings in low resource scenarios is performed by \citet{qi-etal-2018-pre}. The analysis is applied on the same dataset manipulated in this work, TED Talks, which is going to be better described later. However, they use a model based on \citet{bahdanau2016neural} which is trained in a framework different than ours and initialized through a different technique. 

One of the experiments analysed the effect of loading pre-trained word embeddings on reduced datasets, using 3 distinct languages. They report that there is a "sweet spot" for the amount of data available to train where they are the most effective. When loaded to a model trained only on 10\% of TED Talks dataset, pre-trained word embeddings yield small gains when compared to random initialization. However, when trained on 30\% of the dataset, the BLEU gains suffer a peak, which monotonically decreases with an increase in the proportion of the dataset used for training. 

Another experiment tries to enforce consistent embedding spaces across both languages, a technique called word embedding alignment. They use an approach that learns orthogonal transformations that convert the word embeddings of multiple languages to a single space, and use these aligned embeddings instead of independent ones. Unfortunately, mixed results were reported, which makes it hard to derive conclusions regarding its effectiveness. Their last experiments show that pre-trained word embeddings seem to be more effective when applied to more related language pairs (e.g. Portuguese-Spanish). 

Aiming to transfer the knowledge from language models trained on monolingual data to NMT, \citet{gulcere-language-model-nmt} shows 2 attempts to achieve this task: shallow fusion and deep fusion. The first uses a language model during decoding, it will be used to rescore the candidate words that the translation model is considering as the next word to be predicted. The other combines decoder and language model which are coordinated with a controller mechanism. Unfortunately, the improvements reported are limited to the range of 0.5 up to 2 BLEU points, in all experiments.

The results reported in the work done by \cite{qi-etal-2018-pre} for several language pairs in a range of scenarios are more consistent, while the technique presented at \cite{zoph-etal-2016-transfer} is highly language dependent, dataset dependent and subject to high deviation. Therefore, there is some evidence that the choice of pre-trained word embeddings tends to be safer than using transfer learning considering a model trained on a different dataset.

\section{Data Augmentation}

The amount of parallel data is key to the success of NMT, and data-hungry complex models provide limitations regarding data availability for researchers contributing to this field. Unfortunately, large-scale parallel corpora is not available for the majority of the existing language pairs. On the other hand, it is much easier to obtain monolingual corpora. The internet is a concrete example with a diverse selection of websites in multiple languages that corroborates this argument. This scenario motivated the study of data augmentation strategies as a supportive method to NMT.

\subsection{Back Translation}

A branch of research in Data Augmentation studies variations of the technique referred to as Back-Translation (BT)\abbrev{BT}{Back Translation}. In \citet{DBLP:journals/corr/SennrichHB15a}, the authors extract target-side monolingual data, train a model to translate back to the source language and use it to build synthetical translations. Figure \ref{back_translation_figure} represents the augmentation process with a simplified representation. A NMT model is used to translate in the reverse direction (target to source language) when compared to the original dataset, generating artificial sentence pairs that are appended to the original dataset to train another NMT model. 

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.24]{chapter3_and_4_images/back_translation.png}
\caption{The Augmentation of a dataset with Back Translation}
\label{back_translation_figure}
\end{figure*}

BT has shown to be a simple, yet effective method to address low data availability in many domains, as shown in \citet{DBLP:journals/corr/abs-1804-06189}. The study concludes is that there is a trend of increasing translation performance (measured with BLEU) with a certain amount of synthetic translations added, but it appears to tail off when the dataset balance is tipped too far in favour of the synthetic data.

In theory, the reason why performance increases with artificial sentence pairs generated by another model is that when generating new synthetic pairs, "noise features" are added, and they can be beneficial for learning purposes. This sort of feature engineering process in NMT is can only benefit models' performance in some cases. This assumption usually holds when the share of synthetic sentences is limited to a small portion of the whole dataset, keeping the "natural" sentences as the majority. It is hard to define a rule to determine this ideal share, since it varies in range depending on the task languages and text characteristics. Adding too much noise can compromise models' performance, so this technique should be used with caution.

Ways of generating synthetic data are presented and compared in the work presented by \citet{DBLP:journals/corr/abs-1906-03785}. The study evaluates several augmentation methods in scenarios with datasets that pair English (ENG) with high resource languages (HRL) and low resource languages (LRL). These methods use techniques such as pivoting, word substitution (with the use of a dictionary) and even employ an unsupervised NMT approach. A two-step pivoting method is also introduced: it uses a ENG-HRL model to create artificial HRL examples, and then use a HRL-LRL for to create artificial sentences for LRL. The authors propose methods to perform HRL-LRL translation that can benefit from characteristics of the language such as word order, later claiming that this ends up approximating better the true LRL data than converting with a ENG-LRL model. This technique is particularly useful for languages that have even less available resources than Portuguese. 

In some scenarios their work report that methods such as two-step pivoting may outperform back-translation. However, the study is limited to a few datasets and the gains reported in BLEU aren't expressive. As there are consistent results shown by applying the traditional back-translation in other works in a wider range of datasets, it was preferred over the two-step pivoting for the low-resource context of this work.

\chapter{Experiments on Neural Machine Translation}

The design of the experiments presented in this work aim to not only relate the score increases of applying technique $X$ or $Y$, but also seek to answer qualitative hypothesis about the translations generated. Being able to explain which errors are correlated to a specific technique is a process often overlooked in the NLP field, that usually minimizes the evaluation of performance to one or more scores. Questions like these are complex to explain using traditional approaches:

\begin{enumerate}
    \item How switching to subword level influences the bias behind the types of errors a model makes?
    \item Is sentence complexity or the incorporation of external knowledge a determining factor for performance? Do they induce qualitative biases?
    \item Is it beneficial to extend the dataset with synthetic data (back translation)? How such noise affects the errors patterns of a model?
\end{enumerate}

These questions require a deeper analysis that goes over just matching words between the reference and the translations: they demand a universal categorization of the errors that a model can produce. In order to achieve this, a multidimensional criterion to evaluate translation quality is created. The hypothesis raised here is that by analysing such patterns, we enrich the potential insights that can be extracted from an experiment. 

Another goal of the same analysis is to understand the impact on translation quality of different sentence complexity levels, and this is reflected on the choice of datasets. Datasets from different domains were also exploited in order to validate qualitative hypothesis regarding the effectiveness of using distinct domains on BT. Other relevant factors that influenced the choice of datasets were limited GPU memory and data availability constraints. In the following sections the setup of the experiments performed is detailed, stating the qualitative or quantitative hypothesis and comparing the expected outcome with the actual results.

\section{Methodology}
\label{methodology}

Datasets with low to medium complexity levels were carefully picked for the proposed analysis: Tatoeba \cite{DBLP:journals/corr/abs-2010-06354} and TED Talks \cite{cettolo-ted-talks}. Both represent a low-resource scenario due to the scarce number of sentences, in alignment with some references \cite{sennrich-zhang-2019-revisiting} \cite{zoph-etal-2016-transfer}. Tatoeba is mainly constituted of basic to intermediate level english sentences, which can be interpreted as a "school domain", ranging from elementary to middle school education. The Tatoeba dataset contains 143.8k small and basic to intermediate English level sentences, posing a low complexity challenge for the NMT task. It includes 26.3k unique words in PT and 15.3k in EN.

TED Talks is a medium-size dataset covering a range of subjects, including from low to high complexity sentences. It is a mixed domain dataset, since it contains talks from experts on distinct areas. News Commentary v16 \cite{TIEDEMANN12.463} is a news focused domain monolingual dataset, used for BT in this work, and includes a rich range of sentences in terms of content and complexity.

%  \textcolor{red}{2. deveria colocar a métrica de palavras totais e palavras unicas pro dataset de ted talks?}

In the experiments, 10\% of Tatoeba was held out for testing, while the remaining data was split into 10\% for validation and 90\% for training, using a seed equal 0. Despite TED disposing predefined training, test, and validation sets, the original validation set is too small (906 sentences), leading us to move the last 20 talks (2081 sentences) from the training set to this set. As a result, the training set contains 236.1k (1918 talks) sentences, randomly sampled to defining training batches using a seed equal to 157, and the test set includes 11.4k sentences. Additionally, all text was pre-processed to eliminate all XML enclosed sentences and tags, except for the ones related to title and description.

The experiments were conducted using Python, Gensim was the library used to deal with word embeddings, and Spacy's Portuguese and English tokenizers were applied to the mentioned datasets. All experiments were performed on a single GPU, using Google Colaboratory and Kaggle infrastructure. Typically such environments dispose of NVIDIA GPUs like Tesla P100, Tesla K80 or Tesla T4, with a GPU memory ranging from  12GB to 16GB. 

Transformers is the architecture adopted in all experiments, and the parameters tuned for them are $d_{{model}}=256$, $d_{{ff}}=256$, 8 attention heads, and Q, K and V square matrices with dimension 64. The pre-trained \mbox{Fast Text}-based models, which employed embeddings described in \cite{DBLP:journals/corr/abs-1708-06025} for Portuguese and \cite{fares-etal-2017-word} for English are exceptions. Both embeddings had  $d_{{model}}=300$ and 6 attention heads. The weights taken from \citet{fares-etal-2017-word} were pre-trained on the Gigaword corpus, they can be obtained from the ID 16 option on the download list available at their website. All variants adopted the Adam optimizer with $\beta \in (0.9, 0.98)$ and $\epsilon = 10^{-8}$, a learning rate of $10^{-4}$ and the beam search considered a beam with size 3. The early stopping criterion was based on the validation perplexity behaviour for ten epochs, halting the training in case of performance stagnation. During training, a new model was saved each time the best validation perplexity was achieved.

Regarding the BPE implementation, a toolkit developed by Google was preferred due to an interesting feature added over the original algorithm: it allows a priori vocabulary size definition instead of using the traditional number of merges. Sentencepiece \cite{DBLP:journals/corr/abs-1808-06226} is a language agnostic fast and lightweight implementation made in C++, widely used for subword segmentation. It was initialized considering a maximum vocabulary size of 32k tokens.

To assess quantitative performance, Sacrebleu \cite{post-2018-call} and NLTK \cite{Loper02nltk:the} were the BLEU variants exploited in our experiments. The major difference between them resides in a stronger Sacrebleu's penalization over cases where the translated and reference sentences differ in length.

\section{Quantitative study}

To understand and evaluate the translation outcomes derived from a NMT model, both quantitative and qualitative aspects must be taken into account. To evaluate the former, scores such as Sacrebleu and NLTK BLEU provide distinct optics that allow to isolate the sentence length penalization variable. For the latter, the knowledge of an expert may help to break the problem into smaller parts, categorizing quality issues with the goal of capturing error biases. 

The main goal for the experiments in this section is to evaluate how the candidate techniques react to low-resource conditions superficially. They should reveal which of them can help to address issues that arise in such environment and how to tune them based on a set of hyperparameters and setup conditions. To aid the reader on the task of following the rationale, a summary with the descriptions of all the experiments performed is provided here to conduct the evaluation process:

\begin{enumerate}
    \item Impact of restricted dataset content
    \begin{itemize}
        \item The experiment described in \ref{restricted_content_section} evaluates quantitatively the effect of several adverse limited data conditions on NMT performance
    \end{itemize}
    \item Effects of switching to subword level and incorporating external knowledge
    \begin{itemize}
        \item In \ref{subword_external_knowledge}, a discussion regarding ways of loading pre-trained word embeddings and the effectiveness of switching to subword level (BPE) standalone is made. The experiments consider \mbox{Fast Text} and BPE additions to the Transformer
    \end{itemize}
    \item Effectiveness of back translation to address low-resource and its constraints
    \begin{itemize}
        \item The experiment shown in \ref{bt_conditions} evaluates the performance impact of using BT to extend a dataset, considering different synthetic shares for the dataset and applying for same or different domain data
    \end{itemize}
    \item Comparison of Transformer candidates against the Google Translate benchmark
    \begin{itemize}
        \item The last experiment \ref{google_translate} compares the BLEU score of the aforementioned low-resource Transformer candidates with the output of Google Translate, considering the model at the date of 11/01/2022
    \end{itemize}
\end{enumerate}

\subsection{Impact of restricted dataset content}
\label{restricted_content_section}

Issues derived from training with constrained data may affect the model performance in distinct levels, therefore different low-resource scenarios were created to understand such effects To shed light on such limitations, a hypothetical experimental scenario was considered, where only a fraction of TED and Tatoeba training sets were used in training. The following percentages were explored: 33.3\%, 50\%, 66.6\%, 83.3\%. 

The expectation for this experiment is to validate if there is a minimum amount of sentences that a parallel dataset should have to reach its potential. It should become clear that after a certain size (or fraction, in this case) the additional performance obtained is minimal, so other alternatives should be explored rather than just increasing the amount of training data. Table \ref{restricted_content} summarizes the outcomes of this experiment.

\begin{table}[h]
\caption{Restricted data availability scores}
\label{restricted_content}
\fontsize{10}{12}\fontfamily{phv}\selectfont
\begin{tabular}{| p{0.12\linewidth} | P{0.1\linewidth}  | P{0.07\linewidth}  | P{0.07\linewidth}  | P{0.07\linewidth}  | P{0.1\linewidth}  | P{0.07\linewidth}  | P{0.07\linewidth}  | P{0.07\linewidth} |}
    \hline
    \multirow{2}{2.0cm}{Fraction of the Dataset} & \multicolumn{4}{|c|}{Tatoeba} & \multicolumn{4}{|c|}{TED}\\
    \cline{2-9}
    & Sacrebleu & NLTK BLEU & Batch Size & Epochs & Sacrebleu & NLTK BLEU & Batch Size & Epochs \\
    \hline
    33.3\% & 48.64 & 67.09 & 512 & 76 & 24.7 & 57.65 & 30 & 40\\
    \hline
    50\% & 52.53 & 70.12 & 512 & 65 & 25.18 & 56.46 & 30 & 40\\
    \hline
    66.6\% & 55.3 & 72.12 & 512 & 58 & 26.22 & \textbf{56.81} & 29 & 36\\
    \hline
    83.3\% & 56.24 & 73.18 & 512 & 58 & \textbf{26.74} & 56.57 & 28 & 30\\
    \hline
    100\% & \textbf{57.99} & \textbf{74.07} & 512 & 58 & 25.24 & 55.36 & 28 & 30\\
    \hline
\end{tabular}
\end{table}

Results show that the Sacrebleu scores for Tatoeba were about twice the same achieved with TED, corroborating with the much higher complexity of the latter. The BLEU metrics for both datasets have shown a monotonic behaviour, with exceptions to TED in two cases: Sacrebleu (100\%  $\times$ 83.3\%) and NLTK (100\% $\times$ 83.3 and 83.3\% $\times$ 66.6 \%). The reasons for such findings may include: (1) the possible use of synonyms in the translations, an aspect ignored by any BLEU metric; (2) a higher incidence of Repetition errors due to data quality issues (to be discussed further in section \ref{subjective_evaluation}); (3) the more complex and richer TED content, which might have led to a wider subject coverage in the training set, reducing model accuracy, a hypothesis deserving a future investigation. 

Models developed with a fraction of the original training datasets ($66.6\%$) performed surprisingly well, with the peak happening at $100\%$ and $83.3\%$ for Tatoeba and TED, respectively. This indicates that the amount of data provided at $66.6\%$ for the dataset content may be sufficient for the algorithm to not suffer from low-resource limitations, but a more accurate percentage for each case deserves further investigation.

\subsection{Effects of switching to subword level and ways to incorporate external knowledge}
\label{subword_external_knowledge}

Aiming to evaluate the leveraging effects of pre-trained \mbox{Fast Text} and BPE \cite{DBLP:journals/corr/SennrichHB15} strategies in low-resource NMT tasks, BPE models were implemented in the Texar framework  \cite{hu2019texar} (PyTorch version). In contrast, the alternative models considered a customized PyTorch \cite{NEURIPS2019_9015} solution. 

In practical terms, there are a number of alternatives to apply pre-trained word embeddings for NMT, and understanding the most effective approach can only be achieved experimentally. Embedding layers are disposed in sequence to sequence algorithms in 2 different parts of the architecture: inside the encoder or the decoder. The translation task is typically applied to a pair of distinct languages, hence one embedding will replace the source language weights learned by the encoder and an embedding of another language will do the same for the decoder. Another option is to replace the randomly initialized weights in either the encoder or the decoder, inducing the model to be trained from a "partial warm-start". 

Apart from the embedding layer combinations, when loading the weights using the Gensim library, there are some optional parameters be tuned in the $load\_word2vec\_format$ function: $binary$ and $unicode\_errors$. These parameters are usually set to $False$ and $"strict"$, but its reference \citet{fares-etal-2017-word} recommends to load them using $False$ and $"replace"$. Both combinations were used to understand which would yield the best scores, but these parameters were only tuned and applied to the model trained on the TED Talks dataset. Table \ref{transfer_learning_subword} exhibits these results, reproducing the last line of Table \ref{restricted_content} to allow an easier comparison.

\begin{table}[h]
\caption{Transfer learning and subword embeddings translation results}
\label{transfer_learning_subword}
\fontsize{10}{12}\fontfamily{phv}\selectfont
\begin{tabular}{| p{0.18\linewidth} | P{0.09\linewidth}  | P{0.06\linewidth}  | P{0.06\linewidth}  | P{0.07\linewidth}  | P{0.09\linewidth}  | P{0.06\linewidth}  | P{0.06\linewidth}  | P{0.07\linewidth} |}
    \hline
    \multirow{2}{2.2cm}{Technique applied} & \multicolumn{4}{|c|}{Tatoeba} & \multicolumn{4}{|c|}{TED}\\
    \cline{2-9}
    & Sacrebleu & NLTK BLEU & Batch Size & Epochs & Sacrebleu & NLTK BLEU & Batch Size & Epochs \\
    \hline
    None & 57.99 & 74.07 & 512 & 58 & 25.24 & 55.36 & 28 & 30\\
    \hline
    \mbox{Fast Text} (encoder of \cite{DBLP:journals/corr/abs-1708-06025}, decoder random) & 56.96 & 69.91 & 512 & 50 & 24.07 & 61.69 & 30 & 45\\
    \hline
    \mbox{Fast Text} (encoder of \cite{DBLP:journals/corr/abs-1708-06025} + decoder Gigaword) & N/A & N/A & N/A & N/A & 24.54 & 61.76 & 32 & 89\\
    \hline
    \mbox{Fast Text} (encoder of \cite{DBLP:journals/corr/abs-1708-06025} + decoder Gigaword) with source parameters & N/A & N/A & N/A & N/A & 21.26 & 52.31 & 32 & 80\\
    \hline
    Subword BPE & \textbf{66.63} & \textbf{83.02} & 512 & 40 & \textbf{40.26} & \textbf{72.20} & 32 & 40\\
    \hline
\end{tabular}
\end{table}

Curiously, the use of \mbox{Fast Text} embeddings is associated with an unexpected performance drop for both datasets. Conversely, the gains observed with BPE, which also exploits subword embeddings, were impressive. The variants that followed source recommendations also had worse performance than the default parameters: a result that may serve as a feedback for the creators of such embeddings.

One hypothesis for the bad \mbox{Fast Text} performance is a possible overspecialization to other text domains, since it was produced with content mined by a crawler \cite{DBLP:journals/corr/abs-1708-06025}. The higher BPE gain in TED (15.02) compared to Tatoeba (8.64) signalizes the effectiveness of BPE in dealing with more complex NMT scenarios, especially regarding a more diverse vocabulary, avoiding OOV occurrences. 

\subsection{Effectiveness of Back Translation to address low-resource in distinct scenarios}
\label{bt_conditions}

One hypothesis that is still unanswered is whether the noise inserted by a model to generate synthetic pairs is beneficial to performance or not, and to what extent. The approach to solve this problem is experimental, since different domains, synthetic text ratios, natural languages and model characteristics constitute an unique scenario. The experiments described in this section are heavily inspired by questions such as: What is considered an ideal synthetic ratio to apply BT? How changing the augmentation domain impacts scores?

All the experiments focused on exploring Back Translation were restricted to the TED dataset, since Tatoeba's low complexity would have posed an easy challenge where the gains could be dilluted. Data augmentation was performed with synthetic sentences produced with the own TED (using its left out sentences) and with the News dataset. These experiments aim to verify if data augmentation can help to reach higher BLEU scores under different low data availability conditions. 

Some scenarios of comparable models were created to evaluate the use of different domains versus the original (non-synthetic) dataset. The motivation lies in understanding if the addition of extra data brings a boost in performance or even if it might hinder the score. To consider different synthetic ratios, the following candidates were chosen: the TED dataset with 50\%, 33.3\%, 16.6\% and 8.33\% of synthetic data, with all the artificial samples generated by a single EN-PT Transformer. It was trained on the entire TED dataset to generate the synthetic sentences, reaching 27.73 and 63.8 points for the Sacrebleu and NLTK, respectively. 

In addition to the comparable candidates, one extra BT variant was added with a bigger amount of sentences than the rest (120\%). This variant has 20\% of synthetic TED sentences, which makes it comparable with the 83.3\% + 16.6\% of TED version, since both of them contain the same synthetic ratio (20\%). The idea was to check if adding more synthetic data on top of the original dataset would enhance the previously reported scores. One last detail of this experiment is that the subset of back-translated sequences appended to the training sets was randomly sampled using the following seeds: 157 (TED) and 0 (News). Table \ref{tab:table3-bt-results} exhibits the results.

\begin{table}[h]
  \caption{TED Talks back-translation results}
  \fontsize{10}{12}\selectfont
  \label{tab:table3-bt-results}
  \centering
  {\footnotesize
  \begin{tabular}{| p{6.1cm} | p{1.4cm} |  p{1.8cm} | p{1.5cm} | p{1.1cm} | p{1.4cm} |}
    \hline
    Technique applied & Batch size & Epochs Trained & Sacrebleu & NLTK BLEU & Synthetic Ratio \\
    \hline
    None (Original TED) & 30 & 27 & 25.24 & 53.26 & N/A\\
    \hline
    \hline
    Reduction of TED to 50\% & 40 & 30 & 25.18 & \textbf{56.46} & N/A\\
    \hline
    BT (50\% of News synthetic examples) & 34 & 33 & 21.80 & 51.34 & 50\%\\
    \hline
    BT (50\% of TED synthetic examples) & 34 & 28 & \textbf{25.95} & 56.42 & 50\%\\
    \hline
    \hline
    Reduction of TED to 66.6\% & 36 & 29 & 26.22 & 56.81 & N/A\\
    \hline
    BT (33.3\% of News synthetic examples) & 34 & 27 & 24.12 & 53.77 & 33.3\%\\
    \hline
    BT (33.3\% of TED synthetic examples) & 34 & 27 & \textbf{27.54} & \textbf{58.95} & 33.3\%\\
    \hline
    \hline
    Reduction of TED to 83.3\% & 28 & 30 & 26.74 & 56.57 & N/A\\    
    \hline
    BT (16.6\% of News synthetic examples) & 34 & 29 & 31.28 & 63.30 & 16.6\%\\
    \hline
    BT (16.6\% of TED synthetic examples) & 34 & 27 & \textbf{34.62} & \textbf{64.61} & 16.6\%\\
    \hline
    \hline
    Reduction of TED to 91.67\% & 34 & 24 & 26.74 & 56.57 & N/A\\
    \hline
    BT (8.33\% of News synthetic examples) & 36 & 23 & 29.66 & 60.4 & 8.33\%\\
    \hline
    BT (8.33\% of TED synthetic examples) & 34 & 24 & \textbf{32.27} & \textbf{65.13} & 8.33\%\\
    \hline
    \hline
    Augmented TED (100\% of TED + 20\% of News) & 28 & 24 & 29.31 & 60.80 & 16.6\%\\
    \hline
\end{tabular}}
\end{table}
% \begin{table}[h]
%   \caption{TED Talks back-translation results}
%   \fontsize{10}{12}\fontfamily{phv}\selectfont
%   \label{tab:table3-bt-results}
%   \centering
%   {\footnotesize
%   \begin{tabular}{| p{6.1cm} | P{1.6cm} |  P{2.4cm} | P{1.5cm} | P{1.1cm} |}
%     \hline
%     Technique applied & Batch size & Epochs Trained & Sacrebleu & NLTK BLEU \\
%     \hline
%     None (Original TED) & 30 & 27 & 25.24 & 55.36\\
%     \hline
%     Reduction of TED to 50\% & 40 & 30 & 25.18 & \textbf{56.46}\\
%     \hline
%     BT (50\% of News synthetic examples) & 34 & 33 & 21.80 & 51.34\\
%     \hline
%     BT (50\% of TED synthetic examples) & 34 & 28 & \textbf{25.95} & 56.42\\
%     \hline
%     Reduction of TED to 66.6\% & 36 & 29 & 26.22 & 56.81\\
%     \hline
%     BT (33.3\% of News synthetic examples) & 34 & 27 & 24.12 & 53.77\\
%     \hline
%     BT (33.3\% of TED synthetic examples) & 34 & 27 & \textbf{27.54} & \textbf{58.95}\\
%     \hline
%     Reduction of TED to 83.3\% & 28 & 30 & 26.74 & 56.57\\    
%     \hline
%     BT (16.6\% of News synthetic examples) & 34 & 29 & 31.28 & 63.30\\
%     \hline
%     BT (16.6\% of TED synthetic examples) & 34 & 27 & \textbf{34.62} & \textbf{64.61}\\
%     \hline
% \end{tabular}}
% \end{table}

The clearest finding of this experiment is that the increasing score trend switches its behaviour, depending on the size of the synthetic portion with respect to the whole dataset. For a more severe restriction on the dataset size (50\%), using other-domain synthesized sentences is harmful to model performance, while own-domain synthesis resulted in a marginally better BLEU score. However, for a lower percentage of synthetic data, positive effects start to appear. Considering a significant restriction ($\approx 33 \%$), using the same domain sentences in back-translation led to a mild increase in both BLEU values when compared to the Original TED, signalizing that such "noisy" sentences may contribute to increasing translation quality. When applying an intermediate restriction ($\approx 16.6 \%$), both domain approaches are quite effective, resulting in models that largely surpass the model developed over original data. Finally, when considering a small restriction ($\approx 8.3 \%$), the monotonic behaviour of same domain augmentation scores switches its trend and starts decreasing when compared to the previous one, but still better than the benchmark. Regarding other domain augmentation, the score also decreases, following a similar trend to the same domain variant. It is important to emphasize that the scores of all variants were generated using the same test data, otherwise they wouldn't be comparable.

One highlight of the experiment was that the Transformer variant trained on a larger dataset somewhat behaved unexpectedly, suffering from a decrease in BLEU when compared to variants trained with the same synthetic ratio on smaller datasets. The decrease in BLEU was significant: more than 5 points (34.62 vs 29.31) when compared to the 83.3\% TED version. On the other hand, when considering the Transformer trained on the original dataset, the addition of 20\% of News data enhanced performance. One hypothesis that may explain this, but deserves further investigation, is that since the TED dataset contains content from mixed domains, the last piece of the dataset (about 16\%) may contain text whose vocabulary is pretty decorrelated when compared to the other part. The addition of more back translation data could also help to increase performance, but exploring this further will remain as a proposal for future work.

\subsection{Comparison of Transformer candidates against the Google Translate benchmark}
\label{google_translate}

Besides the main goal of tackling low-resource issues, the focus on being efficient and maintaining the best performance is still a matter of great importance. In order to evaluate the performance of the proposed models, being able to compare them against something is crucial. Therefore, Google Translate was chosen to accomplish this task as a benchmark. Table \ref{google-translate-comparison} shows the BLEU score results for the best model candidates among the previous experiments on the test set of TED, along with the benchmark itself.

\begin{table}[h]
  \caption{Performance evaluation of the best candidates against Google Translate}
  \fontsize{10}{12}\fontfamily{phv}\selectfont
  \label{google-translate-comparison}
  \centering
  {\footnotesize
  \begin{tabular}{| p{6.1cm} | P{1.6cm} |  P{2.4cm} | P{1.5cm} | P{1.1cm} |}
    \hline
    Model & Batch size & Epochs Trained & Sacrebleu & NLTK BLEU \\
    \hline
    Transformer (Original TED) & 30 & 27 & 25.24 & 55.36\\
    \hline
    Subword BPE & 32 & 40 & \textbf{40.26} & \textbf{72.20}\\
    \hline
    Reduction of TED to 83.3\% & 28 & 30 & 26.74 & 56.57\\    
    \hline
    BT (16.6\% of News synthetic examples) & 34 & 29 & 31.28 & 63.30\\
    \hline
    BT (16.6\% of TED synthetic examples) & 34 & 27 & \textbf{34.62} & \textbf{64.61}\\
    \hline
    Google Translate (used on 11/01/2022) & Unknown & Unknown & \textbf{52.19} & \textbf{78.74}\\
    \hline
\end{tabular}}
\end{table}

Considering the BLEU metric that takes length penalization into account, sacrebleu, our best model reached 77.1\% of Google's performance, using an infrastructure which is in many orders of magnitude inferior in terms of computational power and costs. When switching to the other metric that ignores the length, the gap between them reduces as the ratio goes up to 91.69\%. The current Google model is able to identify even disease acronyms, such as ACS (Acute Coronary Syndrome), one achievement that is almost impossible to reach when limited to an unique low-resource dataset. It is even able to predict translations having just part of the word as input.

Unfortunately, there is not much to be said regarding the underlying techniques that empower their model. The last papers available suggest that they invested in a RNN-based encoder and decoder with attention, but the type of attention mechanism is not mentioned. When looking at more recent information, based on a blog post by  \citet{google-ai-2020}, they grant a relevant increase in BLEU to back-translation techniques, specially for low-resource languages. The latest papers available are heavily outdated \cite{DBLP:journals/corr/JohnsonSLKWCTVW16} \cite{DBLP:journals/corr/WuSCLNMKCGMKSJL16} and the techniques have probably been deprecated or updated somehow. Recent updates were mostly reported in blogs, with the latest one summarizing their Artificial Intelligence achievements in 2021 \cite{google-ai-2022}, with some mentions to new google translate features, such as the efforts towards reducing gender bias.

Google Translate's model is probably built upon a carefully curated set of techniques and parameters. One possibility to bridge the gap between our models is to try a combination of the presented low-resource techniques, although there are no guarantees that their positive effects would necessarily be constructive when combined. As the main goal in this work is more focused on qualitative outcomes, and not in boosting performance score as far as possible, the energy was focused on being able to quantify and interpret translation bias. The next section provides more detail of such attempts.

\section{Qualitative study}
\label{subjective_evaluation}

Understanding a problem through the lens of a quantitative methodology can already yield a range of findings, but engineers often underestimate the potential of a qualitative approach to complement such results. In modern tech companies, both techniques are commonly used together to provide a clearer scenario of the what the problem is, as being restricted to only one of them means that some gaps can be left and open questions may remain unanswered. When speaking of product management, for instance, designers and analysts are the left and right arms of a manager, used to clarify the scenario in a qualitative and quantitative way, respectively. The same analogy can be applied here, where engineers as researchers when working on the Natural Language domain often refer to experts in linguistics to understand how to tackle and solve a given problem.

\subsection{Challenges of the qualitative analysis}

When performing Machine Translation, a number of language, model specific and data availability nuances might pose a challenge with potential to hinder the results of a qualitative analysis. Most of them are fairly widespread and already known by most researchers in this area, but depending on whether the dataset has been curated, how much of its biases have been identified and addresses, and also how well dimensioned it is; the struggle to create high quality translations can increase significantly. 

The first challenge are data quality issues, such as references that do not match the exact expected translations, and may even contain some English errors. Data quality issues have a significant impact on the ability to correctly evaluate the translations of a model, and of course, they are also scattered throughout the TED Talks dataset \cite{cettolo-ted-talks}. Such data quality issues can also intensify some known flaws of using BLEU for evaluation, such as the lack of ability to catch synonyms and increased losses in score due to human translation biases. Table \ref{reference_errors} shows some of those sentences.

\begin{table}[h]
  \caption{Selected references with errors}
  \fontsize{10}{12}\fontfamily{phv}\selectfont
  \label{reference_errors}
  \centering
  {\footnotesize
  \begin{tabular}{| p{0.35\linewidth} | P{0.35\linewidth} |  P{0.2\linewidth} |}
    \hline
    Original Sentence (PT) & Reference & Suggested Correction \\
    \hline
     A medida que fazíamos nossas futuras expedições Eu estava vendo criaturas em fontes hidrotermais e às vezes coisas que eu nunca tinha visto antes, às vezes coisas que ninguém tinha visto antes, que realmente não haviam sido descritas pela ciência no momento em as vimos e as \textbf{imaginamos}. & As we did some of our subsequent expeditions, I was seeing creatures at hydrothermal vents and sometimes things that I had never seen before, sometimes things that no one had seen before, that actually were not described by science at the time that we saw them and \textbf{imaged} them. & Switch \textbf{imaged} to \textbf{imagined}. \\
    \hline
    Fico feliz em dizer que não temos este tipo de situação – um cirurgião com quem conversei alguns anos atrás que tinha trazido para aqui gêmeos xipófagos a fim de separá-los, em parte para ficar famoso. & I'm glad to say we don't have the kind of situation \textbf{with} - a surgeon I talked to a few years ago who had brought over a set of conjoined twins in order to separate them, partly to make a name for himself. & Remove \textbf{with} from the reference. \\
    \hline
    Fomos para \textbf{São Paulo} onde os outdoors foram banidos. & We went to \textbf{San Paulo} where they have banned outdoor advertising. & Correctly represent the São Paulo city with its original name, without the tilde: Sao Paulo. \\
    \hline
\end{tabular}}
\end{table}

All models when trained show some sort of translation bias after evaluating a number of curated sentences, and usually this bias is not trivial to explain. Splitting this subjectivity into smaller parts that can be addressed by a multidimensional criterion also isn't straightforward. However, the better the answer found to this problem, the better the proposed analysis can provide insights about models' performance. This scenario raises the second challenge, that consists of the limitations regarding the breadth and depth of the qualitative criterion used. To address the complexity of defining such criteria, the help of an expert on the field was mandatory. This motivated us to hire someone with 9 years of teaching and translating experience in English, that describes himself as a C1 level english speaker. 

The participation of a translator in this work was a compensated interaction. In other words, he was called on demand to help, and the whole process was constrained by the budget available for it. Since all the interactions were conducted autonomously, without any type of sponsor, the amount of evaluated sentences and thus the generalization power of the analysis couldn't reach its full potential.

Part of the qualitative work was to understand whether complexity does influence the error patterns when generating translations or not. This led the translator to categorize complexity in different levels, following the CEFR scale \cite{COE}, which will better detailed in a dedicated section ahead. 

The creation of the qualitative multidimensional criterion turned out to be the third challenge: bias. Not the inherent translation biases of the reference, but the potential to bias the classification process of the qualitative criterion. The consulted translator defined some categorization ideas, but understanding his rationale and giving feedbacks while avoiding to not bias the final decision has demonstrated to be a very subtle process. 

The bias challenge was also inherent to the process of sampling sentences: samples had to be curated to match the expected complexities and reach a certain number of sentences per complexity. Random choices of sentences were picked when performing this task to avoid bias, it demonstrated to be a relevant challenge that demanded a lot of manual work. However, when getting near the pre-established goal of 15 sentences per complexity, the approach had to become more flexible. Whenever a pair of sentences was near the threshold between adjacent complexity levels, we had to discuss whether it could be switched to the level that required more samples or if it should remain. 

It is worth to emphasize that 100 distinct sentences were evaluated, but this number is multiplied by the number of models selected for the qualitative experiments, because each one performs its unique types of errors. An analogous discussion to the complexity samples happened specially during the first interactions for the qualitative criterion, since the goal was to make sure that all types of errors were covered in our sample of the population. The clusters should be well-scoped and within a range that did not have potential to hinder interpretability. The discussion was fruitful as it helped to better grasp the consistency of the complexity categories and the qualitative criteria that were used.

% During the production of the thesis, all the quantitative and qualitative interactions performed with the thesis advisor and translators respectively happened in parallel. Everything kept going while the main author also worked in private companies, given that this thesis is the result of a part-time enrollment in the electrical engineering masters course. The whole production process suffered from time and availability constraints because of those peculiarities, something we call a human factor low-resource limitation.

\subsection{The qualitative criteria}

In order to analyse model categories, 2 qualitative dimensions were created to enrich the hypothesis and conclusions of our analysis: sentence complexity and error patterns. One relevant hypothesis raised in the beginning that wasn't addressed was that error biases performed by the same model in different domains and text complexities may change. In order to understand and correlate both dimensions, it was necessary to add sentence complexity metadata and design experiments with this goal. Another intent behind complexity segmentation is that it also should help to better understand associations between text complexity and the error patterns of the techniques exploited. Finally, defining and understanding errors patterns are inside the core of this work, so it was considered as a mandatory dimension.

\begin{table}[h]
\caption{Description of the error classes created}
\fontsize{10}{12}\fontfamily{phv}\selectfont
\label{error_classes}
\centering
  {\footnotesize
  \begin{tabular}{| p{0.15\linewidth} | p{0.8\linewidth} |}
    \hline
    Error class & Description \\
    \hline
    Reference matching & A similar, related word was used in a context where it makes sense, but is different from the reference, or the order of words are switched. \\
    \hline
    Omission & Happens when the model simply ignores part of the source sentence, and it simply doesn’t appear in the model guess. \\
    \hline
    Out of context & The model uses a word that doesn’t fit in the surrounding context, and the translation loses cohesion. This error also contemplates nonexistent words created by BPE. \\
    \hline
    Verb tense & When conjugating a verb, the model misses to capture the right tense and changes it on its guess, generating an innaccurate translation. \\
    \hline
    Meaning deviation & The model translates a sequence of words that is way different from the expected translation. This usually happens when several word choice errors are committed and to preserve cohesion between words, the final sentence result significantly lose its meaning when compared to the source sentence. \\
    \hline
    Insertion & This category indicates that the algorithm has inserted words that are unnecessary given the context. \\
    \hline
    Repetition & Happens when the likelihood estimate of the next translation becomes biased, inducing the model to keep translating the same set of words more than once. \\
    \hline
    $<$unk$>$ & The traditional token that a model outputs whenever none of the words it knows would fit in a given context. \\
    \hline
\end{tabular}}
\end{table}

\subsubsection{Error classes}

To accomplish the goal of being able to discuss error biases, a multidimensional criterion was created along with a human translator to cluster the types of errors. For this criterion, eight categories were considered: Reference Matching, Omission, Out of Context, Verb Tense, meaning deviation, Insertion, Repetition and $<$unk$>$ errors. A detailed error description and their correspondent classes can be found at Table \ref{error_classes}.


Along with the presented errors, there is also the $<$unk$>$ error classification, which happens when the model sees a word during inference that wasn't presented during training. All the sentences evaluated were then classified under these 8 classes of errors. Table \ref{error_examples} shows some cherry-picked sentences and their correspondent evaluation regarding error classes.

\begin{table}[h]
  \caption{Examples of errors for each class}
  \label{error_examples}
  \centering
  {\footnotesize
  \begin{tabular}{| p{3.0cm} | P{2.8cm} | P{2.5cm} | P{4.5cm} |}
    \hline
    Original sentence & Reference & Model guess & English error diagnostic \\
    \hline
    E porque isso é verdade? & 
    And why is it true? & 
    And why is this true? &
    \textbf{Reference matching error}: Switched "it" for "this", without losing meaning. \\
    \hline
    Michael Specter: O perigo da negação da ciência. & 
    Michael Specter: The danger of science denial. & 
    Michael <unk>: the danger of science. & 
    1) Omission error: denial. 2) $<$unk$>$ error: Specter was interpreted as an unknown token. \\
    \hline
    Mas, na verdade, nós somos mudados. Somos marcados, sem dúvida, por um desafio, quer físicamente, emocionalmente, ou ambos. & 
    But, in fact, we are changed. We are marked, of course, by a challenge, whether physically, emotionally or both. & 
    But in fact, we're changed. We're <unk>, of course, by a challenge, whether we can physically, emotionally, or both. &
    1) \textbf{Omission error}: The model omitted "marked". 2) \textbf{Out of context error}: The model added "can", completely changing the original meaning of the sentence 3) \textbf{$<$unk$>$ error}: "marked" wasn't successfully translated. \\
    \hline
    Ganhamos um monte de prêmios. & 
    We've won a bunch of awards. & 
    We won a bunch of awards. We won a lot of awards. &
    1) \textbf{Verb tense error}: Switched Present Perfect "we've won" for Simple Past "we won". 2) \textbf{Repetition error}: "We won a lot of awards" was inserted, repeating the original reference's meaning. \\
    \hline
    E exigiu muita preparação, tivemos que construir câmeras e luzes e todo tipo de coisas. & 
    And it took a lot of preparation, we had to build cameras and lights and all kinds of things. & 
    And it was a lot of please, we had to build cameras and lights and all kinds of things. &
    \textbf{Meaning deviation error}: The model switched "it took a lot of preparation" for "it was a lot of please". \\
    \hline
    A mesma coisa, só que maior. & 
    Same thing, only bigger. & 
    The same thing, only bigger than that. &
    \textbf{Insertion error}: The model inserted "the" and "than that" in the translation. \\
    \hline
\end{tabular}}
\end{table}

The selected examples are less complex and more didactic. They contain simpler mistakes than the average, as the goal is just to consolidate the descriptions in practice. The occurrence of many errors in a small piece of text might constitute a hard error classification challenge, increasing the chance of misclassification bias. Another factor that reinforces this bias is sentence complexity, which is covered as one of the qualitative pillars of the analysis.

\subsubsection{Complexity categories}

To define the categories, random samples were selected from TED, analysed by the human translator and stratified according to the CEFR scale \cite{COE}. Due to dataset characteristics, this study was restricted to sentences classified as A1, A2, B1 and B2. The C1 and C2 levels had very low occurrence on the samples extracted for the translator: less than 5 samples in a total of 100 analysed sentences. As time and budget were natural limitations, the decision was to remove those from the experiments. From the 100 distinct evaluated sentences, 60 were used for the experiments performed in this work, with 15 belonging to each complexity category.

Understanding how to categorize sentences solely based on the category descriptions is a hard task. Table \ref{evaluation_examples} shows some examples that underwent the whole qualitative analysis, with complexity and error classification. It helps in the task of absorbing the scope of the qualitative criteria, since it has one sample per complexity and most of the errors patterns classified and their respective reason.

\begin{table}[h]
  \caption{Evaluation of some translated sentences}
  \label{evaluation_examples}
  \centering
  {\footnotesize
  \begin{tabular}{| p{2.2cm} | P{2.4cm} | P{2.0cm} | P{1.0cm} | P{1.8cm} | P{3.2cm} |}
    \hline
    Original sentence & Reference & Model guess & Model Type & Complexity and reason & English error classification \\
    \hline
    Você não muda nunca, né? & 
    You never change, do you? & 
    You don't change it, do you? & 
    Transformer with BPE & 
    A1 - Simple present & 
    1) Insertion: it was added. 2) Reference matching: Never/Don't are close in meaning \\
    \hline
    Tente perguntar coisas assim: "Como é que foi aquilo" ? & 
    Try asking them things like, "what was that like?" & 
    Try to ask things like this: "how is it that?" & 
    Transformer with BPE & 
    A2 - ING used as infinitive and intermediate level question in Simple Past & 
    1) Verb Tense: switched "try asking" (continuous) for "try to ask" (infinitive). 2) Omission: "them". 2) Insertion: "this". 3) Meaning deviation: "what was that like?" was changed to "how is it that?" \\
    \hline
    E vocês sabem, uma epifania é normalmente algo que encontramos que tínhamos deixado cair em algum lugar. & 
    And you know, an epiphany is usually something you find that you dropped someplace. & 
    And you know, an epiphany is normally something that we've found that we'd left somewhere. & Transformer 66\% of TED & 
    B1 - Simple present and simple past, with uncommon vocabulary & 
    1) Reference matching: usually/normally. 2) Meaning deviation and Verb tense: "we've found that we'd left somewhere" lost both its original meaning and tense \\
    \hline
    Ele tinha acabado de ouvir a primeira e a quarta sinfonia de Beethoven, e veio até o backstage se apresentar. & 
    He had just heard a performance of Beethoven's first and fourth symphonies, and came backstage and introduced himself. & 
    He'd just heard the first one and the fourth symphony of Beethoven, and came to the $<$unk$>$ to introduce him. & 
    Trans-former 33\% of TED & 
    B2 - Past perfect and past simple & 
    1) Omission: The model omitted "a performance of" that appears in the original sentence. 2) Unk error. 3) Insertion: "first \textbf{one}". \\
    \hline
\end{tabular}}
\end{table}

There are no clear hard rules that can describe the thought process when choosing a category or error class. Some patterns were found that inclined the evaluation towards a label, but a broader evaluation and richer exploratory analysis would be necessary to define more embracing and generic rules. Some other sentences are available in a separate repository for curiosity purposes\footnote{Detailed error descriptions and some evaluation samples can be found at \url{https://github.com/Art31/pt-nmt-low-resource.git}.}.

\subsubsection{Design of experiments}

The classification of error classes and complexity for all the 60 sentences culminated in a complex dataset, with lots of binary columns created to label each sentence regarding the occurrence of the qualitative categories. It isn't trivial to extract insights through traditional statistical analysis for this dataset in its raw form, as there are multidimensional associations that couldn't be identified or easily described. This issue motivated the use of dimensionality reduction and clustering techniques to meticulously unveil hidden multidimensional patterns. 

The set of experiments to clarify the translation biases was carefully thought to show evidence of some associations between the model candidate, sentence complexity and their respective error class patterns. To guide the task of following the qualitative rationale, below there is a summary of the experiments performed under the qualitative scope:

\begin{enumerate}
    \item Statistical hypothesis test for error proportions among models
    \begin{itemize}
        \item Multiple Fisher Exact tests are used to validate whether the combination of a technique with a model works better to mitigate some types of errors, or maybe if it is more prone to making an error than others or not
    \end{itemize}
    \item Hierarchical clustering analysis
    \begin{itemize}
        \item Multidimensional associations between the errors, models and sentence complexity are evaluated in this experiment that aims to reach deeper conclusions about the biases of each technique
    \end{itemize}
    \item Correspondence analysis
    \begin{itemize}
        \item This algorithm also drives multidimensional conclusions on the qualitative side, with a visual 2-D representation that enables interesting visual interpretations and complements the previous experiment
    \end{itemize}
\end{enumerate}

Only one of the Back Translation Transformers is considered for the qualitative experiments. The model referred to as BT is the Transformer trained on a dataset composed 83.3\% of the original TED and 16.6\% of the other slice of the same dataset subjected to BT.

\subsection{Error proportion analysis with Fisher Exact test}

The purpose behind this experiment is to understand if given the error occurrences we can assume that one variant is biased to a specific error class, with statistical significance. An appropriate hypothesis test was chosen for this goal, given the limited sample size: the Fisher Exact Test. Fifteen sentences from each complexity level were presented to all 4 candidate models: (1) the best Back Translation variant, trained on 83.3\% of TED augmented with 16.6\% of TED data, (2) the Transformer trained over a fraction of $33\%$ of TED, (3) the Transformer trained on $66.6\%$ of TED and (4) the BPE variant, trained over the entire dataset. 

The justification for using such candidates is based on their performance on previous experiments and the different techniques they apply. Recall that one of the hypothesis raised is whether the technique has potential to influence error patterns, so switching to the subword level and using data augmentation are interesting modifications with unknown qualitative impact. Besides analysing the BT and BPE variants for this goal, the reduced TED Transformers also were added to evaluate the effects of restricting the dataset size over the error patterns.

Table \ref{error_classes_per_model_and_complexity} shows the number of errors committed by each model, stratified by sentence complexity and error category. Considering the limitations of such analysis, such as the reduced sample and the availability of only one translator, all models performed quite similarly regarding the Omission and Verb Tense occurrence. Nonetheless, BPE stands out by having the lowest average occurrence of Insertion, Repetition and Omission errors. Curiously, the Back Translation variant has the highest occurrence of Out of Context and Reference Matching errors, which may be an influence of using of artificial data. Both Transformers trained on reduced TED have a very similar average occurrence for all the errors types, indicating that the low-resource constraints may have a diminished impact on their errors patterns.

\begin{table}[!htp]
\caption{Class-error ratios per dataset and sentence complexity.}
\fontsize{10}{12}\fontfamily{phv}\selectfont
\label{error_classes_per_model_and_complexity}
\centering
\begin{tabular}{| p{0.8cm} | P{1.29cm}  | P{1.45cm}  | P{1.0cm}  | P{1.0cm}  | P{1.0cm}  | P{1.35cm}  | P{1.0cm}  | P{1.0cm} |  P{1.0cm} |}
    \hline
    Model Name & Complexity & Reference Matching & Omission & Out of context & Verb tense & Meaning deviation & Insertion & Repetition & $<$unk$>$ error\\
    \hline
    \multirow{5}{1.2cm}{BT} & A1 & $\nicefrac{6}{15}$ & $\nicefrac{3}{15}$ & $\nicefrac{1}{15}$ & $\nicefrac{1}{15}$ & $\nicefrac{0}{15}$ & $\nicefrac{1}{15}$ & $\nicefrac{0}{15}$ & $\nicefrac{1}{15}$\\
    \cline{2-10}
    & A2 & $\nicefrac{13}{15}$ & $\nicefrac{7}{15}$ & $\nicefrac{2}{15}$ & $\nicefrac{4}{15}$ & $\nicefrac{3}{15}$ & $\nicefrac{6}{15}$ & $\textbf{\nicefrac{1}{15}}$ & $\nicefrac{3}{15}$\\
    \cline{2-10}
    & B1 & $\nicefrac{13}{15}$ & $\nicefrac{5}{15}$ & $\textbf{\nicefrac{3}{15}}$ & $\nicefrac{5}{15}$ & $\nicefrac{5}{15}$ & $\nicefrac{5}{15}$ & $\nicefrac{0}{15}$ & $\nicefrac{3}{15}$\\
    \cline{2-10}
    & B2 & $\textbf{\nicefrac{15}{15}}$ & $\textbf{\nicefrac{11}{15}}$ & $\nicefrac{0}{15}$ & $\textbf{\nicefrac{11}{15}}$ & $\textbf{\nicefrac{7}{15}}$ & $\textbf{\nicefrac{10}{15}}$ & $\textbf{\nicefrac{1}{15}}$ & $\textbf{\nicefrac{8}{15}}$\\
    \cline{2-10}
    & Average & 78.3\% & 43.3\% & 40.0\% & 35.0\% & \textbf{25.0\%} & 36.6\% & \textbf{3.3\%} & \textbf{25.0\%}\\
    \hline
    \multirow{5}{1.2cm}{33\% TED} & A1 & $\nicefrac{5}{15}$ & $\nicefrac{3}{15}$ & $\nicefrac{0}{15}$ & $\nicefrac{2}{15}$ & $\nicefrac{3}{15}$ & $\nicefrac{7}{15}$ & $\nicefrac{4}{15}$ & $\nicefrac{4}{15}$\\
    \cline{2-10}
    & A2 & $\nicefrac{11}{15}$ & $\nicefrac{7}{15}$ & $\textbf{\nicefrac{5}{15}}$ & $\nicefrac{4}{15}$ & $\nicefrac{4}{15}$ & $\nicefrac{6}{15}$ & $\nicefrac{2}{15}$ & $\nicefrac{5}{15}$\\
    \cline{2-10}
    & B1 & $\nicefrac{13}{15}$ & $\nicefrac{9}{15}$ & $\textbf{\nicefrac{5}{15}}$ & $\nicefrac{6}{15}$ & $\nicefrac{8}{15}$ & $\nicefrac{3}{15}$ & $\nicefrac{2}{15}$ & $\nicefrac{5}{15}$\\
    \cline{2-10}
    & B2 & $\textbf{\nicefrac{14}{15}}$ & $\textbf{\nicefrac{11}{15}}$ & $\nicefrac{4}{15}$ & $\textbf{\nicefrac{10}{15}}$ & $\textbf{\nicefrac{13}{15}}$ & $\textbf{\nicefrac{7}{15}}$ & $\textbf{\nicefrac{5}{15}}$ & $\textbf{\nicefrac{9}{15}}$\\
    \cline{2-10}
    & Average & 71.6\% & 50.0\% & 23.3\% & 36.6\% & \textbf{46.6\%} & 38.3\% & \textbf{21.6\%} & \textbf{38.3\%}\\
    \hline
    \multirow{5}{1.2cm}{66\% TED} & A1 & $\nicefrac{4}{15}$ & $\nicefrac{3}{15}$ & $\nicefrac{1}{15}$ & $\nicefrac{0}{15}$ & $\nicefrac{4}{15}$ & $\textbf{\nicefrac{7}{15}}$ & $\nicefrac{5}{15}$ & $\nicefrac{3}{15}$\\
    \cline{2-10}
    & A2 & $\textbf{\nicefrac{12}{15}}$ & $\nicefrac{8}{15}$ & $\textbf{\nicefrac{3}{15}}$ & $\nicefrac{3}{15}$ & $\nicefrac{3}{15}$ & $\nicefrac{6}{15}$ & $\nicefrac{2}{15}$ & $\nicefrac{4}{15}$\\
    \cline{2-10}
    & B1 & $\textbf{\nicefrac{12}{15}}$ & $\nicefrac{8}{15}$ & $\textbf{\nicefrac{3}{15}}$ & $\nicefrac{6}{15}$ & $\nicefrac{5}{15}$ & $\nicefrac{4}{15}$ & $\nicefrac{4}{15}$ & $\nicefrac{5}{15}$\\
    \cline{2-10}
    & B2 & $\textbf{\nicefrac{12}{15}}$ & $\textbf{\nicefrac{11}{15}}$ & $\textbf{\nicefrac{3}{15}}$ & $\textbf{\nicefrac{10}{15}}$ & $\textbf{\nicefrac{9}{15}}$ & $\textbf{\nicefrac{7}{15}}$ & $\textbf{\nicefrac{6}{15}}$ & $\textbf{\nicefrac{7}{15}}$\\
    \cline{2-10}
    & Average & 66.6\% & 50.0\% & 16.6\% & 31.6\% & 35.0\% & 40.0\% & \textbf{28.3\%} & \textbf{31.6\%}\\
    \hline
    \multirow{5}{1.2cm}{BPE} & A1 & $\nicefrac{5}{15}$ & $\nicefrac{0}{15}$ & $\nicefrac{0}{15}$ & $\nicefrac{1}{15}$ & $\nicefrac{2}{15}$ & $\nicefrac{4}{15}$ & $\nicefrac{0}{15}$ & $\nicefrac{0}{15}$\\
    \cline{2-10}
    & A2 & $\nicefrac{10}{15}$ & $\textbf{\nicefrac{9}{15}}$ & $\textbf{\nicefrac{6}{15}}$ & $\nicefrac{4}{15}$ & $\nicefrac{4}{15}$ & $\nicefrac{1}{15}$ & $\nicefrac{0}{15}$ & $\nicefrac{0}{15}$\\
    \cline{2-10}
    & B1 & $\nicefrac{13}{15}$ & $\nicefrac{8}{15}$ & $\nicefrac{1}{15}$ & $\nicefrac{8}{15}$ & $\nicefrac{4}{15}$ & $\nicefrac{3}{15}$ & $\nicefrac{0}{15}$ & $\nicefrac{0}{15}$\\
    \cline{2-10}
    & B2 & $\textbf{\nicefrac{14}{15}}$ & $\textbf{\nicefrac{9}{15}}$ & $\nicefrac{5}{15}$ & $\textbf{\nicefrac{9}{15}}$ & $\textbf{\nicefrac{9}{15}}$ & $\textbf{\nicefrac{8}{15}}$ & $\textbf{\nicefrac{1}{15}}$ & $\nicefrac{0}{15}$\\
    \cline{2-10}
    & Average & 70.0\% & 43.3\% & 20.0\% & 36.6\% & 31.6\% & 26.6\% & \textbf{1.6\%} & \textbf{0.0\%}\\
    \hline
\end{tabular}
\end{table}

Results from Table \ref{error_classes_per_model_and_complexity} underwent multiple Fisher Exact tests to evaluate if the differences observed between the error ratios of the two models are statistically significant. This analysis considered multiple 2x2 tables (one to each class of error), with rows defining the model and columns associated with the occurrence or not of some class of error. The significance level was set to 5\%; thus, the null hypothesis was rejected whenever the $p$-value was lower than $0.05$, representing a statistically significant difference. 

The test was applied considering on 2 different matrices, that consider two dimensions at a time: first, error classes and model variants, then error classes along with complexity category. They have been split into 2 series of tests, since the first comparison analyses mainly the hypothesis that applying techniques may influence the error patterns. The second batch addresses the question of whether we can assume that the error shares and occurrence per class fluctuate depending on the complexity of the sentence presented to the model or not.

Tests belonging to the first matrix have their statistical significant results displayed in Table \ref{fisher_results}. The results indicate that the models trained on reduced versions of the TED dataset have a tendency for reproducing the Repetition error more than the other techniques. Also, the meaning deviation error seems to be better addressed by the model that applied Back Translation, having a statistical difference when compared to the 33\% Transformer, which produced the poorest BLEU score of them all. When comparing the $<$unk$>$ error, the expectations of always rejecting the null hypothesis are met when the BPE Transformer is compared to any other variant, as it replaces $<$unk$>$ tokens for customized words. On top of that, the Back Translation Transformer also had a consistent difference when compared to the models trained on reduced versions of the TED dataset, for the same error class.

\begin{table}[h]
  \caption{Multiple Fisher Exact test error-model results}
  \label{fisher_results}
  \centering
  {\footnotesize
  \begin{tabular}{| p{1.5cm} | P{2.0cm} | P{3.5cm} | P{2.0cm} | P{1.6cm} | P{1.5cm} |}
    \hline
    Error class & Benchmark model & Behaviour & Challenger model & P-value & Odds Ratio \\
    \hline
    Meaning deviation &
    BT Transformer & 
    produces significantly \textbf{less} errors than & 
    33\% Transformer &
    $22*10^{-3}$ &
    $0.381$ \\
    \hline
    Repetition & 
    BT Transformer & 
    produces significantly \textbf{less} errors than & 
    33\% Transformer &
    $4*10^{-3}$ &
    $0.125$ \\
    \hline
    Repetition & 
    BT Transformer & 
    produces significantly \textbf{less} errors than & 
    66\% Transformer &
    $3*10^{-4}$ &
    $0.087$ \\
    \hline
    Repetition & 
    33\% Transformer & 
    produces significantly \textbf{more} errors than & 
    BPE Transformer &
    $98*10^{-5}$ &
    $16.319$ \\
    \hline
    Repetition & 
    66\% Transformer & 
    produces significantly \textbf{more} errors than & 
    BPE Transformer &
    $4*10^{-5}$ &
    $23.325$ \\
    \hline
    $<$unk$>$ & 
    BT Transformer & 
    produces significantly \textbf{less} errors than & 
    33\% Transformer &
    $17*10^{-5}$ &
    $0.146$ \\
    \hline
    $<$unk$>$ & 
    BT Transformer & 
    produces significantly \textbf{less} errors than & 
    66\% Transformer &
    $249*10^{-5}$ &
    $0.196$ \\
    \hline
    $<$unk$>$ & 
    BT Transformer & 
    produces errors, which is not the case of & 
    BPE Transformer &
    $57*10^{-3}$ &
    \infty \\
    \hline
    $<$unk$>$ & 
    33\% Transformer & 
    produces errors, which is not the case of & 
    BPE Transformer &
    $174*10^{-10}$ &
    \infty \\
    \hline
    $<$unk$>$ & 
    66\% Transformer & 
    produces errors, which is not the case of & 
    BPE Transformer &
    $701*10^{-9}$ &
    \infty \\
    \hline
\end{tabular}}
\end{table}

Regarding the second batch of tests that compares error patterns with complexity, a high number of them indicated a statistical significant difference, which corroborates the hypothesis that they have some correlation. In most error classes, except for the Repetition error, the complexities that are spaced more than 1 position further away have rejected the null hypothesis. These are the results listed per error class:

\begin{enumerate}
    \item Reference matching error: Found relevant differences in all tests, except for the pairs $A2|B1$ and $B1|B2$;
    \item Omission error: Found relevant differences in all tests, except for the pairs $A2|B1$ and $A2|B2$;
    \item Out of context error: Found relevant differences in all tests, except for the pairs $A2|B1$, $A2|B2$ and $B1|B2$;
    \item Verb tense error: Found relevant differences in all tests, except for the pair $A2|B1$;
    \item Meaning Deviation error: Found relevant differences in all tests, except for the pairs $A1|A2$ and $A2|B1$;
    \item Insertion error: Found relevant differences in all tests, except for the pairs $A2|B1$, $A1|B1$ and $A1|A2$;
    \item Repetition error: Didn't find any relevant differences in all tests;
    \item $<$unk$>$ error: Found relevant differences in all tests, except for the pairs $A2|B1$, $A1|A2$ and $A1|B1$.
\end{enumerate}

In summary, results show some evidence that indicates that our choice and criteria of segmenting per complexity are indeed relevant for understanding error patterns, otherwise most tests wouldn't show any relevant difference. Also, some techniques have exhibited translation bias when comparing specific errors classes. However, there may be some multidimensional associations hidden in data that cannot be inspected using hypothesis tests. In order to get further insights and become able to clarify more of the outlined hypothesis in the beginning of the chapter, experiments using exploratory multivariate techniques and clustering have been performed after this experiment.

\subsection{Hierarchical Clustering (HC) evaluation} $\abbrev{Hierarchical Clustering}{HC}$

Reputed as one of the classical unsupervised clustering techniques, the HC algorithm is widely used for segmentation, recommendation and even for exploratory analysis. In summary, it basically works by assigning each data point to a separate cluster and then calculates the distance between them with the goal of agglomerating (most commonly) and merging clusters that are nearby. Each merge can only happen once per cluster per iteration, and this process is repeated until no further merges can be made. 

To tune the algorithm and customize its results there are a few hyperparameters that must be chosen: it accepts a number of clusters or a distance threshold as input, the type of distance used to calculate the proximity matrix at each iteration (must be chosen carefully depending on the data type) and the linkage method, which defines the approach to which clusters are agglomerated. All the potential choices for these hyperparameters and the best circumstances to use them will be discussed in the implementation section.

\subsubsection{Why use HC for qualitative analysis?}

The main goal was to use a clustering algorithm to help spot patterns in the dataset, and there are many available options. As we aim for interpretability, K-Means stands out as a nice competitor, and both must be analysed under a set of criteria to decide upon the most appropriate one. Table \ref{clustering_comparison} shows a brief summary of the criteria used and the scenario each algorithm is subjected to.

\begin{table}[h]
  \caption{Comparison of clustering techniques}
  \label{clustering_comparison}
  \centering
  {\footnotesize
  \begin{tabular}{| p{2.5cm} | P{3.0cm} | P{3.0cm} | P{3.0cm} |}
    \hline
    Criteria & Hierarchical Clustering & K-Means & Outcome \\
    \hline
    Bias &
    Linkage algorithm inserts different biases in merge operations & 
    Uses Sum of the Squared Error (SSE) and assumes normally distributed clusters & 
    HC lets you adjust biases while K-Means constraints you, hence HC wins \\
    \hline
    Scalability &
    $O(n^{3})$ complexity & 
    $O(n)$ complexity & 
    K-Means wins (other parameters were removed from the big o notation) \\
    \hline
    Ease of finding optimal # of clusters &
    Can be inferred by distance threshold, the Dendogram helps you find the best number visually & 
    Has to perform several experiments and measure goodness-of-fit through errors & 
    HC's tools easy the task of finding the best number \\
    \hline
\end{tabular}}
\end{table}

When compared to its main benchmark, K-Means Clustering, HC takes away the burden of having to predefine the number of clusters to partition your data in advance, since it gives the option of inferring this number based on a distance threshold. There are visualization techniques for HC such as the Dendogram that helps you find the optimal number. Also, K-Means has a biased approach since it considers the Sum of the Squared Error (SSE) and takes the mean coordinates among all data points. This enables HC to be less biased than its competitor, despite the fact that the linkage parameter does insert some bias on the cluster formation. HC also requires high computational power as it contains a time complexity of $O(n^{3})$ versus $O(n)$ in K-Means, not taking into consideration other parameters that affect the performance of both in a similar way.

Overall, analysing HC, the step-by-step process of the algorithm is considered to be simple to interpret, easy to tune both bias and hyperparameters to find an optimal solution and easy to evaluate goodness-of-fit, but it comes at the cost of poor scalability. Since our dataset is quite small, we preferred the customization power despite sacrificing scalability.

\subsubsection{Implementation of Hierarchical Clustering}

Hierarchical clustering is a broad term that combine 2 families of algorithms: the clustering algorithms that work bottom up (agglomerative) and the algorithms that work top-down (divisive). The former is more commonly used, and the main difference between both is that the agglomerative variant sequentially merge single observations into groups of clusters, while divisive starts with all the data combined in a single cluster and then splits it according to a similarity metric.

To understand the agglomerative variant better, let the distance between 2 clusters $i$ and $j$ be defined by $d_{ij}$, while cluster $i$ contains $n_{i}$ objects. Consider also a matrix \textbf{D} representing the set of all remaining $d_{ij}$. Given that the dataset has a size N, the pseudocode for the algorithm is given in Algorithm \ref{ncss_hierarchical_table}.

\begin{algorithm}
\begin{algorithmic}[1]
\caption{Pseudocode for Hierarchical Clustering, taken from \citet{ncss_hierarchical}}
\label{ncss_hierarchical_table}
\begin{enumerate}
    \item Find the smallest element $d_{ij}$ remaining in \textbf{D}
    \item Merge the clusters $i$ and $j$ unifying all their elements, creating a new cluster $k$
    \item Calculate a new set of distances $d_{km}$, where $m$ represents any other cluster than $k$, using the following distance formula:
    \begin{equation}
    \label{hierarchical_equation}
        d_{km} = \alpha_{i}d_{im} + \alpha_{j}d_{jm} + \betad_{ij} + \gamma|d_{im} - d_{jm}|,
    \end{equation}
    these new distances replace $d_{im}$ and $d_{jm}$ in \textbf{D}. Also let $n_{k} = n_{i} + n_{j}$. Note that the linkage algorithms represent choices for $\alpha_{i}$, $\alpha_{j}$, $\beta$ and $\gamma$.
    \item Repeat steps 1 to 3 until \textbf{D} contains a single group made up off all objects, which would happen by choosing a distance threshold rather than a fixed number of clusters. This will require $N-1$ iterations and each iteration helps to build part of the Dendogram graph.
\end{enumerate}
\end{algorithmic}
\end{algorithm}

The choice of the distance used to calculate the matrix \textbf{D} is usually free, but depending on the linkage approach it may get constrained (e.g. in the ward linkage). The parameters presented in Equation \ref{hierarchical_equation} are configured depending on the linkage algorithm used. The choice of linkage reflects on how the clusters are merged and induces some sort of bias on how the formation of clusters converge. Some of the possible linkage choices are presented below:

\begin{enumerate}
    \item Single Linkage: It can be seen as a nearest neighbor type of clustering, where the distance between two groups is defined as the distance between their two closest observations. It tends to yield clusters in which individuals are added sequentially to a single group, and can lead to premature merging of groups with close pairs even if they are quite dissimilar overall. The coefficients used for the distance equation are $\alpha_{i}=\alpha_{j}=0.5$, $\beta=0$, $\gamma=-0.5$
    \item Complete Linkage: It determines the similarity of pairs that are the farthest from each other, also known as furthest neighbor method. This method usually yields clusetrs that are well separated, but outliers can cause merging of groups later than what is optimal. The coefficients of the distance equation are $\alpha_{i} = \alpha_{j} = 0.5$, $\beta = 0$, $\gamma = 0.5$.
    \item Simple Average: Also called as the weighted pair-group method, this algorithm defines the distance between groups as the average
    distance between each of their members. This distance is weighted, so that the two groups have a proportional influence on the final result. The coefficients of the distance are $\alpha_{i} = \alpha_{j} = 0.5$, $\beta = 0$, $\gamma = 0$. 
    \item Group Average: This linkage algorithm is known as the unweighted pair-group method, the distance between two groups is defined as the average between each of their members. The coefficients of the distance equation are $\alpha_{i} = \frac{n_{i}}{n_{k}}$, $\alpha_{j} = \frac{n_{j}}{n_{k}}$, $\beta=0$ and $\gamma=0$
    \item Ward's Minimum Variance: This method requires the euclidean distance to be used, because groups are formed so that the pooled within-group sum of squares is minimized. At each step, two clusters are fused which result in the least increase in the pooled within-group sum of squares. The coefficients of the distance equation are $\alpha_{i} = \frac{n_{i} + n_{m}}{n_{k} + n_{m}}$, $\alpha_{j} = \frac{n_{j} + n_{m}}{n_{k} + n_{m}}$, $\beta = \frac{-n_{m}}{n_{k} + n_{m}}$ and $\gamma = 0$
\end{enumerate}

\subsubsection{Interpretation of HC}

Before interpreting the results of Hierarchical Clustering applied to some data, one must guarantee that the cluster configuration to where it has converged to is indeed optimal. This is usually performed by setting a distance threshold and computing the all merge operations, then using a metric to validate which splits have better segmented data. One of the metrics available for this is the silhouette coefficient, which measures how similar a data point is within-cluster compared to other clusters. The equation to calculate the coefficient for a data point $i$ is given by

\begin{equation}
\label{silhouette_equation}
s(i) = \frac{b(i) - a(i)}{max\{a(i), b(i)\}},
\end{equation}
where $S(i)$ is the silhouette coefficient for the data point $i$, $a(i)$ is the average distance between $i$ and all the other data points under the scope of the cluster to which $i$ belongs and $b(i)$ is the average distance from $i$ to all the data points belonging to other clusters. Whenever $a(i)=b(i)$ or the cluster of $i$ contains only a single data point, the silhouette coefficient is set to 0, following the orientations of \citet{ROUSSEEUW198753}, the creator of the metric.

If the result of Equation \ref{silhouette_equation} is at its largest (close to 1), this implies that $a(i)$ is much smaller than the smallest dissimilarity $b(i)$. Therefore, we can say that $i$ is well-clustered, as the distance gap means that it has been assigned to a very appropriate cluster. Another situation occurs when $s(i)$ is about zero, which means that $a(i)$ and $b(i)$ are approximately equal, hence it is not clear whether $i$ should have been assigned to either cluster $A$ or $B$. The worst scenario takes place when $s(i)$ is close to $-1$, an indication that $a(i)$ is much larger than $b(i)$, so $i$ lies on the average closer to cluster $B$ than $A$. In this situation it makes more sense to assign $i$ to $B$, and it provides evidence that $i$ has been misclassified. Usually, when performing an analysis using this coefficient, a single Silhouette Coefficient is generated for the whole dataset, as the number provided is an average of the coefficients for all data points.

Another complementary tool that can help evaluate and understand this algorithm is the Dendogram, which is a tree diagram that generates a graph that allows to visualize the merge operations and the threshold distance upon which each potential cluster group prevails. The diagram usually works by stretching lines from the $x$ axis for distinct heights (the $y$ axis), where the height that they form a right angle represents the distance threshold where a cluster group gets merged into other groups, as shown in Figure \ref{dendogram_example}. Each line may represent a single data point or a set of data points. In the case of Figure \ref{dendogram_example}, the leftmost line of the red cluster contains only one observation, while the leftmost line of the blue cluster has 2 observations.

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.5]{chapter5_images/dendogram_distance_example.png}
\caption{Toy Dendogram example}
\label{dendogram_example}
\end{figure*}

The number of clusters that would be formed at a particular cutoff can be easily determined in Figure \ref{dendogram_example} by drawing a horizontal line at that value in the $y$ axis and counting the number of lines that horizontal line intersects. For instance, one line drawn at distance 1.5 would split the data into 4 different clusters. One way to obtain the optimal number of clusters is by drawing the cutoff line at the segment that contains the biggest distance, which is this case happens to be under the segment where 1.5 is.

In order to reach good results, the use of both techniques (Silhouette Coefficient and Dendogram) or even others available in the literature is recommended to find the best number of clusters. It is also recommended to perform an exploratory analysis in the resulting set of clusters, firstly to check if all clusters contain a reasonable amount of data points, otherwise they won't be representative. Then, if the clusters are representative, then take the average and other descriptive statistic metric from the dimensions clustered with the goal of understanding if the cluster distributions reveal multidimensional similarities between the data points or if they seem to have no reason for been grouped. If interpreting the result is hard, then maybe the chosen number of clusters isn't optimal or the algorithm may not be the best tool to segment the dataset. 

\subsubsection{Models and error classes}

To perform this experiment, all the 15 sentences extracted to match each one of the 4 complexities were labelled with binary columns that indicate the absence or presence of one error class. Every one of the 4 models had 60 sentences representing them, totalling 240. The original dataset contains one column for each type of error, along with columns that represent the models used and columns that represent the complexity to which each sentence was classified. However, to simplify our analysis, we have split the results of the same experiment into 2 pairs of interest: models compared to error classes and sentence complexity compared to error classes. In this section, the former one will be presented.

\begin{figure*}[h]
\centering 
\includegraphics[keepaspectratio,scale=0.4]{chapter5_images/hc_dendogram.png}
\caption{Dendogram of Hierarchical Clustering experiments}
\label{hc_dendogram}
\end{figure*}

Hierarchical Clustering has been applied to the dataset using the manhattan distance, which is appropriate for categorical data, that happens to be the case here since the columns representing the error classes are binary. The Linkage parameter has been chosen after running some experiments with different linkage types and numbers of clusters, with the goal of understanding better how the clusters were formed. The complete linkage approach was chosen since the other ones that are applicable using the manhattan distance had many clusters representing few data points, an issue that could potentially hinder our analysis. 

After choosing the complete linkage, the next step was to compute the whole Dendogram, which can be seen in Figure \ref{hc_dendogram}. The Dendogram has been truncated and as a consequence it grouped some of the data points, otherwise it would be hard to understand the lineage of merges and the cluster groups. Inspecting the graph induces the reader to a doubt regarding where to put the cutoff, since the distances are more or less the same for each cluster group. This is a consequence of using categorical data, it looks like the distances are binned and the number of clusters could not be inferred by graphical inspection in this case. Despite such limitations, there seem to have some option such as 2, 5 and 9 clusters.

Since using the Dendogram for determining the number of cluster didn't work, the silhouette coefficient has been calculated for some linkage approaches and a set of different clusters ranging from 3 to 10. The resulting coefficients are arranged in Table \ref{silhouette_results}. The group average linkage coefficients are shown just for comparison, since it didn't make sense to use them as they created disproportionate cluster groups where some clusters represent a few points and others contain the majority of the dataset. 

\begin{table}[h]
  \caption{Silhouette coefficients per linkage approach and number of clusters}
  \label{silhouette_results}
  \centering
  {\footnotesize
  \begin{tabular}{| p{2.5cm} | P{2.0cm} | P{2.0cm} |}
    \hline
    Linkage & Number of clusters & Silhouette coefficient \\
    \hline
    Complete & 3 & 0.1415 \\
    \hline
    Complete & 4 & 0.1442 \\
    \hline
    Complete & 5 & \textbf{0.2320} \\
    \hline
    Complete & 6 & 0.2199 \\
    \hline
    Complete & 7 & 0.2368 \\
    \hline
    Complete & 8 & 0.2665 \\
    \hline
    Complete & 9 & 0.2931 \\
    \hline
    Complete & 10 & 0.2930 \\
    \hline
    Group Average & 2 & 0.3449 \\
    \hline
    Group Average & 3 & 0.2659 \\
    \hline
    Group Average & 4 & 0.2349 \\
    \hline
\end{tabular}}
\end{table}

Considering the coefficients of Table \ref{silhouette_results} and also calculating the amount of sentences per cluster, we concluded that the most appropriate setup was the one using complete linkage and 5 clusters. If the number of clusters was increased, the complexity of interpreting the results also increases significantly, without a great compensation on the coefficient side. For instance, using 10 clusters would lead to an increase of only $0.061$ points (about 26\%) with a substantial increase in complexity when performing the analysis, while using 7 clusters would lead to an increase of $0.048$ (about 2\%). On the other hand, when choosing a smaller number of clusters such as 3, the decrease in the coefficient would be of $0.0905$ (about 39\%), which is a penalizing trade-off.

After fixing the amount of clusters and setting the experiment configuration, some descriptive metrics were generated for each cluster to help understand how the dataset has been segmented: the average incidence of errors among all error classes, the average occurrence of errors per error class and the share of sentences that belong to each of the models. All these metrics are exposed in Table \ref{error_model_incidence}.

\begin{table}[h]
  \caption{Prevalence of error classes and models per cluster}
  \label{error_model_incidence}
  \centering
  {\footnotesize
  \begin{tabular}{| p{2.0cm} | P{3.5cm} | P{1.2cm} | P{1.2cm} | P{1.2cm} | P{1.2cm} | P{1.2cm} |}
    \hline
    Dimension Name & Dimension Value & Cluster 0 & Cluster 1 & Cluster 2 & Cluster 3 & Cluster 4 \\
    \hline
    \multirow{8}{2.0cm}{Error class} & Reference Matching & 86\% & 74\% & 97.75\% & 41.67\% & 0\% \\
    \cline{2-7}
    & Omission & 46\% & 58\% & 48.31\% & 75\% & 20.51\% \\
    \cline{2-7}
    & Out of Context & 16\% & 2\% & 17.98\% & 100\% & 12.82\% \\
    \cline{2-7}
    & Verb Tense & 98\% & 52\% & 1\% & 58.33\% & 2.56\% \\
    \cline{2-7}
    & Meaning Deviation & 18\% & 62\% & 29.21\% & 91.67\% & 15.38\% \\
    \cline{2-7}
    & Insertion & 26\% & 72\% & 23.6\% & 50\% & 23.08\% \\
    \cline{2-7}
    & Repetition & 12\% & 40\% & 2.23\% & 8.33\% & 10.25\% \\
    \cline{2-7}
    & $<$unk$>$ & 2\% & 80\% & 14.6\% & 16.67\% & 2.56\% \\
    \hline
    \multirow{4}{2.0cm}{Model Name} & 33\% TED Transformer & 22\% & 30\% & 23.60\% & 33.33\% & 23.08\% \\
    \cline{2-7}
    & 66\% TED Transformer & 22\% & 38\% & 20.22\% & 16.67\% & 25.64\% \\
    \cline{2-7}
    & BPE Transformer & 34\% & 4\% & 26.97\% & 41.67\% & 30.77\% \\
    \cline{2-7}
    & BT Transformer & 22\% & 28\% & 29.21\% & 8.33\% & 20.51\% \\
    \hline
    \hline
    \multicolumn{2}{|c|}{Average error incidence among all classes} & 38\% & 55\% & 29.35\% & 55.21\% & 10.9\% \\
    \hline
    \multicolumn{2}{|c|}{Amount of sentences} & 50 & 50 & 89 & 12 & 39 \\
    \hline
\end{tabular}}
\end{table}

To complement the overall descriptive statistics derived from the cluster analysis, we have also generated a Table that contains all the types of errors in each cluster that a model has made, considering only those that reached a higher incidence than 50\% of all their sentences in that cluster. The error class for all models are shown if at least one model has performed the error for more than 50\% of its sentences. The goal of this analysis is to highlight the strongest associations between error classes and models, reduce subjectivity and reach more feasible conclusions. This relation is available in Table \ref{cluster_ranking_model_error}.

\begin{table}[t!]
  \caption{Rank of most frequent errors per model in each cluster}
  \label{cluster_ranking_model_error}
  \centering
  {\footnotesize
  \begin{tabular}{| p{3.2cm} | P{1.4cm} | P{1.8cm} | P{3.5cm} | P{0.9cm} | P{1.3cm} |}
    \hline
                   Error class &     Value &        Sentences &                    Model Name &  Rank &  Cluster \\
    \hline
         Verb tense  &  1.0000 &             11 &           33\% TED Transformer &     1 &        0 \\
    \hline
         Verb tense  &  1.0000 &             11 &           66\% TED Transformer &     2 &        0 \\
    \hline
         Verb tense  &  1.0000 &             17 &               BPE Transformer &     3 &        0 \\
    \hline
         Verb tense  &  0.9090 &             11 &                 BT Transformer &     4 &        0 \\
    \hline
 Reference matching  &  0.9090 &             11 &           33\% TED Transformer &     1 &        0 \\
    \hline
 Reference matching  &  0.8181 &             11 &           66\% TED Transformer &     3 &        0 \\
 \hline
 Reference matching  &  0.8823 &             17 &               BPE Transformer &     2 &        0 \\
 \hline
 Reference matching  &  0.8181 &             11 &                 BT Transformer &     4 &        0 \\
 \hline
          Insertion  &  0.8666 &             15 &           33\% TED Transformer &     2 &        1 \\
          \hline
          Insertion  &  0.6842 &             19 &           66\% TED Transformer &     3 &        1 \\
          \hline
          Insertion  &  1.0000 &              2 &               BPE Transformer &     1 &        1 \\
          \hline
          Insertion  &  0.5714 &             14 &                 BT Transformer &     4 &        1 \\
          \hline
         Verb tense  &  0.4000 &             15 &           33\% TED Transformer &     3 &        1 \\
         \hline
         Verb tense  &  0.3684 &             19 &           66\% TED Transformer &     4 &        1 \\
         \hline
         Verb tense  &  1.0000 &              2 &               BPE Transformer &     1 &        1 \\
         \hline
         Verb tense  &  0.7857 &             14 &                 BT Transformer &     2 &        1 \\
         \hline
  Meaning deviation  &  0.8666 &             15 &           33\% TED Transformer &     2 &        1 \\
  \hline
  Meaning deviation  &  0.5263 &             19 &           66\% TED Transformer &     3 &        1 \\
  \hline
  Meaning deviation  &  1.0000 &              2 &               BPE Transformer &     1 &        1 \\
  \hline
  Meaning deviation  &  0.4285 &             14 &                 BT Transformer &     4 &        1 \\
  \hline
              <unk>  &  0.8666 &             15 &           33\% TED Transformer &     1 &        1 \\
              \hline
              <unk>  &  0.7894 &             19 &           66\% TED Transformer &     3 &        1 \\
              \hline
              <unk>  &  0.0000 &              2 &               BPE Transformer &     4 &        1 \\
              \hline
              <unk>  &  0.8571 &             14 &                 BT Transformer &     2 &        1 \\
              \hline
           Omission  &  0.6000 &             15 &           33\% TED Transformer &     2 &        1 \\
           \hline
           Omission  &  0.5263 &             19 &           66\% TED Transformer &     3 &        1 \\
           \hline
           Omission  &  0.5000 &              2 &               BPE Transformer &     4 &        1 \\
           \hline
           Omission  &  0.6428 &             14 &                 BT Transformer &     1 &        1 \\
           \hline
 Reference matching  &  0.6000 &             15  &           33\% TED Transformer &     4 &        1 \\
 \hline
 Reference matching  &  0.6842 &             19  &           66\% TED Transformer &     3 &        1 \\
 \hline
 Reference matching  &  1.0000 &              2  &               BPE Transformer &     1 &        1 \\
 \hline
 Reference matching  &  0.9285 &             14  &                 BT Transformer &     2 &        1 \\
 \hline
 Reference matching  &  0.9523 &             21  &           33\% TED Transformer &     4 &        2 \\
 \hline
 Reference matching  &  1.0000 &             18  &           66\% TED Transformer &     1 &        2 \\
 \hline
 Reference matching  &  1.0000 &             24  &               BPE Transformer &     2 &        2 \\
 \hline
 Reference matching  &  0.9615 &             26  &                 BT Transformer &     3 &        2 \\
 \hline
  Meaning deviation  &  1.0000 &              4  &           33\% TED Transformer &     1 &        3 \\
  \hline
  Meaning deviation  &  1.0000 &              2  &           66\% TED Transformer &     2 &        3 \\
  \hline
  Meaning deviation  &  1.0000 &              5  &               BPE Transformer &     3 &        3 \\
  \hline
  Meaning deviation  &  0.0000 &              1  &                 BT Transformer &     4 &        3 \\
  \hline
          Omission  &  1.0000 &              4 &           33\% TED Transformer &     1 &        3 \\
          \hline
          Omission  &  0.5000 &              2 &           66\% TED Transformer &     4 &        3 \\
          \hline
          Omission  &  0.6000 &              5 &               BPE Transformer &     3 &        3 \\
          \hline
          Omission  &  1.0000 &              1 &                 BT Transformer &     2 &        3 \\
          \hline
         Verb tense  &  1.0000 &              4 &           33\% TED Transformer &     1 &        3 \\
         \hline
         Verb tense  &  0.5000 &              2 &           66\% TED Transformer &     2 &        3 \\
         \hline
         Verb tense  &  0.4000 &              5 &               BPE Transformer &     3 &        3 \\
         \hline
         Verb tense  &  0.0000 &              1 &                 BT Transformer &     4 &        3 \\
         \hline
     Out of context  &  1.0000 &              4 &           33\% TED Transformer &     1 &        3 \\
     \hline
     Out of context  &  1.0000 &              2 &           66\% TED Transformer &     2 &        3 \\
     \hline
     Out of context  &  1.0000 &              5 &               BPE Transformer &     3 &        3 \\
     \hline
     Out of context  &  1.0000 &              1 &                 BT Transformer &     4 &        3 \\
     \hline
\end{tabular}}
\end{table}

Although the cluster highlights and the ranking of error class incidence per model are interesting, it doesn't make sense to stop here since the experiment aims to interpret qualitatively the multidimensional relationships. In the bullet points below, statements and conclusions about the data previously presented in the tables are made. They aim to clarify some of the relationships that the algorithm helped to spot:

\begin{enumerate}
    \item Cluster 0: This cluster contains 50 sentences (20.8\% of the total). BPE has the biggest share of sentences (34\%), with the other models having 22\% each. The average occurrence of errors is 38\%, ranking 3rd place (exactly in the middle) when compared to the others. Verb Tense and Reference Matching stand out with an occurrence of 98\% and 86\% respectively, with the least frequent errors being $<$unk$>$ and Repetition with 2\% and 12\% each.
    
    It seems that cluster 0 is biased towards BPE due to its higher share, and it also concentrates the sentences with the highest occurrence of Verb Tense and Reference Matching, since all models made these errors in more than 80\% of their sentences. There are signals that these errors are quite horizontal and common to all models, specially to the dominant model (BPE). This cluster also has the lowest occurrence of the $<$unk$>$ error, which is compatible with the fact that BPE dominates this cluster, since the model is immune to this error.
    
    \item Cluster 1: It contains 50 sentences (20.8\% of the total). The 66\% Transformer has the highest share of sentences (38\%), followed by the 33\% Transformer with 30\%, BT with 28\% and BPE with 4\% (2 sentences). The average occurrence of errors is 55\%, ranking 2nd place with respect to the rest. The $<$unk$>$ (80\%), Reference Matching (74\%) and Insertion (72\%) errors stand out as the most frequent from the rest, while Out of Context error almost didn't happen (2\%) and Repetition (40\%) are the least occurring ones.
    
    Although this is the cluster with the second highest incidence of errors, the gap is narrow (0.21\%) when compared to the first place. It has a strong participation (68\%) of the Transformers trained on a reduced dataset (33\% and 66\%), with BPE and BT occupying less than a third, hence can tell that it is mostly represented by the former models. These 2 dominating models demonstrate a high occurence of Insertion, Meaning Deviation and $<$unk$>$ errors, losing to BT in the ranking when talking of Omission and Verb Tense. This cluster's error distribution difference to the first one are clear: it has the highest occurrence of the Repetition error and the incidence of the $<$unk$>$ error is much more expressive. The main characteristic that sets them apart is that in the first cluster BPE predominates. These distinct error distributions indicate a different bias for such models, which corroborates the hypothesis that the technique indeed has an impact on the translation behaviour. 
    
    \item Cluster 2: Its the biggest cluster with 89 sentences (37\% of the total). The models share almost the same quantity of sentences, with BT and BPE Transformers being the most occurring (29\% and 27\%) while the others have about the same slice of the rest. The average frequency of errors is 29\%, being the second with less mistakes. Reference Matching is almost ubiquitous (98\%), whereas curiously the other errors have all less than 50\% of occurrence. Verb Tense and Repetition almost don't happen with 2\% and 1\% respectively.
    
    The most representative cluster is also fairly equally distributed, and the high incidence of Reference Matching in this sample, ranging from 95.23\% to 100\% in the ranking, indicates that this is an error that these models can hardly avoid. In fact, this error is the most common among all the clusters and in the original dataset. It is known to be a flaw of the translation process and probably it can only be addressed by modifying the methodology (such as using references from several human translators), not by the technique or the model. Moreover, this cluster basically tells that in most of the dataset, the models do not perform the Verb Tense or the Repetition errors, but as their incidence varies a lot from cluster to cluster, we cannot claim this as a rule as they happen to other sentences.
    
    \item Cluster 3: It is a small cluster with 12 errors (5\% of the total), with BPE having the most sentences (42\%), followed by the 33\% Transformer (33\%) and a very small contribution from BT (8\%). The average frequency of errors is the highest, with 55.2\%, where Out of Context error is unanimous (100\%), followed by Meaning Deviation (92\%) and Omission (75\%). The least frequent errors are Repetition (8\%) and $<$unk$>$ (16\%).
    
    This cluster has a small share of the total sentences, but concentrates the ones that were the most difficult for all models, curiously with a higher share belonging to BPE. The errors that stood out in this cluster deviating from the share of the others were the Meaning Deviation error (91.67\% vs 42.85\% on average), Out of Context (100\% vs 29.76\% on average) and Omission (75\% vs 49.56\%). Anticipating one result of the complexity section, this cluster has about 75\% of its sentences belonging to B2 and B1, while the others had a lower share (from 66\% to 7.6\%) in these higher complexities, which explains the high error metrics here. The share of errors in this cluster doesn't weight much in the overall bias of the original models as it contains only 5\% of all sentences, hence its sample reduces the relevance of conclusions derived from interpreting its distribution.
    
    \item Cluster 4: A medium-sized 39 error cluster, encompassing the rest of the dataset with 16\% of the total. The BPE Transformer is the most influential contributing with 30\% of the sentences, while the rest is approximately equally distributed among the rest, with BT being the weakest (20.5\%). Errors have spawned the least in this cluster (10.9\%), with no errors occurring more than 50\% of the sentences. Insertion is the most frequent one (23\%), while Reference Matching does not happen at all, followed by $<$unk$>$ and Verb Tense, both with 2\%.
    
    This cluster contrasts with the last one as its low occurrence of errors happens as a result of the lower complexity of its sentences, with 84.6\% being A1. This may explain why no errors happen in more than 50\% of sentences and its low average incidence of errors. Regarding the scarcity of errors, the error classes that drive more attention are Reference Matching (0\% vs 59.88\% on average), Verb Tense (2.56\% vs 42.38 on average) and $<$unk$>$ (2.56\% vs 23.17\% on average).
\end{enumerate}

The results reported for this experiment attempt to elaborate qualitative hypothesis that justify the distributions and biases shown for each model. Despite the sample limitations and constraints established by the error classes created, it sheds some light on the impact that each technique has into the errors made by the model. There is some evidence that the reduced models tend to perform the Repetition and $<$unk$>$ errors, which may be associated with a lack of vocabulary sort of bias. Errors like Reference Matching seem to happen independently of the technique and Verb Tense is also horizontal, but usually more common when BPE is responsible for most sentences. Whenever the Transformers trained on reduced TED 33\% and 66\% appear, their share of sentences in a cluster tends to be the same, which can be an evidence that the algorithm sees some correlation between their translation biases. Also, curiously, whenever the Meaning Deviation error is present, the BT Transformer has the lowest occurrence, and it is also less present in clusters where BT has a higher share.

Some results already anticipated comparisons with the complexity analysis to provide a deeper understanding of the characteristics of each cluster, but the reader is still encouraged to go through the whole complexity analysis that will be presented in the next section. 

\subsubsection{Sentence complexity and error classes}

The same analysis and rationale presented in the last section applies to this experiment, the difference is that before we had ignored the presence of complexity as a dimension, but now we do this for the model variants and focus on relating complexity with error classes. Therefore, the qualitative results presented here had the same silhouette coefficient (0.2320) and used the same amount of clusters (5). 

To gain a better understanding of the distribution of each cluster, descriptive metrics such as the average incidence of errors among all error
classes, the average and median occurrence of errors per error class and the share of sentences that belong to each complexity type have been generated and are exposed in Table \ref{error_complexity_incidence}. The main difference between Tables \ref{error_complexity_incidence} and \ref{error_model_incidence} are the last 4 rows that show the complexity share in each cluster rather than model share. 

\begin{table}[h]
  \caption{Error class and complexity incidence per cluster}
  \label{error_complexity_incidence}
  \centering
  {\footnotesize
  \begin{tabular}{| p{2.0cm} | P{3.5cm} | P{1.2cm} | P{1.2cm} | P{1.2cm} | P{1.2cm} | P{1.2cm} |}
    \hline
    Dimension Name & Dimension Value & Cluster 0 & Cluster 1 & Cluster 2 & Cluster 3 & Cluster 4 \\
    \hline
    \multirow{8}{2.0cm}{Error class} & Reference Matching & 86\% & 74\% & 97.75\% & 41.67\% & 0\% \\
    \cline{2-7}
    & Omission & 46\% & 58\% & 48.31\% & 75\% & 20.51\% \\
    \cline{2-7}
    & Out of Context & 16\% & 2\% & 17.98\% & 100\% & 12.82\% \\
    \cline{2-7}
    & Verb Tense & 98\% & 52\% & 1\% & 58.33\% & 2.56\% \\
    \cline{2-7}
    & Meaning Deviation & 18\% & 62\% & 29.21\% & 91.67\% & 15.38\% \\
    \cline{2-7}
    & Insertion & 26\% & 72\% & 23.6\% & 50\% & 23.08\% \\
    \cline{2-7}
    & Repetition & 12\% & 40\% & 2.23\% & 8.33\% & 10.25\% \\
    \cline{2-7}
    & $<$unk$>$ & 2\% & 80\% & 14.6\% & 16.67\% & 2.56\% \\
    \hline
    \multirow{4}{2.0cm}{CEFR Complexity} & A1 & 4\% & 16\% & 19.10\% & 0\% & 84.62\% \\
    \cline{2-7}
    & A2 & 32\% & 20\% & 31.46\% & 25\% & 7.69\% \\
    \cline{2-7}
    & B1 & 42\% & 8\% & 32.58\% & 25\% & 7.69\% \\
    \cline{2-7}
    & B2 & 22\% & 56\% & 16.85\% & 50\% & 0\% \\
    \hline
    \hline
    \multicolumn{2}{|c|}{Average error incidence among all classes} & 38\% & 55\% & 29.35\% & 55.21\% & 10.9\% \\
    \hline
    \multicolumn{2}{|c|}{Amount of sentences} & 50 & 50 & 89 & 12 & 39 \\
    \hline
\end{tabular}}
\end{table}

As this experiment has an intersection of results with the last one, since the error class distribution is the same, the relations described here will be more concise to avoid redundancy when comparing to the results already reported. Looking at the distributions of sentece complexity per cluster, a first look indicates that the average error incidence is somewhat correlated to which complexity dominates in that cluster. The correlation between the error incidence and the share of complex sentencees (considering B1 and B2) reached 0.9248, which indicate that the segmentation performed is somewhat accurate and there is a strong evidence that the complexity also influences the translation bias.

Analogously to the analysis performed in the model-error section, a ranking of the complexities per cluster that had at least one error class with more than 50\% of occurrence has also been generated and is available in Table \ref{cluster_ranking_complexity_error}. The data exhibited in this table will be commented for each cluster to with the goal of extracting qualitative conclusions: 

\begin{table}[t!]
  \caption{Rank of most frequent errors per complexity in each cluster}
  \label{cluster_ranking_complexity_error}
  \centering
  {\footnotesize
  \begin{tabular}{| p{3.2cm} | P{1.4cm} | P{1.8cm} | P{3.5cm} | P{0.9cm} | P{1.3cm} |}
         \hline
         Error class         &     Value &        Sentences &                    Complexity &  Rank &  Cluster \\
         \hline
         Reference matching  &  1.0000 &             11 &         B2 &     1 &        0 \\
         \hline
         Reference matching  &  0.8750 &             16 &         A2 &     2 &        0 \\
         \hline
         Reference matching  &  0.8571 &             21 &         B1 &     3 &        0 \\
         \hline
         Reference matching  &  0.0000 &              2 &         A1 &     4 &        0 \\
         \hline
         Verb tense         &  1.0000 &              2 &         A1 &     1 &        0 \\
         \hline
         Verb tense         &  1.0000 &             21 &         B1 &     2 &        0 \\
         \hline
         Verb tense         &  1.0000 &             11 &         B2 &     3 &        0 \\
         \hline
         Verb tense         &  0.9375 &             16 &         A2 &     4 &        0 \\
         \hline
          $<$unk$>$             &  1.0000 &              4 &         B1 &     1 &        1 \\
          \hline
          $<$unk$>$             &  0.9000 &             10 &         A2 &     2 &        1 \\
          \hline
          $<$unk$>$             &  0.7500 &              8 &         A1 &     3 &        1 \\
          \hline
          $<$unk$>$             &  0.7500 &             28 &         B2 &     4 &        1 \\
          \hline
          Insertion         &  0.8000 &             10 &         A2 &     1 &        1 \\
          \hline
          Insertion         &  0.7500 &              4 &         B1 &     2 &        1 \\
          \hline
          Insertion         &  0.7143 &             28 &         B2 &     3 &        1 \\
          \hline
          Insertion         &  0.6250 &              8 &         A1 &     4 &        1 \\
          \hline
          Meaning deviation  &  0.6786 &             28 &         B2 &     1 &        1 \\
          \hline
          Meaning deviation  &  0.6000 &             10 &         A2 &     2 &        1 \\
          \hline
          Meaning deviation  &  0.5000 &              8 &         A1 &     3 &        1 \\
          \hline
          Meaning deviation  &  0.5000 &              4 &         B1 &     4 &        1 \\
          \hline
           Omission         &  0.8214 &             28 &         B2 &     1 &        1 \\
           \hline
           Omission         &  0.5000 &              4 &         B1 &     2 &        1 \\
           \hline
           Omission         &  0.3000 &             10 &         A2 &     3 &        1 \\
           \hline
           Omission         &  0.1250 &              8 &         A1 &     4 &        1 \\
           \hline
         Reference matching  &  1.0000 &              4 &         B1 &     1 &        1 \\
         \hline
         Reference matching  &  0.8929 &             28 &         B2 &     2 &        1 \\
         \hline
         Reference matching  &  0.5000 &              8 &         A1 &     3 &        1 \\
         \hline
         Reference matching  &  0.4000 &             10 &         A2 &     4 &        1 \\
         \hline
         Verb tense         &  0.8214 &             28 &         B2 &     1 &        1 \\
         \hline
         Verb tense         &  0.5000 &              4 &         B1 &     2 &        1 \\
         \hline
         Verb tense         &  0.1250 &              8 &         A1 &     3 &        1 \\
         \hline
         Verb tense         &  0.0000 &             10 &         A2 &     4 &        1 \\
         \hline
         Reference matching  &  1.0000 &             28 &         A2 &     1 &        2 \\
         \hline
         Reference matching  &  1.0000 &             15 &         B2 &     2 &        2 \\
         \hline
         Reference matching  &  0.9655 &             29 &         B1 &     3 &        2 \\
         \hline
         Reference matching  &  0.9412 &             17 &         A1 &     4 &        2 \\
         \hline
          Meaning deviation  &  1.0000 &              3 &         A2 &     1 &        3 \\
          \hline
          Meaning deviation  &  1.0000 &              6 &         B2 &     2 &        3 \\
          \hline
          Meaning deviation  &  0.6667 &              3 &         B1 &     3 &        3 \\
          \hline
           Omission         &  1.0000 &              3 &         B1 &     1 &        3 \\
           \hline
           Omission         &  0.8333 &              6 &         B2 &     2 &        3 \\
           \hline
           Omission         &  0.3333 &              3 &         A2 &     3 &        3 \\
           \hline
         Out of context     &  1.0000 &              3 &         A2 &     1 &        3 \\
         \hline
         Out of context     &  1.0000 &              3 &         B1 &     2 &        3 \\
         \hline
         Out of context     &  1.0000 &              6 &         B2 &     3 &        3 \\
         \hline
         Verb tense         &  0.8333 &              6 &         B2 &     1 &        3 \\
         \hline
         Verb tense         &  0.6667 &              3 &         B1 &     2 &        3 \\
         \hline
         Verb tense         &  0.0000 &              3 &         A2 &     3 &        3 \\
         \hline
\end{tabular}}
\end{table}

\begin{enumerate}
    \item Cluster 0: It is mostly allocated with sentences belonging to higher complexities (66\% of B1 and B2), with only 4\% belonging to A1. The presence of high complexities makes sense given the average incidence of errors to be about 38\% (which could be an even higher amount). Curiously, the Reference Matching error has no occurrence in A1 sentences, while the other complexities push its average to higher levels as they are all greater than 85\%. Regarding the Verb Tense error, which is another one that has a high incidence in this cluster, it seems that its average is uncorrelated to the sentence complexities.
    \item Cluster 1: This cluster has a similar allocation to higher complexities (64\% of B1 and B2), a bit similar to the previous cluster, but with a higher share allocated to A1 (16\%). It has the second highest error average of errors per sentence (55\%), much higher than cluster 1, even though it has a similar complexity distribution. A look in the rank tells that error classes do not change their average occurrence proportionally to the complexity share of sentences, such as $<$unk$>$ and Insertion. This conclusion is based on the fact that the average does not follow a monotonic trend when compared to a gradually increasing complexity, with some of those errors even having a smaller incidence among B2 sentences. Conversely, some errors do attend the approximately monotonic trend, namely Verb Tense, Omission and Meaning Deviation. The other Reference Matching error is somewhat neutral, with half of the complexities achieving expectations and the others not. 
    \item Cluster 2: It's the largest cluster with 89 sentences, with 49\% of them belonging to the more challenging complexities B1 and B2. It has the second smallest error average (29.35\%), as expected smaller than other clusters with higher complexities on average. Only one error had more than 50\% of incidence, and the complexity rank for this error shows that it appears to be a bit uncorrelated to this dimension, since A2 and B2 have the same error average.
    \item Cluster 3: This tiny cluster of 12 sentences has 75\% of them belonging to B2 and B1 complexities, the highest among all, which probably is correlated with the fact that it has the highest error average (55.2\%). Curiously, the HC algorithm has eliminated the A1 complexity of this cluster, which makes sense if the goal was to concentrate the most error prone sentences. Its ranking of most occurring errors per complexity shows some errors that appear to not increase their incidence with an increase in the CEFR dimension, which are Meaning Deviation and Out of Context. On the other hand, Verb Tense and Omission show a relevant difference for A2 and B2 sentences, as expected. 
    \item Cluster 4: The last cluster contains 39 sentences and is the one with the lowest occurrence of errors, about 10.9\%. This is somewhat compatible to the fact that 84.6\% of those sentences are A1 and none belong to B2. There are no errors capable of reaching an occurrence higher than 50\%, so there isn't much to analyse here, it looks like the algorithm preferred to agglomerate easier sentences that are less prone to errors.
\end{enumerate}

Now with complexity, model types and error classes all explored using the most appropriate Hierarchical Clustering parameters, the motivations behind the segmentation performed for each cluster are clearer. The algorithm has agglomerated sentences with similar complexity in some clusters, specially 3 and 4, notwithstanding the fact that it didn't receive this information beforehand. Also, for a significant share of sentences the average error incidence also increases with complexity, which is an evidence that the criteria used to rank the CEFR levels was at some level right. 

The diversity and share of errors also differs a lot from cluster to cluster depending on the sentence complexity share. For instance the Verb Tense errors seem to have some correlation with the difficulty of sentences by looking at Table \ref{cluster_ranking_complexity_error}, except for cluster 0. The same happens for Omission, with no exceptions. Nonetheless, it is hard to separate the influence of complexity or model technique to understand why the final error distribution behaves in some manner, as the influence on it is multidimensional, but there are already some interesting trends outlined that wouldn't be feasible by assessing its performance purely through scores. 
 
\subsection{Correspondence Analysis (CA) experiment} $\abbrev{Correspondence Analysis}{CA}$ 

% https://github.com/MaxHalford/prince#multiple-correspondence-analysis-mca
% https://ce.aut.ac.ir/~shiry/lecture/Advanced%20Machine%20Learning/Manifold_Modern_Multivariate%20Statistical%20Techniques%20-%20Regres.pdf

CA in practice is an exploratory multivariate technique that is capable of processing a N-dimensional table, extract its principal components and generate coordinates for them. These coordinates can be plotted into a N-Dimensional visualization that can help interpret relations between the dimensions. The most common case happens with 2 dimensions, where a co-occurrence matrix is converted into a 2-D visualization, and its rows and columns are depicted as points. The table that contains the dimension occurrences, which is also called contingency matrix, may have 2 dimensions or $N$ dimensions, being called Simple Correspondence Analysis in the former case and Multiple Correspondence Analysis in the latter. 

\subsubsection{Why use CA for qualitative analysis?}

CA is considered to be the Principal Components Analysis (PCA) \abbrev{Principal Components Analysis}{PCA} variant suitable for categorical data, as mentioned in \citet{Greenacre1987TheGI}, which makes it a perfect fit for the purpose of this work. The coordinates generated by the algorithm are analogous to factors in PCA (used for continuous data), but the key difference is that they partition the Chi-square value is used in testing independence instead of the total variance.

This algorithm benefits from the properties of Singular Value Decomposition (SVD) \abbrev{Singular Value Decomposition}{SVD} similarly to how PCA does it. It basically diagonalizes the covariance matrix of the input data, finding the major axes to represent the same distribution with a confidence level that depends on the eigenvalues of the first 2 factors. Given that the algorithm is able to fit the data with a decent explained variance, the resulting visualization enables a geometric interpretation that explains the relations between the variables, made possible also by the use of the Chi-square statistic. The potential to extract relevant relations depends on whether the explained variance is considered to be significant or not. There is no hard rule to describe what is a good explained variance, it depends on the rigor of the analysis. Values above 80\% considering the principal components are considered satisfactory and above 90\% are an almost ideal fit.

The CA algorithm has been applied to the same qualitative dimensions mentioned in the previous experiments (models and error classes or complexities and error classes). Literature shows that the conclusions derived from Simple CA are straightforward to interpret, and assessing its effectiveness and reliability is possible through explained variance. Hence, we believe that is has potential to corroborate and possibly complement the conclusions derived from other experiments.

\subsubsection{Implementation of CA}

Some techniques and properties benefited by CA were already mentioned in the beginning of this section, but the complete theory and calculus motivation behind the CA algorithm goes beyond the scope of this work. For more details the reader is referred to \citet{multivariate-izenman}. In order to gain a better understanding of the \textit{modus operandi} of this algorithm, a step by step pseudocode is provided here, heavily inspired by some course material \cite{ncss_correspondence} and the aforementioned book \cite{multivariate-izenman}. 

A co-occurrence matrix \textbf{K}, also referred to as 2-way contingency matrix in literature, is a matrix whose cells enumerate the co-occurrences of specific row-column combinations. Consider the matrix \textbf{K} of dimensions $r \times s$, where $r$ is the number of rows and $s$ is the number of columns. Each row contains one category of dimension $a$ (error class, for instance), that has $r$ categories in total. The same applied for the columns, that contain $s$ categories of dimension $b$. This matrix is the input for the CA algorithm, that is subjected to the following steps in order to obtain the algorithm's output:

\begin{enumerate}
    \item The first assertion that must be made since its necessary for the algorithm to work is that $\textbf{K} \in \mathbb{Z}^{*(r \times s)}_{+}$, therefore all elements of $\mathbf{K}$ must be non-negative and none of the row or column entries sum up zero
    
    \item Compute the proportion matrix $\textbf{P}=XY^{T}\frac{1}{\sum k}$, by dividing each element of K by the sum of all numbers in \textbf{K}. In mathematical terms: \begin{equation}
    P={p_{ij}}=\frac{k_{ij}}{\sum_{i}\sum_{j}k_{ij}}
    \end{equation}
    The expected format of the table would match the representation of Table \ref{correspondence_input}:
\begin{table}[H]
  \caption{Input Matrix \textbf{P} after normalizing \textbf{K}}
  \label{correspondence_input}
  \centering
  {\footnotesize
  \begin{tabular}{| p{2.3cm} | P{1.0cm} | P{1.0cm} | P{1.0cm} | P{1.0cm} | P{1.0cm} | P{1.0cm} | P{2.0cm} |}
    \hline
    Row Variable & $B_{1}$ & $B_{2}$ & \dotsc & $B_{j}$ & \dotsc & $B_{s}$ & Row Total \\
    \hline
    \hline
    $A_{1}$ &
    $p_{11}$ & 
    $p_{12}$ & 
    \dotsc &
    $p_{1j}$ &
    \dotsc &
    $p_{1s}$ &
    $p_{1+}$ \\
    \hline
    $A_{2}$ &
    $p_{21}$ & 
    $p_{22}$ & 
    \dotsc &
    $p_{2j}$ &
    \dotsc &
    $p_{2s}$ &
    $p_{2+}$ \\
    \hline
    \vdots &
    \vdots & 
    \vdots & 
    \vdots &
    \vdots &
    \vdots &
    \vdots &
    \vdots \\
    \hline
    $A_{i}$ &
    $p_{i1}$ & 
    $p_{i2}$ & 
    \dotsc &
    $p_{ij}$ &
    \dotsc &
    $p_{is}$ &
    $p_{i+}$ \\
    \hline
    \hline
    \vdots &
    \vdots & 
    \vdots & 
    \vdots &
    \vdots &
    \vdots &
    \vdots &
    \vdots \\
    \hline
    $A_{r}$ &
    $p_{r1}$ & 
    $p_{r2}$ & 
    \dotsc &
    $p_{rj}$ &
    \dotsc &
    $p_{rs}$ &
    $p_{r+}$ \\
    \hline
    \hline
    Column Total &
    $p_{+1}$ & 
    $p_{+2}$ & 
    \dotsc &
    $p_{+j}$ &
    \dotsc &
    $p_{+s}$ &
    1 \\
    \hline
\end{tabular}}
\end{table}

    \item The totals of both the rows and columns of \textbf{P} can also be embedded in the vectors \textbf{r} and \textbf{c}, respectively. An accurate representation of these vectors would be 
    
    \begin{equation}
        \textbf{P1}_{s} =
        \begin{bmatrix}
            p_{1+} \\
            \vdots \\
            p_{r+} 
        \end{bmatrix}
        = \textbf{r}
    \end{equation}
    and 
    \begin{equation}
        \textbf{P}^{T}\textbf{1}_{r} =
        \begin{bmatrix}
            p_{+1} \\
            \vdots \\
            p_{+s} 
        \end{bmatrix}
        = \textbf{c},
    \end{equation}
    
    which are also known as average row and column profiles. Consider the generation of matrices $D_{r} = [diag(\textbf{r})]^{-1/2}$ and $D_{c} = [diag(\textbf{c})]^{-1/2}$, that are constituted of a matrix of zeroes whose diagonal are the row totals and columns totals of the previous table, respectively. The elements inside the \textbf{r} and \textbf{c} vectors can also be manipulated to generate the diagonal elements of the square matrices
    \begin{equation}
     \textbf{D}_{r} =
     \begin{bmatrix}
     p_{1+} & 0 & 0 \\
     0 & \ddots & 0 \\
      0 & 0 & p_{r+} 
     \end{bmatrix}
    \end{equation}
    and
    \begin{equation}
     \textbf{D}_{c} = 
     \begin{bmatrix}
     p_{+1} & 0 & 0 \\
     0 & \ddots & 0 \\
     0 & 0 & p_{+s}
     \end{bmatrix},
    \end{equation}
    that contain the row and column totals in their diagonal.
    
    \item After obtaining the matrices $\textbf{D}_{r}$ and $\textbf{D}_{c}$, it becomes possible to calculate $\textbf{P}_{r}$ and $\textbf{P}_{c}$, which represent the row and column profiles of P:
    
    \begin{equation}
        \textbf{P}_{r} = \textbf{D}_{r}^{-1}\textbf{P}
        \begin{bmatrix}
            \textbf{a}_{1}^{T} \\
            \vdots \\
            \textbf{a}_{r}^{T}
        \end{bmatrix}
    \end{equation}
    where 
    \begin{equation}
        \textbf{a}_{i}^{T} =
        \begin{bmatrix}
        \frac{k_{i1}}{k_{i+}}, \dotsc, \frac{k_{is}}{k_{i+}}
        \end{bmatrix}
    \end{equation}
    and
    \begin{equation}
        \textbf{P}_{s} = \textbf{D}_{c}^{-1}\textbf{P}^{T}
        \begin{bmatrix}
            \textbf{b}_{1}^{T} \\
            \vdots \\
            \textbf{b}_{s}^{T}
        \end{bmatrix}
    \end{equation}
    where
    \begin{equation}
        \textbf{b}_{j}^{T} =
        \begin{bmatrix}
        \frac{k_{ij}}{k_{+j}}, \dotsc, \frac{k_{rj}}{k_{+j}}
        \end{bmatrix}.
    \end{equation}
    
    \item In this step, a key insight of the algorithm enables its output to become an interpretable graph, where the distances resemble relationships between the dimensions. Please note that the steps have been summarized, but the most important manipulations were kept. The chi-square distribution is used as a measure of distance to compute the row and column distances, and it is calculated as follows: 
        \begin{itemize}
            \item Row profile distances. Consider $a_{i}$ and $a_{i^{'}}$ that are entries of the row profiles. The squared $\chi^{2}$ distance between them is given by
        \begin{equation}
            d^{2}(\textbf{a}_{i}, \textbf{a}_{i^{'}}) = (\textbf{a}_{i} - \textbf{a}_{i^{'}})^{T}\textbf{D}_{c}^{-1}(\textbf{a}_{i} - \textbf{a}_{i^{'}}) = \sum^{s}_{j=1}\frac{k}{k_{+j}} (\frac{k_{ij}}{k_{i+}} - \frac{k_{i^{'}j}}{k_{i^{'}+}}).
        \end{equation}
        However, the distance that can correlate a dimension category with its corresponding centroid is the most valuable to be represented in the plot. Considering row profiles, their centroids are represented by \textbf{c}. The value of the distance to their centroid can be obtained by  
        \begin{equation}
            d^{2}(\textbf{a}_{i}, \textbf{c}) = (\textbf{a}_{i} - \textbf{c})^{T}\textbf{D}_{c}^{-1}(\textbf{a}_{i} - \textbf{c}) = \frac{1}{k_{i+}} \sum^{s}_{j=1} \frac{k}{k_{i+}k_{+j}} (k_{ij} - \frac{k_{i+}k_{+j}}{k}),
        \end{equation}
        which when summed over all row profiles results in 
        \begin{equation}
        \label{profile_totals}
        k \sum_{i=1}^{r} p_{i+} + d^{2}(a_{i}, c) = \sum_{i=1}^{r} \sum_{j=1}^{s} \frac{(k_{ij} - \frac{k_{i+}k_{+j}}{k})^{2}}{\frac{k_{i+}k_{+j}}{k}},
        \end{equation}
        which is similar to another statistic that we know of. Taking Equation \ref{profile_totals}, and matching $O_{ij}$ with $k_{ij}$ and $E_{ij}$ with $\frac{k_{i+}k_{+j}}{k}$, we find out that it can be approximated with the Pearson's chi-squared statistic, which is
        \begin{equation}
        \label{pearson_statistic}
        X^{2} = \sum_{i} \sum_{j} \frac{(O_{ij} - E_{ij})^{2}}{E_{ij}}.
        \end{equation}
        In Equation \ref{pearson_statistic}, $O_{ij}$ represents the observed cell frequency and $E_{ij}$ the expected cell frequency. The calculation of these terms will be crucial for the next steps.
        
        \item Column profile distances. The reproduction of the steps followed in the row dimension are equivalent for the column dimension, changing only the axis regarding the algebraic manipulations. The column profiles distances can be calculated as
        \begin{equation}
        d^{2}(\textbf{b}_{j}, \textbf{b}_{j^{'}}) = (\textbf{b}_{j} - \textbf{b}_{j^{'}})^{T}\textbf{D}_{r}^{-1}(\textbf{b}_{j} - \textbf{b}_{j^{'}}) = \sum^{r}_{i=1}\frac{k}{k_{i+}} (\frac{k_{ij}}{k_{+j}} - \frac{k_{ij^{'}}}{k_{+j^{'}}}).
        \end{equation}
        Following the same rationale for row profiles, the distance of the column with respect to its centroid is the metric of interest
        \begin{equation}
        d^{2}(\textbf{b}_{j}, \textbf{r}) = (\textbf{b}_{j} - \textbf{r})^{T}\textbf{D}_{r}^{-1}(\textbf{b}_{j} - \textbf{r}) = \frac{1}{k_{+j}} \sum^{r}_{i=1} \frac{k}{k_{i+}k_{+j}} (k_{ij} - \frac{k_{i+}k_{+j}}{k}),
        \end{equation}
        that can analogously be summed over all the column profiles, reaching an Equation that resembles \ref{profile_totals}. Similarly, this resulting equation has structure similarities with the Pearson's chi-squared statistic in Equation \ref{pearson_statistic}.
        \end{itemize}
    
    \item Now we derive the total inertia equation and its decomposition. As we are using dummy categorical variables to represent the dimensions, 
    we proceed with one standard assumption of contingency table analysis where the row and column totals are considered to be fixed and the cell frequencies in \textbf{K} are allowed to vary given those constraints. The relative frequency matrix incorporates such restrictions:
    \begin{equation}
    k^{-1} \textbf{X} (\textbf{I}_{k} - k^{-1}\textbf{J}_{k})\textbf{Y}^{T} = \textbf{P} - \textbf{r}\textbf{c}^{T} = \Tilde{\textbf{P}},
    \end{equation}
    where $K=XY^{T}$, $\textbf{J}_{k}=\textbf{1}_{a}\textbf{1}_{a}^{T}$ is an ($a \times a$)-matrix of 1s and the matrix $\Tilde{\textbf{K}}=k\Tilde{\textbf{P}}$ is also known as the matrix of residuals. This name is given because its $ij$th entry shows the difference between the observed cell frequency $O_{ij}=k_{ij}$ and the expected cell frequency $E_{ij}=\frac{k_{i+}k_{+j}}{k}$, resulting in $\Tilde{k}_{ij}=O_{ij}-E_{ij}$.
    The last step to reach the inertia calculation is to calculate the ($s \times s$)-matrix $\textbf{R}_{0}$
    \begin{equation}
    \textbf{R}_{0} = \textbf{D}_{c}^{\frac{-1}{2}}\Tilde{\textbf{P}}^{T}\textbf{D}_{r}^{-1}\textbf{\Tilde{P}}\textbf{D}_{c}^{\frac{-1}{2}},
    \end{equation}
    where $\textbf{D}_{r}^{-1}=diag\{\textbf{r}^{-1}\}$ and $\textbf{D}_{c}^{\frac{-1}{2}}=diag\{\textbf{c}^{\frac{-1}{2}}\}$. The entry in the $j$th row and $j^{'}$th column of $\textbf{R}_{0}$ is given by
    \begin{equation}
    (k_{+j}k_{+j^{'}})^{\frac{-1}{2}} \sum_{i=1}^{r} \frac{1}{k_{i+}} (k_{ij} - \frac{k_{i+}k_{+j}}{k}) (k_{ij^{'}}-\frac{k_{i+}k_{+j^{'}}}{k})
    \end{equation}
    and the $j$th diagonal entry of $\textbf{R}_{0}$ is obtained by setting $j=j^{'}$,
    \begin{equation}
    \frac{1}{k_{+j}} \sum_{i=1}^{r} \frac{1}{k_{i+}} (k_{ij} - \frac{k_{i+}k_{+j}}{k})^{2}.
    \end{equation}
    The trace of $\textbf{R}_{0}$, which is also the sum of the eigenvalues of $\textbf{R}_{0}$ is 
    \begin{equation}
    \sum_{j=1}^{s} \lambda_{j}^{2} = tr\{\textbf{R}_{0}\} = \sum_{i=1}^{r} \sum_{j=1}^{s} \frac{1}{k_{i+}k_{+j}} (k_{ij} - \frac{k_{i+}k_{+j}}{k})^{2} = \frac{X^{2}}{k},
    \end{equation}
    where $X^{2}$ is given by Equation \ref{pearson_statistic}. Finally, the accumulated contribution of the first $t$ principal components (or inertias) can by calculated with
    \begin{equation}
    \frac{\lambda_{1}^{2} + \dotsc + \lambda_{t}^{2}}{\sum_{j=1}^{s} \lambda_{j}^{2}}.
    \end{equation}
    
    \item Compute the scaled matrix \textbf{M} where $\textbf{M}=D_{r}^{\frac{-1}{2}}\frac{\Tilde{\textbf{K}}}{k}D_{c}^{\frac{-1}{2}}$.
    
    \item Compute the SVD of matrix $\textbf{M} = \textbf{U} \textbf{D}_{\lambda} \textbf{V}^{T}$, where \textbf{U} is an $(r \times s)$ unitary matrix where $\textbf{U}^{T}\textbf{U}=\textbf{I}_{s}$, $\textbf{D}_{\lambda}=diag\{\lambda_{1}, \dotsc, \lambda_{s}\}$ is an $(s \times s)$ diagonal matrix with its principal diagonal corresponding to the singular values, and \textbf{V} is an $(s \times s)$ unitary matrix, where $\textbf{V}^{T}\textbf{V}=\textbf{I}_{s}$ and $\textbf{V}^{T}$ is the conjugate transpose of \textbf{V} %https://gregorygundersen.com/blog/2018/12/10/svd/
    
    \item Calculate the matrices that represent the principal axes of the row and column profiles, namely $\textbf{A}=\textbf{D}_{r}^{\frac{-1}{2}}\textbf{U}$ and $\textbf{B}=\textbf{D}_{r}^{\frac{-1}{2}}\textbf{V}$
    
    \item After obtaining \textbf{A} and \textbf{B}, a series of manipulations are performed to reach the coordinate matrices, which will not be demonstrated here but are available in the reference book \cite{multivariate-izenman}. The standard principal coordinates matrix can be obtained by calculating
    $\textbf{G}_{S}=\textbf{U}\textbf{D}_{r}^{\frac{-1}{2}}$ and $\textbf{H}_{S}=\textbf{D}_{c}^{\frac{-1}{2}}\textbf{V}$. The rows of each pair in $\textbf{G}_{S}$ represent a row profile category and the columns also contain categories, but belonging to the other dimension.
    
    \item In order to be able to apply the conventional interpretation criteria to the generated plot, the coordinates must be scaled, otherwise the interpretation guidelines may not be valid anymore and this will hinder the extraction of insights. The scaling techniques and their appropriate use cases deserve a deeper explication that is available in the material \cite{multivariate-izenman}. To obtain the scaled coordinates, we compute the matrices containing the principal coordinates of row and column profiles 
    $\textbf{G}_{P}=\textbf{D}_{\lambda}\textbf{U}\textbf{D}_{r}^{\frac{-1}{2}}$ and $\textbf{H}_{P}=\textbf{D}_{\lambda}\textbf{D}_{c}^{\frac{-1}{2}}\textbf{V}$. These matrices contain the data that is necessary to extract associations and properties from both dimensions, and they mark the end of the algorithm.

\end{enumerate}

By following the steps above until 11, the ($x, y$) coordinates will be available for all categories in both dimensions in the $\textbf{G}_{P}$ and $\textbf{H}_{P}$ matrices, respectively. Such rows and columns when drawn as a scatter plot in a two dimensional cartesian plane can be used to indicate how each category in their dimension is positioned and how to correlate them. Depending on whether the relation between two points is intra or interdimensional, there are different methodologies to interpret, which will be explained in the next section. 

\subsubsection{Interpretation of CA}

% source: https://www.displayr.com/interpret-correspondence-analysis-plots-probably-isnt-way-think/

Given that the Simple Correspondence Analysis was chosen based on our analysis demands, we will limit the interpretation explained here to the two-way contingency tables (associations between 2 variables). It is important to emphasize that correspondence analysis will only correlate the relative variations considering the association of 2 or more variables. This means that variations within the same dimension that are uncorrelated with the other won't generate a clear visual interpretation. The final visualization has 3 graphical features that should be considered for interpretation: 

\begin{enumerate}
    \item The distance of the categorical variable (row or column) with respect to the origin
    \item The proximity of data points that belong to the same dimension (comparison of rows with themselves or columns with themselves). This is the recommended measure of intradimensional similarity 
    \item The angle that connects a row and a column to the origin. This is the recommended measure of interdimensional similarity
\end{enumerate}

Regarding the first item, whenever a category is far from the origin, it can be inferred that they are more discriminating. That means that this specific category is uncorrelated with others and grants characteristics that distinguish an observation from the rest. The opposite is also valid, if the category is near the origin it means that probably they are common or share characteristics with other nearby categories.

The second item means that categories of the same dimension usually share similar characteristics when they have small distances between them. Conversely, if a pair of categories are far from each other in the cartesian plane, it means that they are somewhat uncorrelated and their distribution with respect to the other dimension have significant differences.

Lastly, the third interpretation item is the only one that lets you compare categories that belong to distinct dimensions. Draw a line between the origin and a row category that you wish to compare, and repeat the same process for the desired column category. If the angle formed between those straight lines is very acute (close to 0), it means that their occurrence are probably associated. If that angle is near 90 degrees, than probably those categorical variables do not possess any type of association at all, however, if the angle is close to 180 degress, than those categories are probably negatively associated.

\subsubsection{Models and error classes}

The dataset that underwent this experiment consists of all the 60 distinct sentences multiplied by the number of models, which in this case are 4: the BPE Transformer, the models trained on 33\% of TED and 66\% of TED and the Back Translation Transformer. All these sentences have been labelled with binary columns indicating the presence or absence of each of the 8 error classes. After grouping this dataset by model type and summing the binary error columns, a 2-way contingency table is created, where the rows are the model types and the columns represent each one of the 8 error classes. The algorithm had this table as input.

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.5]{chapter5_images/correspondence_model_error.png}
\caption{Correspondence analysis visualization comparing models and error classes}
\label{correspondence_model_error}
\end{figure*}

The algorithm has successfully fitted the dataset with an explained variance of 94.57\%, with about 74.56\% belonging to the first component and 20.01\% to the second component. This score indicates a very good representation of the data. Figure \ref{correspondence_model_error} shows the resulting visualization from the experiment. 

Some errors have stood out from the rest by denoting singular characteristics: Repetition, $<$unk$>$ and Out of context are all far from the origin, while the others are more concentrated in a smaller area. Reference Matching is located very close to Insertion, while Verb Tense is also close to Omission, indicating that error patterns of these pairs share similarities over the models. All these errors are located in an area with approximately the size of a quadrant, which means that despite the pairwise similarity is higher, similarity within the group also exists. 

Regarding models, the Back Translation model stands out from the others, while 33\% and 66\% are close, indicating that their error patterns share similarities. BPE seems to be isolated, so it has a weak relationship with the rest, but not as far from the origin as Back Translation. BPE error patterns are probably uncorrelated with Back Translation as they are the most distant models in the cartesian plane.

Below the findings of the experiment that relate both models and errors are listed, following a model-centered approach:

\begin{enumerate}
    \item BPE shows a positive correlation with the Omission error, being the model with the strongest relationship with this error. There is also some positive correlation with Verb Tense, Out of Context and Meaning Deviation. It is negatively correlated with the Insertion error and also with the $<$unk$>$ error, as its placed in about 180 degrees of them (which makes sense even more for $<$unk$>$ since the algorithm does not produce that error at all). At the same time it appears to have no correlation with the Repetition and Reference Matching errors, as it creates an angle near 90 degrees with them in the origin
    \item Both reduced models 33\% and 66\% possess similar relationships with errors. They are highly correlated with the Repetition error, still positively correlated with Meaning Deviation and $<$unk$>$ errors and negatively correlated with the Reference Matching and Verb Tense errors. They also seem to have almost no correlation with the Omission, Insertion and Out of Context errors
    \item Finally, the Back Translation model has high positive correlation with the Reference Matching and Insertion errors, and also some positive correlation with $<$unk$>$, while having almost no correlation with Verb Tense and Omission errors. It also has a strong negative correlation with the Repetition and Meaning Deviation errors, and in a smaller degree with Out of Context
\end{enumerate}

Some interesting hypothesis can be outlined based on the graphical inspection. Augmenting with data from a different domain induces the model to create translations that don't match the reference, potentially indicating that it finds synonyms for words of the same sentence. BPE seems to frequently omit words although it is also efficient against the Insertion of unnecessary words. Regarding the models trained on reduced TED, they seem to repeat the same words when they do not possess vocabulary knowledge to finish a sentence with certainty. Also, the reduced vocabulary seems to prevent the occurrence of synonyms, probably because of the limited options of words.

\subsubsection{Sentence complexity and error classes}

In this second round of Correspondence Analysis experiments, the dataset containing the binary flags indicating error classes per sentence has been grouped by complexity label and summed over the binary columns, generating another 2-way contingency table where the rows are CEFR complexity levels and columns represent the error classes. The explained variance indicates another successful representation of data: for both components it reaches 96.29\%, with a contribution of 64.28\% from the first component and 32.01\% from the second component. The resulting visualization can be seen in Figure \ref{correspondence_complexity_error}.

\begin{figure*}[tb]
\centering 
\includegraphics[keepaspectratio,scale=0.6]{chapter5_images/correspondence_complexity_error.png}
\caption{Correspondence analysis visualization comparing complexities and error classes}
\label{correspondence_complexity_error}
\end{figure*}

The categories that are most isolated and distinguish themselves from the others are the Repetition, Out of Context and Verb Tense errors as well as A1 complexity. The complexity labels are well distributed along the graph area, which indicate that they are probably segmenting well the sentences provided. The pairs B1 and B2, A2 and B1 are closer to each other, which makes sense as they represent consecutive complexities. The space between the error classes has also improved when compared to the last experiment, which means that they hold more unique patterns when talking of the relativities between their dimension and sentence complexity. The proximity of Insertion with Repetition indicates some similarity between these errors, which has also manifested in the previous experiment. Omission and Reference Matching errors are also close to each other, but this relation has only appeared in this experiment.

Regarding the category associations along both dimensions, all the positive, negative and absence of correlations derived from graphical inspection that were observed will be explained below. Although the graphical interpretation can be associated with correlation, it doesn't follow the pearson correlation formula for instance, it just considers associations between both dimensions derived from their co-occurrence matrix properties. We highlight the following observations:

\begin{enumerate}
    \item A1 shows potential for positive correlation with the Insertion, $<$unk$>$ and Repetition errors, negative correlation with Verb Tense and Omission errors and potential absence of correlation with the Reference Matching, Out of Context and Meaning Deviation errors
    \item The Figure indicates that A2 can be positively correlated with Reference Matching, Out of Context and Omission errors, probably negatively correlated with $<$unk$>$, Meaning Deviation and Verb Tense errors and lacks correlation with the Insertion and Repetition errors
    \item B1 has positive correlation with Omission, Verb Tense and Out of Context errors. Negative correlation shows up with Insertion, Repetition and $<$unk$>$ errors, and also apparently there is no correlation with the Reference Matching and Meaning Deviation errors
    \item Lastly, B2 seems to have positive correlation with Verb Tense, Meaning deviation and $<$unk$>$ errors, negative correlation with Out of Context and Reference Matching errors and absent correlation with the Repetition, Omission and Insertion errors
\end{enumerate}

The relations extracted through graphical interpretation have shown some interesting trends. The high explained variance and well distributed data points for both dimensions show that the algorithm's bias probably had a strong match with the data distribution bias, indicating that the categories have segmented very well the sentences. Qualitatively "simpler" errors such as Insertion and Repetition are usually correlated with lower complexities such as A1 and uncorrelated or negatively correlated with higher complexities, such as B1. Conversely, $<$unk$>$ and Meaning Deviation errors that are more associated with complex translations are usually positively associated with higher complexities such as B2 and negatively associated with lower complexities, such as A1. As the complexity points are scattered across the cartesian plane, the other errors create different relations with the complexity levels, but we believe they are situated in some sort of "medium level" complexity. It is a hard claim to make, it cannot be confirmed without an increased sample size, but it's one intuition that the results are providing.

\subsection{Consolidated result of the qualitative study}

Each experiment performed in the qualitative domain has presented a number of conclusions and hypothesis, but being able to visualize the whole picture based on the statements of the isolated experiments is a hard task. In this section we have filtered the most relevant patterns and findings and will try to establish a connection between them, aiming to provide a clear and concise view about the outcome of the qualitative analysis. 

The clarification of the qualitative results starts with a brief discussion about the expectations set for each experiment and a comparison of their results. Just comparing the average occurrence of errors between models cannot prove that one model indeed is more susceptible to that specific error than the other. Solely interpreting this average and comparing across dimensions (in our case, model-error or complexity-error) results in an analysis subjected to statistical flaws. This argument led the study to use the Fisher Exact hypothesis test, which has mathematical background to corroborate and complement the differences in averages and reveal potential biases, given that the null hypothesis is rejected. On a second perspective, the Hierarchical Clustering algorithm segments the dataset and aggregates similar sentences into groups of clusters, each of them tracing an unique profile of error statistics that carry some weight when defining the whole model-error bias, for instance. Although the interpretation is very subjective, it helps to reveal multidimensional associations between the model, error and complexity dimensions. Finally, the Correspondence Analysis experiment provides an easier interpretation along with richer and clearer associations between the dimensions of interest. All the categories belonging to the dimensions can be compared with clearly stated rules that can help to identify presence of positive or negative correlation, or even the absence of it. 

\begin{table}[t!]
  \caption{Main model-error conclusions extracted from qualitative experiments}
  \fontsize{10}{12}\fontfamily{phv}\selectfont
  \label{qualitative_error_model_consolidated}
  \centering
  {\footnotesize
  \begin{tabular}{| p{1.5cm} | P{1.1cm} | P{2.2cm} | P{2.5cm} | P{2.7cm} | P{2.3cm} |}
    \hline
    Relation-ship & Experiment & Back Translation & 33\% TED & 66\% TED & BPE Transformer \\
    \hline
    \multirow{3}{1.0cm}{Positive error correlation} & Fisher Exact Test & N/A & Meaning Deviation (vs BT), Repetition (vs BT, BPE), $<$unk$>$ (vs BT, BPE) & Meaning Deviation (vs BT), Repetition (vs BT, BPE), $<$unk$>$ (vs BT, BPE) & N/A \\
    \cline{2-6}
    & HC & Omission (cl1), $<$unk$>$ (cl1), Reference Matching (cl2) & Verb Tense (cl0, cl3), Reference Matching (cl0), Meaning Deviation (cl1), Omission (cl1, cl3), Out of Context (cl3) & Verb Tense (cl0), Reference Matching (cl2), Meaning Deviation (cl3), Out of Context (cl3) & Reference Matching (cl0, cl1, cl2), Verb Tense (cl0, cl1), Insertion (cl1), Meaning Deviation (cl1) \\
    \cline{2-6}
    & CA & Reference Matching, Insertion, $<$unk$>$ & Repetition, Meaning Deviation, $<$unk$>$ & Repetition, Meaning Deviation, $<$unk$>$ & Omission, Meaning Deviation, Out of Context, Verb Tense \\
    \hline
    \multirow{3}{1.0cm}{Negative error correlation} & Fisher Exact Test & Meaning Deviation (vs 33\%), Repetition (vs 33\%, 66\%), $<$unk$>$ (vs 33\%, 66\%) & N/A & N/A & Repetition (vs 33\%, 66\%), $<$unk$>$ (vs all) \\
    \cline{2-6}
    & HC & Out of Context (cl1), Meaning Deviation (cl1, cl3) & Reference Matching (cl1, cl2), Verb Tense (cl1) & Insertion (cl1), Verb Tense (cl1), Meaning Deviation (cl1), $<$unk$>$ (cl1), Reference Matching (cl1) & $<$unk$>$ (all), Omission (cl1) \\
    \cline{2-6}
    & CA & Repetition, Meaning Deviation, Out of Context & Reference Matching, Verb Tense & Reference Matching, Verb Tense & Insertion, $<$unk$>$ \\
    \hline
    \multirow{3}{1.0cm}{Almost zero correlation} & Fisher Exact Test & Omission, Out of Context, Reference Matching, Insertion, Verb Tense & Omission, Out of Context, Reference Matching, Insertion, Verb Tense & Omission, Out of Context, Reference Matching, Insertion, Verb Tense & Omission, Out of Context, Reference Matching, Insertion, Verb Tense \\
    \cline{2-6}
    & HC & Verb Tense (cl0, cl1, cl3) & Out of context (cl3) & Out of context (cl3) & Out of context (cl3) \\
    \cline{2-6}
    & CA & Verb Tense, Omission & Omission, Out of Context, Insertion & Omission, Out of Context, Insertion & Repetition, Reference Matching \\
    \hline
\end{tabular}}
\end{table}

\subsubsection{Models and error classes}

The core goal of the study is to understand the error biases and be able to correlate them to a specific technique, but as this may represent only a subset of the problem, other relevant dimensions were added aiming to refine its explication. Sentence complexity has been chosen as an auxiliary dimension for this, others could be considered, but unfortunately resources are scarce to increase the depth, specially considering low-resource constraints. Therefore, the explored domain is limited to these 3 dimensions. The principal results of the associations that belong to the core goal (correlate models and error patterns) are displayed in Table \ref{qualitative_error_model_consolidated}, they are split among positive, negative and absence of correlation, considering the incidence of error classes and their relationship with each model.

Notice that each error class relationship outlined in a cell has in parenthesis the scenario where it was revealed. In the Fisher experiment, the classes reported always consider a comparison against another model, whereas in HC each conclusion was extracted from a specific cluster, and in CA the process is easier since it is inspected graphically, it doesn't need a reference. The correlation categories with no findings were filled with "N/A". The almost zero correlation row had a different motivation for each experiment: in Fisher it basically complements the classes that didn't reject the null hypothesis of positive and negative correlation, while HC if the share of sentences per class between clusters varies but the error average remains, it falls into this category. Finally, in CA there is no inference to be made, since the interpretation guidelines already provide a clear diagnostic.

When reading the results, it is important to keep in mind that there is quantitative evidence available to reinforce the argument that results of the CA algorithm are more reliable than the ones from HC. This is due to the explained variance achieved of about 95\%, which is considered to be optimal, while a silhouette coefficient of 0.2320 is notably far from the desired target of 1. This corroborates the assumptions made when writing the interpretations in the following paragraphs, which will follow the logic that the findings of CA will be considered stronger statistically speaking than the ones from HC.

The Back Translation Transformer is probably more subjected to the Reference Matching error based on results from HC and CA, with potentially some synergies with the Insertion error denoted by CA. The hypothesis of the Omission and $<$unk$>$ errors being positively correlated are discarded by (1), the suggestion from CA that the former has almost zero correlation and (2), the Fisher Test accusing that the latter is less frequent in this model versus the reduced TED models, which seems to be stronger since the angle shown in CA is bigger than 60 degrees. The BT model also spawns less often the Out of Context error according to CA, which is also true for Repetition and Meaning Deviation, backed by CA and Fisher (at least when compared to low-resource models). Lastly, the Verb Tense error seems to be supported in unanimity that it isn't correlated with BT at all, and a similar trend is followed by Omission, which is supported by Fisher and CA.

As already noticed before, both the 33\% and 66\% TED models share similar results in all experiments. The Fisher test and CA indicate that the Repetition and Meaning Deviation errors are common errors produced by these models. They also produce the $<$unk$>$ token a lot more when compared to BT and BPE based on Fisher, but its hard to ensure this since CA tells a different story in the absence of correlation part. Speaking of correlation absence, both CA and Fisher test tell that the incidence of the Omission, Insertion and Verb Tense errors is indifferent with an increase or decrease of sentences evaluated by the 33\% and 66\% TED models. This may also apply to the Out of Context error, according to HC and Fisher, but that affirmation is made without agreement by CA (given the negative correlation hypothesis). Regarding negative correlation, it seems that the Reference Matching and Out of Context errors are pointed out to occur less with the models trained on reduced TED by HC and CA.

Finally, considering the BPE Transformer, the CA row in Table \ref{qualitative_error_model_consolidated} shows that the occurrence of Meaning Deviation, Verb Tense, Out of Context and Omission errors is directly proportional to an increase in the number of sentences generated by such model. The $<$unk$>$ token has a negative correlation with the model, meaning that it is efficient against it, according to all models. In the meanwhile, CA indicates that Insertion also should also not spawn a lot when using BPE and Fisher tells the same regarding Repetition. Regarding absence of correlation, CA points that the Reference Matching error is uncorrelated with the model.

These were the straightforward findings to correlate models and errors, however there are some reflections and conclusions that come as a side product but are not evident in those statements. For instance, one of such statements is that augmenting with data from a different domain induces the model to create translations that don’t match the reference. This cannot be interpreted as a rule, further studies are required to establish this pattern, but developing this hypothesis wouldn't be even possible without applying the aforementioned algorithms. Other similar statements include the fact that BPE seems to frequently omit words although it is also efficient against the Insertion of unnecessary words.

Models under heavier low-resource constraints such as the ones trained on the reduced TED seem to repeat the same words when they do not possess vocabulary knowledge to finish a sentence with certainty (according to CA and Fisher). Also, the reduced vocabulary seems to prevent the occurrence of synonyms, probably because of the limited options of words. In all experiments, both the models trained on 33\% and 66\% of the dataset share similar error distributions, are situated nearby in graphical visualizations (CA) or segmented in clusters with a similar share of sentences. This may represent a proof that the experiments shows some correlation between their translation biases.

\subsubsection{Sentence complexity and error classes}

A similar thought process was conducted for the experiments that aimed to correlate sentence complexity and error classes. As complexity was included more as a complement to understand the error biases of models, it doesn't make sense to analyse each complexity level in the same manner as done for each model that participated in the study, as done in the previous section. Also, there is no clear value for instance in correlating the models directly with complexities. The objective here is to understand how complexity impacts the error patterns, if the criteria established for the CEFR segmentation performed made sense and how it is reflected on the distribution of errors. A proxy used to validate this is checking if an increase in complexity level leads to an increase, decrease or no difference in the amount of errors made.

\begin{table}[h!]
  \caption{Main complexity-error conclusions extracted from qualitative experiments}
  \fontsize{10}{12}\fontfamily{phv}\selectfont
  \label{qualitative_complexity_model_consolidated}
  \centering
  {\footnotesize
  \begin{tabular}{| p{3.0cm} | P{2.0cm} | P{8.2cm} |}
    \hline
    Relationship & Experiment & Errors that are proportional to complexity \\
    \hline
    \multirow{3}{3.0cm}{Errors directly proportional with complexity} & Fisher Exact Test & Reference Matching, Omission (except for A2|B2), Out of Context (except for A2|B2), Verb Tense, Meaning Deviation \\
    \cline{2-3}
    & HC & Reference Matching (cl0), Verb Tense (cl1, cl3), Meaning Deviation (cl1), Omission (cl1, cl3) \\
    \cline{2-3}
    & CA & Insertion (A1), Repetition (A1), $<$unk$>$ (A1, B2), Meaning Deviation (B2), Verb Tense (B1, B2), Reference Matching (A2), Out of Context (A2, B1), Omission (A2, B1) \\
    \hline
    \multirow{3}{3.0cm}{Errors inversely proportional with complexity} & Fisher Exact Test & Insertion (except for A1|B2, A2|B2), $<$unk$>$ (except for A1|B2, A2|B2) \\
    \cline{2-3}
    & HC & $<$unk$>$ (cl1), Insertion (cl1), Meaning Deviation (cl3) \\
    \cline{2-3}
    & CA & Omission (A1), Reference Matching (A2, B2), Out of Context (A2, B2), Insertion (B1), Repetition (B1), $<$unk$>$ (A2, B1), Meaning Deviation (A2), Verb Tense (A1) \\
    \hline
    \multirow{3}{3.0cm}{Almost zero correlation with complexity} & Fisher Exact Test & Repetition \\
    \cline{2-3}
    & HC & Out of context (cl3), Verb Tense (cl0) \\
    \cline{2-3}
    & CA & Insertion (A2, B2), Repetition (A2, B2), Meaning Deviation (A1, B1), Out of Context (A1), Meaning Deviation (A1), Reference Matching (B1), Omission (B2) \\
    \hline
\end{tabular}}
\end{table}

With the motivations explained, Table \ref{qualitative_complexity_model_consolidated} contains the results. Notice that each technique share a similar reference of comparison as before, which is represented between parenthesis. HC highlights cluster averages, Fisher compares which tests did not show a difference for higher versus lower complexities and CA tells the type of correlation between errors and complexities, similarly to what happened for the model-error pair. The same analogy that CA had a better performance when fitting the dataset, and hence will weight more on the final outcome than other techniques can be applied here.

Most of the error classes (5 out of 8) stood out from the rest for being directly proportional with complexity: Reference Matching, Omission, Out of Context, Verb Tense and Meaning Deviation. Some of them in unanimity, others supported by strong evidence provided by some experiments but not from all. In addition, there are some errors that appear to have an inverse relationship with complexity: Insertion and $<$unk$>$, backed by all the experiments. By elimination, all the experiments but HC have indicated that the Repetition error seem to not be affected by complexity, so presenting more complex or less complex sentences to a model is unlikely to change its probability to happen. It is the only error that is potentially more prone to the bias of a model or a technique rather than the complexity.

There is a story to tell based on the results that go beyond the straightforward interpretation of them: all experiments indicate that the categories have segmented very well the sentences. The position of the complexity categories in the CA plot in Figure \ref{correspondence_complexity_error} are a strong evidence, along with the hypothesis test not finding any relevant changes between any pairs. To some extent, but in a less evident manner, the distribution of errors per cluster also supports the argument that a good segmentation took place. The algorithm has agglomerated sentences with similar complexity in clusters 3 and 4, despite the fact that it didn’t receive this information beforehand.  Its interesting to note that errors have found more occurrences in specific complexity levels. Not always they were directly proportional, but in most times (62.5\% to be precise) errors had a trend to increase based on an increase in the complexity of sentences presented to a model, while 25\% were inversely proportional and 12.5\% showed no difference. A worrisome scenario would happen if most errors showed no correlation with the complexity categories, but numbers show that this is far from reality.


% \begin{table}[h]
%   \caption{Qualitative results interpretation for the unsupervised model-error analysis}
%   \fontsize{10}{12}\fontfamily{phv}\selectfont
%   \label{tab:table4-multidim-results}
%   \centering
%   {\footnotesize
%   \begin{tabular}{| p{0.80cm} | P{0.80cm} | P{1.35cm} | P{1.2cm} | P{1.6cm} | P{1.35cm} | P{1.2cm} | P{1.4cm} | P{1.6cm} |}
%     \hline
%     \multirow{2}{0.8cm}{1st Model} & \multirow{2}{0.8cm}{2nd Model} & \multicolumn{3}{|P{4.1cm}|}{Hierarchical Clustering} & \multicolumn{3}{|P{4.0cm}|}{Correspondence Analysis} & \multirow{2}{1.6cm}{Discussion} \\
%     \cline{3-8}
%     & & Common Errors & Rare Errors & Result & Common Errors & Rare Errors & Result & \\
%     \hline
%     33\% Reduced TED Model & 66\% Reduced TED Model & Insertion and $unk$ & Repetition and Verb Tense & They share similar proportions in the clusters they appear in & Repetition and $unk$ & Similar word choice and Verb tense & The dots that represent both models are very close & Both indicate that they have very similar error patterns and stand out from the others\\
%     \hline
%     \multicolumn{2}{|P{1.9cm}|}{BPE} & Out of Context, Sentence Choice, Omission and Verb Tense & $unk$ and Repetition & Verb Tense stands out as common, Repetition and $unk$ as uncommon & Omission, Out of Context, Sentence Choice and Verb Tense & $unk$ and Insertion & Model positions itself far from the others and with different correlations & Common errors converge completely and $unk$ is unrelated in both \\
%     \hline
%     \multicolumn{2}{|P{1.9cm}|}{Back Translation} & Similar Word Choice, Omission and Sentence Choice & Repetition, $unk$ and Out of Context & $unk$ and Repetition perform bad while Similar Word choice stands out in all clusters & Similar Word Choice and Insertion & Out of Context, Sentence Choice and Repetition & Model positions itself far from the others and with different correlations & Similar word choice stands out while Repetiton and Out of Context are unrelated \\
%     \hline
% \end{tabular}}
% \end{table}

% \begin{table}[h]
% \caption{Transfer learning and subword embeddings translation results}
% \fontsize{10}{12}\fontfamily{phv}\selectfont
% \begin{tabular}{| p{2.5cm} | p{1.5cm}  | p{1.0cm}  | p{0.9cm}  | p{1.1cm}  | p{1.5cm}  | p{1.0cm}  | p{0.9cm}  | p{1.1cm} |}
%     \hline
%     \multirow{2}{2.5cm}{Technique applied} & \multicolumn{4}{|p{5.0cm}|}{Tatoeba} & \multicolumn{4}{|p{5.0cm}|}{TED}\\
%     \cline{2-9}
%     & Sacrebleu & NLTK BLEU & Batch Size & Epochs & Sacrebleu & NLTK BLEU & Batch Size & Epochs \\
%     \hline
%     None & 57.99 & 74.07 & 512 & 54 & 25.24 & 65.36 & 34 & 29\\
%     \hline
%     \mbox{Fast Text} & 56.96 & 69.91 & 512 & 50 & 24.07 & 61.69 & 30 & 45\\
%     \hline
%     \mbox{Fast Text} (encoder + decoder Gigaword) & N/A & N/A & N/A & N/A & 24.54 & 61.76 & 32 & 89\\
%     \hline
%     \mbox{Fast Text} (source recommendations) non-binary loading (encoder + decoder Gigaword) & N/A & N/A & N/A & N/A & 21.26 & 52.31 & 32 & 80\\
%     \hline
%     Subword BPE & \textbf{66.63} & \textbf{83.02} & 512 & 40 & \textbf{40.26} & \textbf{72.20} & 32 & 40\\
%     \hline
% \end{tabular}
% \end{table}
 
\chapter{Conclusion}

Over the course of the previous chapters a lot of context was given, the main problem to be addressed was defined, and after reviewing the literature some techniques were elected to be analysed. The analysis of the results was customized to increase its depth. The goal was to be able to interpret the technique's bias despite the challenge of pursuing this qualitative path, which is usually overlooked in the NMT field.

We have started this work providing context of how the NLP field evolved to tackle Machine Translation and the challenges that arised from it. The scarcity of Portuguese papers about NMT at the time of writing provided additional technical and language intrinsic challenges to contribute to the field, meanwhile it was also seen as an inspiration to make more impact. All Portuguese language challenges were categorized in the main aspects that linguistics state for all languages in Chapter \ref{machine_traslation_challenges}. The language was rated to some subjective level of difficulty in each of those criteria to provide context of the work's complexity. Subsequently, the traditional ways of representing and modelling natural language were presented, along with how they evolved to contemplate the task of Machine Translation. Popular NLP algorithms such as word2vec were presented and explained in equations to provide intuition of the common ways Artificial Intelligence has to tackle NLP related tasks.

After providing context on some fundamentals of NLP, we presented the recurrent neuron in a simplified graph and analysed how it fits into one of the first NMT algorithms to become widespread \cite{cho-etal-2014-learning}. The main strengths and drawbacks of the algorithm were presented, some of them already addressed by the same authors, that some months later came up with a variant that addressed the fixed length vector issue \citet{bahdanau2016neural}. These models represent the pioneer studies in this area, they have also inspired studies using other neural architectures. Yet one architecture has prevailed upon the others for some time: the Transformer. The Transformer was introduced later, trying to provide implementation details that were complex to grasp when reading the original paper. Not only the architecture was shown in detail, but all the other components that help the Transformer reach its performance, such as beam search, positional encoding and the attention mechanism. Later, some Transformer variants that became popular in the literature were commented, exploring not only architecture variations but also the training objective.

In Chapter \ref{low_res_context} the definition of the low-resource term is given, along with the motivation that led us to follow this trend. Some practical challenges for performing the experiments were also discussed, mentioning how we approached the problem or how issues were addressed whenever possible. Subsequently, all the theory behind NMT related techniques such as Subword Embeddings, Transfer Learning (for word embeddings) and Back Translation was presented. Some of those techniques weren't created specifically for tackling low-resource issues, BPE for instance was aiming at more efficient embeddings that could solve the $<$unk$>$ token issue. Back Translation is one example of technique aligned with Data Augmentation purposes of solving low data availability by expanding the dataset without the need of extra datasets. 

Lastly, in chapter 5 the hypothesis we would like to clarify experimentally are mentioned, the connection between them and the experiment design is established, along with the expectations that were set for each one of them. The chapter starts by describing the datasets and the experimentation environment. Important information for reproducibility is given here, along with the assessment metrics. Later, the quantitative study was presented, where we compare different dataset content restrictions in experiment 1, and discover that low data availability has a less harmful effect than expected, with an increase from 83.3\% to 100\% of the dataset size even hindering performance. In experiment 2, the utility of subword embeddings and transfer learning is confronted, resulting in a worse performance than expected for Fast Text and a promising achievement for BPE. The effectiveness of Back Translation to address low resource is tested in experiment 3, where we discover that same domain augmentation overcomes different domain and that a synthetic ratio of 16.6\% yield the best results. Finally, we compare our best models against the Google Translate benchmark in experiment 4, and discover that we are able to reach 77.1\% of its performance, using the most complete assessment metric (sacrebleu). 

The qualitative study started by mentioning important challenges that to some extent had an impact on the outcome of the analysis, such as data quality issues and interaction bias with the translator. We don't believe they are relevant enough to invalidate any of the conclusions. Subsequently, the qualitative criteria was presented, which is basically composed of 2 dimensions: the error classes and the sentence complexities (based on CEFR \cite{COE}). The qualitative experiments are presented in the sequence, all focused on understanding multidimensional correlation between error classes and complexity or model plus technique. In such experiments, we use Fisher Exact Test to extract statistically relevant differences in both dimension pairs, Hierarchical Clustering to segment the dataset and identify bias within the clusters and Correspondence Analysis to find visual correlations between the dimension pairs. 

Our findings show that some techniques are more vulnerable to some error classes and efficient against others. In some cases all algorithms identify the same relationship between 2 dimensions, providing stronger evidence that the choice of a technique indeed has an impact on translation bias. For instance, all the algorithms show that Back Translation is efficient against the Meaning Deviation error and uncorrelated with Verb Tense, while the models trained on a reduced dataset are more vulnerable to the Meaning Deviation and Repetition errors. All experiments interpret both reduced models as having a similar relationship with the error classes, indicating that their translation bias probably converges. According to CA, BPE seems to be vulnerable to the Meaning Deviation, Verb Tense, Out of Context and Omission errors while still efficient against Insertion and, obviously, the $<$unk$>$ error.

Regarding complexity and error classes, the analysis has focused on understanding what relationship error classes have with an increase in complexity. The goal is to validate if the incidence of errors is higher or smaller when complexity increases, to check if that class is prone to a change purely based on complexity of if it is more correlated with the model or technique bias. The experiments show that most of the error classes (5 out of 8) behaved as directly proportional with complexity: Reference Matching, Omission, Out of Context, Verb Tense and Meaning Deviation. Meanwhile there are other errors that appear to have an inverse relationship with complexity, Insertion and $<$unk$>$, supported by all the experiments. One error possess no correlation with changes in complexity, so presenting more complex or less complex sentences to a model is unlikely to change its probability to happen, which is the repetition error. It is the only error that is potentially more prone to the bias of a model or a technique rather than the complexity.

The reader is encouraged to go through the chapters to better understand the findings mentioned here. The gaps we couldn't fill in our analysis and interesting directions to be explored will be mentioned in the next section.

\section{Next steps}

There are mainly 2 directions of improvement for this study: focusing on going further in depth in the qualitative analysis, by adding more sentences, qualitative criteria or experiments, or expanding its scope to contemplate more techniques. Both directions can be valuable to be explored, depending on the use case and objective. Reproducing the same sequence of experiments shown here can be promising specially for other researchers that wish to understand how can they address specific translation errors in their models. 

Unfortunately, our sample size of unique sentences could not go much further than 100 sentences, as the interaction with the translator required a budget that was limited. Because of this, we acknowledge the potential of enhancing the criteria if a bigger sample is analysed by a group of translation experts. Maybe there are other types of errors that could be covered, or even a category that was set but could be split into more granular categories to add more details. The qualitative criteria would benefit from receiving more samples and the review of other linguistic experts, thriving diversity and becoming more robust. That would help solving in more depth the subjectivity challenge and bringing more representativeness for the analysis. 

Considering the algorithms used to extract multidimensional associations, there might be also other algorithms that can be applied to tackle the same problem and reach a good fit when applied to the datasets used. This would enable the study to also derive other conclusions that haven't been considered here and could be useful. Hierarchical Clustering, Correspondence Analysis and Fisher Exact Test seem to complement each other very well, but the possibility of another algorithm to reveal unknown correlations is real.

Expanding the scope to analyse qualitatively more recent NMT algorithms can also help to understand if they are being efficient in addressing the known flaws of the vanilla Transformer, for instance. Maybe they are improving translation errors of some kind while sacrificing others, and perhaps a technique which a different qualitative profile can show up as a good complement for that model. This is a dimension of NMT performance that deserves further research and investigation, and has the potential to reveal new issues and unlock more consistent translation quality for modern Machine Translation models.

\backmatter
\bibliographystyle{coppe-unsrt}
\bibliography{example}

\appendix
\chapter{Algumas Demonstra{\c c}\~oes}

% Fazer uma releitura da dissertação como um todo, precisa ser breve e tocar nos pontos principais, não escrever mais do que duas páginas. 

% \definecolor{maroon}{cmyk}{0,0.87,0.68,0.32}
% \begin{table}[h]
% \caption{Next steps chronogram}
% \label{tab:citation}
% \centering
% {\footnotesize
% \begin{tabular}{| p{6cm} | p{3.5cm} | p{1.5cm} | p{3.5cm} |}
% \hline
% Deliverable & \verb|Start Date| & \verb|Duration| & \verb|End Date|\\
% \hline
% Run dataset experiments without subword, embeddings or data augmentation for CBIC paper & \verb|06/05/2021| & \verb|7 days| & \verb|13/05/2021|\\
% Run dataset experiments with subword and embeddings for CBIC paper & \verb|13/05/2021| & \verb|7 days| & \verb|20/05/2021|\\
% Implement data augmentation techniques & \verb|13/05/2021| & \verb|7 days| & \verb|20/05/2021|\\
% Run dataset experiments with data augmentation for CBIC paper & \verb|20/05/2021| & \verb|7 days| & \verb|27/05/2021|\\
% Review paper to CBIC (if possible to publish) & \verb|20/05/2021| & \verb|7 days| & \verb|27/05/2021|\\
% Submit paper to CBIC (if possible) & \verb|29/05/2021| & \verb|1 day| & \verb|30/05/2021|\\
% Properly fill equations and enrich discussion regarding dissertation & \verb|03/06/2021| & \verb|14 days| & \verb|17/06/2021|\\
% Come up with a formula to analyse whether a training setup will work on a single GPU based on parameters & \verb|17/06/2021| & \verb|21 days| & \verb|08/07/2021|\\
% \rowcolor{maroon!20} 
% Dissertation 1st review & \verb|24/06/2021| & \verb|7 days| & \verb|01/07/2021|\\
% Rethink the set of experiments to dissertation based on presented techniques & \verb|08/07/2021| & \verb|7 days| & \verb|15/07/2021|\\
% Run dissertation remaining experiments and fill results & \verb|08/07/2021| & \verb|21 days| & \verb|29/07/2021|\\
% Review, identify gaps and improve dissertation & \verb|15/07/2021| & \verb|14 days| & \verb|29/07/2021|\\
% \rowcolor{maroon!20} 
% Dissertation 2nd review & \verb|29/07/2021| & \verb|7 days| & \verb|05/08/2021|\\
% Write STIL paper & \verb|05/08/2021 (?) (TBD)| & \verb|14 days| & \verb|12/08/2021 (?) (TBD)|\\
% Run experiments to STIL & \verb|12/08/2021 (?) (TBD)| & \verb|7 days| & \verb|19/08/2021 (?) (TBD)|\\
% \rowcolor{maroon!20} 
% Examiner group proposition & \verb|12/08/2021| & \verb|7 days| & \verb|19/08/2021|\\
% STIL paper review & \verb|19/08/2021 (?) (TBD)| & \verb|7 days| & \verb|26/08/2021 (?) (TBD)|\\
% Act upon feedbacks given from examiner group & \verb|19/08/2021| & \verb|7 days| & \verb|26/08/2021|\\
% Submit paper to STIL & \verb|26/08/2021 (?) (TBD)| & \verb|1 day| & \verb|27/08/2021 (?) (TBD)|\\
% Review, identify gaps and improve dissertation & \verb|26/08/2021| & \verb|21 days| & \verb|16/09/2021|\\
% \rowcolor{maroon!20} 
% Dissertation final review & \verb|16/09/2021| & \verb|7 days| & \verb|23/09/2021|\\
% \rowcolor{maroon!20} 
% Dissertation presentation & \verb|23/09/2021| & \verb|7 days| & \verb|30/09/2021|\\
% \hline
% \end{tabular}}
% \end{table}

  
% \title{Tackling neural machine translation in low-resource settings: a Portuguese case study}

% % \author{Arthur T. Estrella\inst{1}, João B. O. Souza Filho\inst{2}}
% % \author{Author 1\inst{1}, Author 2\inst{2}}


% % \address{
% % % Instituto Alberto Luiz Coimbra de Pós-Graduação e Pesquisa de Engenharia -- Federal University of Rio de Janeiro (UFRJ)\\
% % Electrical Engineering Program (PEE/COPPE), Federal University of Rio de Janeiro\\ PO Box 68504, RJ 21941-972, Brazil
% % \email{atelles@coppe.ufrj.br, jbfilho@poli.ufrj.br}
% % % Department W, University X\\ Postal Code Y, City Z, Brazil
% % % \email{xxx@xxx.xxx.xx, xxx@xxx.xxx.xx}
% % }

% \begin{abstract}
% Neural machine translation (NMT) nowadays requires an increasing amount of data and computational power, so succeeding in this task with limited data and using a single GPU might be challenging. Strategies such as the use of pre-trained word embeddings, subword embeddings, and data augmentation solutions can potentially address some issues faced in low-resource experimental settings, but their impact on the quality of translations is unclear. This work evaluates some of these strategies on two low-resource experiments beyond just reporting BLEU:  errors are categorized on the Portuguese-English pair with the help of a translator, considering semantic and syntactic aspects. The BPE subword approach has shown to be the most effective solution, allowing a BLEU increase of 59\% p.p. compared to the standard Transformer.
% \end{abstract}

% % Neural machine translation (NMT) exploits deep models that require an increasing amount of data and computational power. However, succeeding in this task with limited data and using a single GPU can be challenging. Techniques such as pre-trained word embeddings, subword embeddings and data augmentation can potentially address some low-resource issues, but their impact on the quality of translations is unknown. This work evaluates the quantitative impact of such techniques and also classifies translation errors made by the model on the Portuguese-English pair, with a complexity drill down. This allows a better inference of the relations between translation quality aspects and the usual assessment metrics.
     
% % \begin{resumo} 
% % \end{resumo}

% \section{Introduction}

% % ------------- Joao
% % Since the creation of the Neural Machine Translation (NMT) branch, many solutions seem to have ignored the associated computational burden, solely focusing in surpassing the state-of-the-art. As a consequence, most models have been progressively adopting deeper architectures, increasing the number of parameters and requiring huger datasets. The excessive focus on boosting performance regardless complexity deviated researchers from a further criticism over how the proposed architectures address the translation task, if simpler models can achieve a similar performance than more complex ones, and how to cope with translation errors.

% Since the rise of the Neural Machine Translation (NMT) branch, many solutions solely focused on surpassing the state-of-the-art, ignoring the associated computational burden. Thus, most models have been progressively adopting deeper architectures, hugely increasing the number of network parameters and, as a result, the dependence on more extensive datasets. The excessive focus on boosting performance regardless of complexity deviated the researchers from a more profound criticism over how such architectures address the translation task, if more cost-effective models can be proposed, and how to better cope with translation errors.

% % Later, some efforts have been dedicated to the proposition of models and strategies to allow the NMT systems to achieve relevant performance, but considering less-complex and computing demanding solutions. 

% % ------------- Joao (VALE DESTACAR QUE PRE-TRAINED WORD EMBEDDINGS É A MESMA COISA QUE PRIOR DEFINITION. ACRESCENTEI BACK TRANSLATION QUE ESTAVA FALTANDO)
% % Low-resource domains in NMT can be roughly defined as practical development scenarios wherein the GPU memory and data available to train the models are more severely limited. Some techniques can potentially help under those circumstances, like the use of pre-trained word embeddings, the prior definition of the starting weights of the network embedding layer \cite{qi-etal-2018-pre} (warm-start), or even the production of embeddings at a subword level \cite{DBLP:journals/corr/SennrichHB15}. 

% Low-resource NMT domains are defined as practical development scenarios wherein the GPU memory and the amount of data available to train some model are limited. Some techniques can potentially help under those circumstances, as the prior initialization of neural network embedding weights \cite{qi-etal-2018-pre} with pre-trained word embeddings, the production of embeddings at a subword level \cite{DBLP:journals/corr/SennrichHB15} or data augmentation with a monolingual dataset \cite{DBLP:journals/corr/SennrichHB15a} (also known as back-translation). 

% % ------------- Joao (REDUZI ESSE PARAGRAFO TENTANDO MANTER A QUALIDADE)
% % To the best of our knowledge, experimental studies considering low-resource contexts, as discussing and evaluating the cost-effectiveness of strategies aiming to circumvent the practical issues faced in such domains, especially considering the Portuguese-to-English pair are missing in the literature. Many previous works only focus on optimizing metrics such as BLEU. Despite its usefulness, this index is limited due to only accounting for matches of fixed number of n-grams, penalizing correct but different lexical translations.

% To the best of our knowledge, experimental studies discussing and evaluating the cost-effectiveness of strategies aiming to circumvent the practical issues faced with low-resource domains, especially considering the English-to-Portuguese pair, are missing in the literature. Many previous works only focused on optimizing metrics such as BLEU. Despite its usefulness, this index is limited due to only accounting for matches of a fixed number of n-grams, penalizing correct but different lexical translations.

% % ------------- Joao (REDUZI ESSE PARAGRAFO TENTANDO MANTER A QUALIDADE)
% % This work uses Transformers \cite{DBLP:journals/corr/VaswaniSPUJGKP17} as one state-of-the-art reference model to  experimentally evaluate the impact of strategies like transfer learning, sub-word modeling, and data augmentation in the translation quality, considering a low-resource setting, characterized by the use of small and medium size databases, only one average size GPU, and the English-to-Portuguese pair. Additionally, a qualitative analysis over the translation errors produced by the models is derived over a sample of sentences by a native translator, considering a  multidimensional criterion, aiming to evaluate models' behavior in a wider scope than BLEU. 

% This work\footnote{ACKNOWLEDGMENT - This study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior – Brasil(CAPES) – Finance Code 001.} uses Transformers \cite{DBLP:journals/corr/VaswaniSPUJGKP17} to experimentally evaluate the impact of strategies such as transfer learning (by the use of pre-trained word embeddings), subword modelling, and data augmentation in the translation quality. It considers only one average size GPU and small to medium sized datasets (low-resource), focusing on the English-to-Portuguese pair. Additionally, a qualitative analysis of the translation errors is derived over a sample of sentences by a native translator, considering a multidimensional criterion, aiming to evaluate models' performance on a broader scope than BLEU. 

% % ------------- Joao (REDUZI ESSE PARAGRAFO TENTANDO MANTER A QUALIDADE)
% % This paper is structured as follows: Section II provides a brief coverage of the Transformer architecture applied in NMT taks, and Section III discusses the main issues to be tackled in low-resource domains along with some potential strategies to be exploited in these cases. Section IV provides a brief description of the datasets used in this work and depicts quantitative results for the strategies here considered, including a qualitative analysis for a subset of these models. Finally, section VI poses the conclusions and next steps.

% This paper is structured as follows: Section II provides a brief coverage of the Transformer architecture, and Section III discusses the main issues to be tackled in low-resource domains, along with some potential strategies that can be exploited in such cases. Section IV provides a brief description of the datasets considered in this work and depicts quantitative and qualitative results for the strategies here considered. Finally, Section VI poses the conclusions and next steps.

% \section{The Transformer Architecture}

% Transformers refers to a branch of algorithms based on the seminal work of \cite{DBLP:journals/corr/VaswaniSPUJGKP17}, representing the state-of-art. Differently from the sequential processing inherent to the Recurrent Neural Networks (RNN) adopted in previous NMT models, the Transformer model processes large sentences in parallel, establishing a richer set of interrelations between source and target sentence words, thus leading to a better inference of the words' context and a higher performance with long sentences. The reader is referred to the original paper for more details about this architecture.

% % REMOVING THIS TO SAVE SPACE -------------------
% % The model has an encoder and a decoder. The former is responsible for mapping an input sequence of symbol representations to a sequence of continuous representations, while the latter generates an output sequence of symbols iteratively, and these symbols are the most probable translations to the input. Both contain 6 stacked Transformer layers. In the encoder, each layer contains a multi-head self-attention mechanism, followed by layer normalization and a feed-forward layer. The structure in the decoder replicates the encoder, with the addition of an extra masked multi-head attention layer over the output of the encoder stack before multi-head attention. The reader is referred to as the original paper to have more details about its architecture.
% % ---------------------------------

% % --------------- ATTEMPT 2
% % Another contribution of this model is the concept of Multi-head attention, where each attention head only processes a subset of the dimensions from the original input vectors. At first there was a belief that an increase in multi-head self-attention units consistently contributed to a better Transformer performance, a fact refuted in a later study \cite{DBLP:journals/corr/abs-1905-09418}. According to this reference, only a small subset of heads are enough to sustain the Transformer translation performance. By visualizing the importance distribution through a heat map correlating each output word to every input word, the authors discovered that several heads learn similar dependency mappings. Thus, considering this map as a qualitative feature importance measure per head, after pruning the heads with equivalent dependency maps, no noticeable loss in translation quality was observed.

% % The study is an evidence of a common behaviour of the field: researchers that have access to powerful multi-GPU infrastructure usually focus on raising the performance bar while overlooking costs. Bypassing translation quality issues is safer since this kind of analysis is subjective and may not leverage the final result as expected. 
% % --------------- ATTEMPT 2

% % --------------- ATTEMPT 1
% % Transformers refers to a branch of algorithms based on the seminal work of \cite{DBLP:journals/corr/VaswaniSPUJGKP17}. Differently from the sequential processing adopted in RNN based models, the Transformer takes large sentences and process them in parallel, it is capable of assigning weights even with unequal many-to-many relations of words between the source and target sentence. Theoretically, this gives the Transformer advantages when dealing with long sentences when compared with RNNs. Another innovation to this model is the concept of Multi-head attention, which may be interpreted as a strategy of dimensionality reduction that easies the model training, reducing its complexity, since each attention head only processes a subset of the dimensions from the original input vectors.
  
% % This work raised a relevant question to the NMT community: are the current deep learning models biased towards increasing the complexity as well as the number of model parameters unnecessarily? Cost-effective models have a smaller carbon footprint and energy waste, leading to a lower impact on nature, this could bring the NLP community closer to the status of green AI, hence increasing credibility of deep learning research.
% % --------------- ATTEMPT 1

% \section{Tackling low-resource settings}\label{sec:lowres}

% Low-resource constraints refer to limitations on dataset quality/size and computational resources available. Small datasets can be strongly biased in specific contexts, which may induce the predictions produced by the decoder model to move away from the reference or even turn the training process unstable,  reducing the final model performance. In turn, computational aspects are primarily related to the number of GPUs available and their standalone memory. Memory constraints directly affect the definition of training and model hyperparameters, such as the batch size, the number of hidden layers, the embedding size, and the size of the attention mechanisms. To avoid an experiment failure due to an out of memory error, one should first consider the largest sentence size, a common batch size limiting factor. Too small batches may lead to unstable and biased training, increasing the epoch time and resulting in sub-optimal translation quality. Moreover, the 
% estimation a priori of a minimum quantity of sentences for an adequate translation is also a challenging task, severely depending on the complexity of the application domain.
% Hopefully, the following strategies may mitigate the need for abundant data and GPU memory in the translation task:

% \subsection{Transfer learning methods}

% % In the NMT context,  this technique was adopted in \cite{zoph-etal-2016-transfer}, considering that the weights derived with a high resource language pair model can be transferred to another unrelated low-resource language pair, both cases with English as the target language. As a result, according to BLEU score, the newly trained NMT model surpassed the previous state-of-the-art system.

% % NOT MANDATORY - CAN BE REDUCED/REMOVED ---------
% % In the NMT context, one of the first works to explore the prior initiatilization of word embeddings was \cite{zoph-etal-2016-transfer}. They transfer the weights derived with a high resource language pair model to another unrelated low-resource language pair model, both cases with English as the target language. As a result, according to BLEU score, the newly trained NMT model surpassed the previous state-of-the-art system.

% % Another technique that generates a starting point for the embedding layer in neural models uses pre-trained word embeddings, trained in a unsupervised fashion. This initialization is helpful since it assigns vectors with values that reflect semantical and synthetical properties learned by the embeddings, instead of the default random initialization. The use of pre-trained words can be effective in low-resource scenarios, as points out the analysis presented in \cite{qi-etal-2018-pre}. They mention the existence of a "sweet spot" for the dataset size to which the word embeddings are more effective, usually when it isn't big enough for training a given model, leading to convergence issues. Another finding is that this strategy seem to be more effective when applied to translation tasks that contain more related language pairs, such as Spanish and Portuguese. 
% % ------------------------------

% Transfer learning exploits other application parameters as initial values for the NMT model training (warm-start). A typical example refers to using pre-trained word embeddings in the embedding layer of neural models instead of the default random parameters' initialization. This procedure accelerates and improves training since these embeddings already carry out some semantic word meanings to be subsequently refined to a particular translation task (fine-tuning).    

% The use of pre-trained words can be pretty effective in low-resource scenarios, as pointed in the analysis conducted in \cite{qi-etal-2018-pre}. According to the authors, there is a ''sweet spot'' for dataset size, according to which this strategy is more effective. Similarly, the impact over more related  language pairs is often higher, such as Spanish and Portuguese.

% \subsection{Subword methods}

% Subword embeddings represent a useful strategy to reduce the out of vocabulary (OOV) occurrences. The central idea is decomposing words into sub-parts (character groups), which are common to many words, turning the model less susceptible to the vocabulary content and its size. This technique, referred to as BPE (Byte Pair Encoding) - a compression algorithm, was introduced by \cite{DBLP:journals/corr/SennrichHB15}. 
% Roughly, BPE breaks the words in a corpus into smaller parts (the smallest BPE unit is a character); some of them subsequently merged, with the number of merge operations being the main hyperparameter to be tuned. The main BPE drawback is the impossibility of defining a maximum vocabulary size a priori. A clear advantage of moving from word-level to subword NMT using BPE is reported in  \cite{sennrich-zhang-2019-revisiting}: an increase of BLEU score from 7.2 to 16.6 in an ultra-low-resource setting as well as a consistent rise in the BLEU values for a wide range of application scenarios.


% % \textcolor{red}{Can be reduced} 
% % The pioneer work introducing subword embeddings is due to \cite{DBLP:journals/corr/SennrichHB15}, referred to as BPE (Byte Pair Encoding) - a compression algorithm. The subword tokens are generated in 4 steps: (1) the definition of the number of merge operations for character sequences, (2) the representation of each word in the corpus as a combination of the characters along with a special end-of-word token, (3) the iterative counting of character pairs in all tokens of the vocabulary, and (4) the repetition of step 3 until the desired number of merge operations is completed or when the maximum word frequency achieves 1. The number of merge operations represents the main hyperparameter to be tuned in this algorithm, but one inconvenience of using it is the impossibility of defining the maximum vocabulary size a priori. When changing from word-level to subword NMT using BPE, a later work \cite{sennrich-zhang-2019-revisiting} reported an increase of BLEU score from 7.2 to 16.6 in a ultra-low-resource setting. Consistent increases in BLEU were reported for a wide range of scenarios.

% % NOT MANDATORY - CAN BE REDUCED/REMOVED

% % Pre-trained word embeddings also followed the subword trend with the proposition of the \mbox{Fast Text} algorithm \cite{DBLP:journals/corr/BojanowskiGJM16}. This algorithm treats words as a bag of character n-grams, and adds tokens that allow to distinguish prefixes and suffixes from other character sequences. Each word is assigned wtith a given number of n-grams, and in practice n usually is greater than or equal to 3 or smaller than 6. Finally, a word is represented by an index and the sum of the vector representations of its correspondent set of n-grams: the subwords.

% Pre-trained word embeddings also followed the subword trend with the proposition of the \mbox{Fast Text} algorithm \cite{DBLP:journals/corr/BojanowskiGJM16}. This technique treats words as a bag of character n-grams and adds tokens to distinguish among prefixes, suffixes, and other character sequences. In practice, each word is assigned to a given number of $n$-grams, typically $3 \leq n <6$. Finally, the word is represented by the sum of the vector representations (embeddings) associated with the n-grams  composing it.

% \subsection{Data Augmentation}

% Large-scale parallel corpora is not a common resource for most existing language pairs, unlike monolingual corpora. However, is it possible to exploit the abundant and large monolingual datasets widely available these days for data augmentation? The answer is yes and this technique is referred to as Back-Translation (BT) \cite{DBLP:journals/corr/SennrichHB15a}.  
% The idea is quite simple: let us  consider the development of a translation model from a language A to B. New pairs of sentences can be simply synthesized by training an inverse model, i.e., a translator from the language B to A, with the same sentence pairs. Once completed this  training, this auxiliary model can be fed with corpora from a similar or a different context domain to produce new sentence pairs. BT has shown to be a simple but effective method to address low data availability in many domains, as shown in \cite{DBLP:journals/corr/abs-1804-06189}, often increasing translation performance.  

% % Other ways of generating synthetic data are presented and compared in the work presented by \cite{DBLP:journals/corr/abs-1906-03785}. Augmentation methods are discussed in scenarios with high resource languages (HRL) and low-resource languages (LRL), where it can be performed via pivoting, word substitution (with the use of a dictionary) and even through unsupervised methods. They also introduce a two-step pivoting method that can use a HRL to create artificial examples for a LRL, when only a small dataset with both languages is available. This is particularly useful for languages that have even less available resources than Portuguese. 

% \section{Experimental Setup}

% % \subsection{Datasets of this work}

% Datasets with low to medium complexity levels were carefully picked for the proposed analysis: Tatoeba \cite{DBLP:journals/corr/abs-2010-06354} and TED Talks \cite{cettolo-ted-talks}. Both represent a low-resource scenario due to the scarce number of sentences, in alignment with some references \cite{sennrich-zhang-2019-revisiting} \cite{zoph-etal-2016-transfer}. 
% News Commentary v16 \cite{TIEDEMANN12.463} 
% is a different domain monolingual dataset used for BT and includes a rich range of sentences in terms of content and complexity.

% The reduced Tatoeba dataset contains 143.8k small and basic to intermediate English level sentences, posing a low complexity challenge for the NMT task. It includes 26.3k unique words in PT and 15.3k in EN. TED Talks is a medium-size dataset covering a range of subjects, including from low to high complexity sentences.  

% In the experiments, 10\% of Tatoeba was held out for testing, while the remaining data was split into 10\% for validation and 90\% for training, using a seed equal 0. Despite TED disposing predefined training, test, and validation sets, the original validation set is too small (906 sentences), leading us to move the last 20 talks (2081 sentences) from the training set to this set.
% As a result, the training set contains 236.1k (1918 talks) sentences, randomly sampled to defining training batches using a seed equal to 157, and the test set includes 11.4k sentences. 
% Additionally, all text was pre-processed to eliminate all XML enclosed sentences and tags, except for the ones related to title and description.

% The experiments were performed on a single GPU, using Google Colaboratory and Kaggle infrastructure. Typically such environments dispose of NVIDIA GPUs like Tesla P100, Tesla K80 or Tesla T4, with a GPU memory ranging from  12GB to 16GB. 

% \section{Results}

% % ------------- Joao (REDUZI ESSE PARAGRAFO TENTANDO MANTER A QUALIDADE)
% % All Transformer models adopted the following parameters: $d_{{model}}=256$, $d_{{ff}}=256$, 8 attention heads and the Q, K and V square matrices have dimension 64. The exception were the models exploiting the pre-trained word embeddings made available in \cite{DBLP:journals/corr/abs-1708-06025} to which $d_{{model}}=300$  and the number of attention heads was 6. The optimizer was Adam with $\beta = (0.9, 0.98)$ and $\epsilon = 10^{-8}$. The learning rate of all variants was set to $10^{-4}$ and the beam search considered a beam of size 3. The early stopping criterion was based on validation perplexity behavior for 10 epochs, halting training in case of performance stagnation.

% Regarding the Transformers, the parameters adopted were $d_{{model}}=256$, $d_{{ff}}=256$, 8 attention heads, and Q, K and V square matrices with dimension 64. The  pre-trained \mbox{Fast Text}-based models, which employed embeddings described in \cite{DBLP:journals/corr/abs-1708-06025}, are exceptions, considering  $d_{{model}}=300$ and 6 attention heads. All variants adopted the Adam optimizer with $\beta \in (0.9, 0.98)$ and $\epsilon = 10^{-8}$, a learning rate of $10^{-4}$ and the beam search considered a beam with size 3. The early stopping criterion was based on the validation perplexity behaviour for ten epochs, halting the training in case of performance stagnation.


% Sacrebleu \cite{post-2018-call} and NLTK \cite{Loper02nltk:the} are two BLEU variants used for  assessing the performance of the models. The major difference between them resides in a stronger Sacrebleu's penalization over cases where the translated and reference sentences differ in length.

% % \subsection{Restricting database content}
% \subsection{Effects of restricting dataset content}
% \label{sec_res_data_cont}

% % ------------- Joao (REDUZI ESSE PARAGRAFO TENTANDO MANTER A QUALIDADE)
% % To shed a light over possible impacts on the performance of NMT models, when submitted to application scenarios with severe restrictions on the number of pair of sentences available for model development, we consider a hypothetical scenario in which only a fraction of TED and Tatoeba training sets are used for model learning. This undersampling procedure considered the following percentages: 33.3\%, 50\%, 66.6\%, 83.3\%. Table 1 summarizes the results.

% To shed light on the possible effects of limited data on the performance of NMT models, we considered a hypothetical experimental scenario where only a fraction of TED and Tatoeba training sets were used in training, according to the following percentages: 33.3\%, 50\%, 66.6\%, 83.3\%. Table 1 summarizes the results.

% % \begin{table}[ht]
% % \centering
% % \caption{Data augmentation scores}
% % \label{tab:exTable1}
% % \includegraphics[width=.98\textwidth]{augmentation_table.png}
% % \end{table}

% \begin{table}[h]
% \caption{Data augmentation scores}
% \fontsize{10}{12}\fontfamily{phv}\selectfont
% \begin{tabular}{| p{0.12\linewidth} | P{0.1\linewidth}  | P{0.07\linewidth}  | P{0.07\linewidth}  | P{0.07\linewidth}  | P{0.1\linewidth}  | P{0.07\linewidth}  | P{0.07\linewidth}  | P{0.07\linewidth} |}
%     \hline
%     \multirow{2}{2.0cm}{Fraction of the Dataset} & \multicolumn{4}{|c|}{Tatoeba} & \multicolumn{4}{|c|}{TED}\\
%     \cline{2-9}
%     & Sacrebleu & NLTK BLEU & Batch Size & Epochs & Sacrebleu & NLTK BLEU & Batch Size & Epochs \\
%     \hline
%     33.3\% & 48.64 & 67.09 & 512 & 76 & 24.7 & 57.65 & 30 & 40\\
%     \hline
%     50\% & 52.53 & 70.12 & 512 & 65 & 25.18 & 56.46 & 30 & 40\\
%     \hline
%     66.6\% & 55.3 & 72.12 & 512 & 58 & 26.22 & \textbf{56.81} & 29 & 36\\
%     \hline
%     83.3\% & 56.24 & 73.18 & 512 & 58 & \textbf{26.74} & 56.57 & 28 & 30\\
%     \hline
%     100\% & \textbf{57.99} & \textbf{74.07} & 512 & 58 & 25.24 & 55.36 & 28 & 30\\
%     \hline
% \end{tabular}
% \end{table}

% % ------------- Joao (REDUZI ESSE PARAGRAFO TENTANDO MANTER A QUALIDADE)
% % Results show a monotonic behavior of BLEU metrics for both databases. Besides, the much higher BLEU values achieved for Tatoeba compared to TED models (almost twice for Sacrebleu) corroborates with higher complexity of TED database. A exception is a slightly drop in BLEU values in two cases: Sacrebleu (100\%  $\times$ 83.3\%) and NLTK (100\%  83.3 and 83.3\% $\times$ 66.6 \%). Reasons behind these findings may include: (1) the possible use of synonyms in the translations, aspect ignored by BLEU metrics; (2) a higher number of repetition errors due to data quality issues (an aspect discussed in more details in the following section); (3) the richer content of TED which might have led to the inclusion of talks on less similar subjects than those covered in the test set, deserving a future further investigation. Surprisingly, models considering only 66.6\% of training datasets perform quite well.

% Results show that the Sacrebleu scores for Tatoeba were about twice the achieved with TED, corroborating with the much higher complexity of the latter. The BLEU metrics for both datasets have shown a monotonic behaviour, with exceptions to TED in two cases: Sacrebleu (100\%  $\times$ 83.3\%) and NLTK (100\% $\times$ 83.3 and 83.3\% $\times$ 66.6 \%). The reasons for such findings may include: (1) the possible use of synonyms in the translations, an aspect ignored by any BLEU metric; (2) a higher incidence of repetition errors due to data quality issues (to be discussed further in the following section); (3) the more complex and richer TED content, which might have led to a wider subject coverage in the training set, reducing model accuracy, a hypothesis deserving a future investigation. Finally, models developed with a fraction of the original training datasets ( $66.6\%$) performed surprisingly well.

% \subsection{Effects of transfer learning and subword embeddings strategies}

% Aiming to evaluate the leveraging effects of pre-trained \mbox{Fast Text} and BPE \cite{DBLP:journals/corr/SennrichHB15} strategies in low-resource NMT tasks, BPE models were implemented in the Texar framework  \cite{hu2019texar} (PyTorch version). In contrast, the alternative models considered a customized PyTorch \cite{NEURIPS2019_9015} solution. Table 2 exhibits these results, reproducing the last line of Table 1 to allow an easier   comparison of the  results. 

% % BACKUP
% % \begin{table}[h]
% % \caption{Transfer learning and subword embeddings translation results}
% % \fontsize{10}{12}\fontfamily{phv}\selectfont
% % \begin{tabular}{| p{2.2cm} | p{1.5cm}  | p{1.0cm}  | p{0.9cm}  | p{1.1cm}  | p{1.5cm}  | p{1.0cm}  | p{0.9cm}  | p{1.1cm} |}
% %     \hline
% %     \multirow{2}{2.1cm}{Technique applied} & \multicolumn{4}{|p{5.0cm}|}{Tatoeba} & \multicolumn{4}{|p{5.0cm}|}{TED}\\
% %     \cline{2-9}
% %     & Sacrebleu & NLTK BLEU & Batch Size & Epochs & Sacrebleu & NLTK BLEU & Batch Size & Epochs \\
% %     \hline
% %     None & 57.99 & 74.07 & 512 & 54 & 25.24 & 65.36 & 34 & 29\\
% %     \hline
% %     \mbox{Fast Text} & 56.96 & 69.91 & 512 & 50 & 24.07 & 61.69 & 30 & 45\\
% %     \hline
% %     Subword BPE & \textbf{66.63} & \textbf{83.02} & 512 & 40 & \textbf{40.26} & \textbf{72.20} & 32 & 40\\
% %     \hline
% % \end{tabular}
% % \end{table}

% \begin{table}[h]
% \caption{Transfer learning and subword embeddings translation results}
% \fontsize{10}{12}\fontfamily{phv}\selectfont
% \begin{tabular}{| p{0.15\linewidth} | P{0.1\linewidth}  | P{0.06\linewidth}  | P{0.06\linewidth}  | P{0.07\linewidth}  | P{0.1\linewidth}  | P{0.06\linewidth}  | P{0.06\linewidth}  | P{0.07\linewidth} |}
%     \hline
%     \multirow{2}{2.1cm}{Technique applied} & \multicolumn{4}{|c|}{Tatoeba} & \multicolumn{4}{|c|}{TED}\\
%     \cline{2-9}
%     & Sacrebleu & NLTK BLEU & Batch Size & Epochs & Sacrebleu & NLTK BLEU & Batch Size & Epochs \\
%     \hline
%     None & 57.99 & 74.07 & 512 & 58 & 25.24 & 55.36 & 28 & 30\\
%     \hline
%     \mbox{Fast Text} & 56.96 & 69.91 & 512 & 50 & 24.07 & 51.69 & 30 & 45\\
%     \hline
%     Subword BPE & \textbf{66.63} & \textbf{83.02} & 512 & 40 & \textbf{40.26} & \textbf{72.20} & 32 & 40\\
%     \hline
% \end{tabular}
% \end{table}

% % ------------- Joao (REDUZI ESSE PARAGRAFO TENTANDO MANTER A QUALIDADE)
% % Results point out that the \mbox{Fast Text} was an ineffective strategy, achieving a disappointing performance in both databases. Despite BPE also exploiting word embeddings, the gains observed in this case were impressive. One hypothesis for the bad performance of the former is a possible overspecialization of its word embeddings to other text domains, as it was created using content mined by a crawler \cite{DBLP:journals/corr/abs-1708-06025}. However, this finding requires a further investigation, on course now. Besides,  BPE prevents the generation of $<$unk$>$ tokens, but may lead to nonexistent or different words from those considered by the reference. Nonetheless, the higher gains of using BPE in TED talk ($\approx 15$ percentual points) compared to Tatoeba ($\approx 9$ percentual points) signalizes that BPE might be an effective strategy when dealing with more complex NMT tasks, since TED includes a more diverse vocabulary, thus is more prone to the occurrence of OOV words. 

% Curiously, the use of \mbox{Fast Text} embeddings is associated with an unexpected performance drop for both datasets. Conversely, 
% the gains observed with BPE, which also exploits word embeddings, were impressive. One hypothesis for the bad \mbox{Fast Text} performance is a possible overspecialization to other text domains, since it was produced with content mined by a crawler \cite{DBLP:journals/corr/abs-1708-06025}. The higher BPE gain  in TED (15.02) compared to Tatoeba (8.64) signalizes 
% the effectiveness of BPE in dealing with more complex NMT scenarios, especially regarding a more diverse vocabulary, avoiding OOV occurrences.

% \subsection{Effects of the Back-Translation (BT) strategy}

% The BT experiments were restricted to the TED dataset. Data augmentation was performed with synthetic sentences produced with the own TED (using its left out sentences) and with the News dataset. These experiments aimed to verify if data augmentation can result in higher BLEU scores under low-resource constraints. 

% A single EN-PT Transformer was trained with the entire TED dataset to generate the synthetic sentences, reaching 27.73 and 63.8 points for the Sacrebleu and NLTK, respectively. The subset of back-translated sequences appended to the training sets was randomly sampled using the following seeds: 157 (TED) and 0 (News).

% \begin{table}[h]
%   \caption{TED Talks back-translation results}
%   \fontsize{10}{12}\fontfamily{phv}\selectfont
%   \label{tab:table3-bt-results}
%   \centering
%   {\footnotesize
%   \begin{tabular}{| p{6.1cm} | P{1.6cm} |  P{2.4cm} | P{1.5cm} | P{1.1cm} |}
%     \hline
%     Technique applied & Batch size & Epochs Trained & Sacrebleu & NLTK BLEU \\
%     \hline
%     None (Original TED) & 30 & 27 & 25.24 & 55.36\\
%     \hline
%     Reduction of TED to 50\% & 40 & 30 & 25.18 & \textbf{56.46}\\
%     \hline
%     BT (50\% of News synthetic examples) & 34 & 33 & 21.80 & 51.34\\
%     \hline
%     BT (50\% of TED synthetic examples) & 34 & 28 & \textbf{25.95} & 56.42\\
%     \hline
%     Reduction of TED to 66.6\% & 36 & 29 & 26.22 & 56.81\\
%     \hline
%     BT (33.3\% of News synthetic examples) & 34 & 27 & 24.12 & 53.77\\
%     \hline
%     BT (33.3\% of TED synthetic examples) & 34 & 27 & \textbf{27.54} & \textbf{58.95}\\
%     \hline
%     Reduction of TED to 83.3\% & 28 & 30 & 26.74 & 56.57\\    
%     \hline
%     BT (16.6\% of News synthetic examples) & 34 & 29 & 31.28 & 63.30\\
%     \hline
%     BT (16.6\% of TED synthetic examples) & 34 & 27 & \textbf{34.62} & \textbf{64.61}\\
%     \hline
% \end{tabular}}
% \end{table}

% % ------------- Joao (REDUZI ESSE PARAGRAFO TENTANDO MANTER A QUALIDADE)
% % Table \ref{tab:table3-bt-results} exhibits the results. For a more severe restriction on the dataset (50\%), the use of other domain synthesized sentences is harmful to model performance, while considering own-domain sentences is useless as results in practically the same BLUE values obtained without any strategy. However, for lower percentages of synthesized data, positive effects of including synthesized data start to appear. Considering an intermediate restriction ($\approx 33 \%$), the use of same domain sentences in back-translation leads to a mild increase in both BLEU values, signalizing that such "noisy" sentences may contribute to model performance, as the BLEU values in this case are slightly better than those obtained by the model developed over all data (Original TED). Finally, considering a less strict restriction ($\approx 16.6 \%$), the same and different domains addiction are quite effective, resulting in models that largely surpasses the Original TED model. 

% Table \ref{tab:table3-bt-results} exhibits the results. For a more severe restriction on the dataset size (50\%), using other domain synthesized sentences is harmful to model performance, while own-domain synthesis resulted in a marginally better BLEU score. However, for a lower percentage of synthetic data, positive effects start to appear. Considering an intermediate restriction ($\approx 33 \%$), using the same domain sentences in back-translation led to a mild increase in both BLEU values compared to the Original TED, signalizing that such "noisy" sentences may contribute to increasing translation quality. Finally, considering a small restriction ($\approx 16.6 \%$), both domain approaches are quite effective, resulting in models that largely surpasses the model developed over original data.  

% % \begin{figure*}[tb]
% % \centering 
% % \includegraphics[keepaspectratio,scale=0.5]{model_and_complexity.png}
% % \caption{Radar charts for error classes per model and complexity}
% % \label{error_classes_per_model_and_complexity}
% % \end{figure*}

% \subsection{Subjective evaluation}


% % The evaluation of error classes in model guesses was refined until reaching 8 categories, their descriptions are shown in table \ref{error_classes_descriptions}. For illustration purposes, Table \ref{evaluation_example} contain 4 arbitrary sentences included in this analysis and classified by the specialist. Their complexity is classified and justified, along with the errors spotted in the translation given by the model. The first model guess obtained a Sacrebleu of 38.26 and the second 8.89.

% Regarding the identification of error patterns, a multidimensional evaluation in eight categories was considered: Reference Matching, Omission, Out of Context, Verb Tense, sentence choice (the translation is OK, but the outcome is entirely different from the reference), Insertion, repetition and $<$unk$>$ errors \footnote{A detailed error description and some evaluation samples can be found at \url{https://github.com/Art31/pt-nmt-low-resource.git}.}.
% Table \ref{error_classes_per_model_and_complexity} shows the number of errors committed by each model, stratified by sentence complexity and error category. Considering the limitations of such analysis, such as the reduced sample and the analysis of only one translator,  
% both models performed quite similarly regarding the "similar word choice" occurrence. Nonetheless, the BPE produced fewer errors related to "Omission" (levels A1 and B1), "sentence choice" (B1), "Insertion" (A1, A2 e B1), "repetitions" (all), and "$<$unk$>$ errors" (all), performing worse regarding "Out of Context" and "Verb Tense".


% % \begin{table}[htb!]
% % \caption{Algoritmo e Parâmetros}
% % \centering
% % \noindent\resizebox{1\textwidth}{!}{%
% % \begin{tabular}{c|r|l}

% % Algoritmo & Parâmetros & Valores \\ 
% % \hline
% % RF &
% %     \rightspecialcell{
% %     'n estimators':\\
% %         'min samples leaf':\\
% %         'max features':\\
% %      } &
% %     \rightspecialcell{
% %      8, 16, 32, 64, 128, 256, 512, 1024, 2048, 3000,\\
% %          1, 2, 5 ,\\
% %         'sqrt', 10, 11, 12, 14, 15, 16 }
% %     \\ 
% %     \hline

% % LGR &
% %     \rightspecialcell{
% %         'solver':\\
% %         'penalty':\\
% %         'C': \\ \\ \\
% %         'max iter':\\
% %     }
% %     &
% %     \rightspecialcell{
% %         'newton cg','lbfgs','liblinear','sag','saga',\\
% %         'l1','l2',\\
% %         list(sorted( set(\\
% %         list(np.logspace( \-3,3,7))\\
% %         + list(np.logspace( \-4, 4, 20))))),\\
% %         100, 200, 4000  \\
% %     }
% %     \\ \hline


% % \end{tabular}
% % \label{acmTab:algoritmosEstruturadosEparametros}
% % %
% % }
% % \end{table}


% % The error analysis will focus on 2 dimensions: identify whether there are relevant differences in the share of errors per technique and if there is some correlation between complexity and error classes. The pattern drawn in the polar chart can be seen as a sort of footprint. The share of errors classified with complexity drill down is given on Figure \ref{error_classes_per_model_and_complexity}. The different patterns drawn in the radar chart show that A1 and B2 complexities provide different challenges to the models when compared to the rest. Naturally, the amount of errors increase with complexity. The BPE error pattern is also significantly different when compared to the other variants.

% % \begin{table}[ht]
% % \centering
% % \caption{Percentage of Error Classes per Complexity}
% % \label{error_classes_per_model_and_complexity}
% % \includegraphics[width=1\textwidth]{model_complexity_error.png}
% % \end{table}

% \begin{table}[!htp]
% \caption{Class-error ratios per dataset and sentence complexity.}
% \fontsize{10}{12}\fontfamily{phv}\selectfont
% \label{error_classes_per_model_and_complexity}
% \centering
% \begin{tabular}{| p{0.9cm} | P{1.2cm}  | P{1.1cm}  | P{0.9cm}  | P{1.2cm}  | P{1.0cm}  | P{1.4cm}  | P{0.9cm}  | P{1.1cm} |  P{1.0cm} |}
%     \hline
%     Model Name & Comple-xity & Similar word choice & Omis-sion & Out of context & Verb tense & Sentence choice & Inser-tion & Repeti-tion & $<$unk$>$ error\\
%     \hline
%     \multirow{5}{1.2cm}{66\% TED} & A1 & $\nicefrac{2}{10}$ & $\nicefrac{3}{10}$ & $\nicefrac{0}{10}$ & $\nicefrac{0}{10}$ & $\nicefrac{1}{10}$ & $\textbf{\nicefrac{6}{10}}$ & $\nicefrac{3}{10}$ & $\nicefrac{2}{10}$\\
%     \cline{2-10}
%     & A2 & $\nicefrac{7}{10}$ & $\nicefrac{6}{10}$ & $\nicefrac{1}{10}$ & $\nicefrac{1}{10}$ & $\nicefrac{3}{10}$ & $\nicefrac{4}{10}$ & $\nicefrac{2}{10}$ & $\nicefrac{2}{10}$\\
%     \cline{2-10}
%     & B1 & $\nicefrac{7}{10}$ & $\nicefrac{5}{10}$ & $\textbf{\nicefrac{3}{10}}$ & $\nicefrac{3}{10}$ & $\nicefrac{3}{10}$ & $\nicefrac{2}{10}$ & $\nicefrac{2}{10}$ & $\nicefrac{3}{10}$\\
%     \cline{2-10}
%     & B2 & $\textbf{\nicefrac{8}{10}}$ & $\nicefrac{7}{10}$ & $\textbf{\nicefrac{2}{10}}$ & $\textbf{\nicefrac{7}{10}}$ & $\textbf{\nicefrac{5}{10}}$ & $\nicefrac{5}{10}$ & $\textbf{\nicefrac{6}{10}}$ & $\textbf{\nicefrac{5}{10}}$\\
%     \cline{2-10}
%     & Average & 60.0\% & 52.5\% & 15.0\% & 27.5\% & 30.0\% & \textcolor{red}{\textbf{42.5\%}} & \textbf{\textcolor{red}{32.5\%}} & \textbf{\textcolor{red}{30.0\%}}\\
%     \hline
%     \multirow{5}{1.2cm}{BPE} & A1 & $\nicefrac{2}{10}$ & $\nicefrac{0}{10}$ & $\nicefrac{0}{10}$ & $\nicefrac{1}{10}$ & $\nicefrac{1}{10}$ & $\nicefrac{2}{10}$ & $\nicefrac{0}{10}$ & $\nicefrac{0}{10}$\\
%     \cline{2-10}
%     & A2 & $\nicefrac{7}{10}$ & $\textbf{\nicefrac{6}{10}}$ & $\textbf{\nicefrac{3}{10}}$ & $\nicefrac{4}{10}$ & $\textbf{\nicefrac{3}{10}}$ & $\nicefrac{1}{10}$ & $\nicefrac{0}{10}$ & $\nicefrac{0}{10}$\\
%     \cline{2-10}
%     & B1 & $\nicefrac{4}{10}$ & $\nicefrac{3}{10}$ & $\nicefrac{1}{10}$ & $\nicefrac{3}{10}$ & $\nicefrac{1}{10}$ & $\nicefrac{0}{10}$ & $\nicefrac{0}{10}$ & $\nicefrac{0}{10}$\\
%     \cline{2-10}
%     & B2 & $\textbf{\nicefrac{8}{10}}$ & $\nicefrac{5}{10}$ & $\textbf{\nicefrac{3}{10}}$ & $\textbf{\nicefrac{5}{10}}$ & $\nicefrac{2}{10}$ & $\textbf{\nicefrac{6}{10}}$ & $\textbf{\nicefrac{1}{10}}$ & $\nicefrac{0}{10}$\\
%     \cline{2-10}
%     & Average & 52.5\% & 35.0\% & 17.5\% & 32.5\% & 17.5\% & \textbf{\textcolor{red}{22.5\%}} & \textbf{\textcolor{red}{2.5\%}} & \textbf{\textcolor{red}{0.0\%}}\\
%     \hline
% \end{tabular}
% \end{table}

% %Based on the error class gap between both models, the Fisher Exact Test was used to check if we can assume with statistical relevance that they have different performance, considering one error class versus the rest. Consider a confusion matrix with rows representing the models, a column representing the error and another column representing the other errors for this. Whenever a p-value is lower than $0.05$ the null hypothesis can be rejected and it is possible to say that one model makes more mistakes of that class than the other. The analysis concluded that the repetition error is indeed more frequent for the 66\% variant when compared to BPE ($p=0.0036$). The same test was applied to relate complexity and classes of errors, and it indicated that the Insertion error is more frequent in all complexities when compared to A1, and B1 has more Out of Context errors than A1.

% Results from Table \ref{error_classes_per_model_and_complexity} underwent a Multiple Fisher test to evaluate if the 
% differences observed between
% the error ratios of the 
% two models are statistically significant. 
% This analysis considered multiple 2x2 tables (one to each class of error), with rows defining the model and columns associated with the occurrence or not of some class of error. The significance level was set to 5\%; thus, the null hypothesis was rejected whenever the $p$-value was lower than $0.05$,
% representing a statistically significant difference. This analysis concluded that the "repetition error" ($p=0.0002$), the "$<$unk$>$ error" ($p=0.0001$) and the "Insertion error" ($p=0.0001$) are indeed less frequent 
% in BPE than 66\% TED.

% % \textcolor{red}{Tiraria esta análise estatística pois não ficou claro o que quer mostrar e a importância disso para o problema.}

% \section{Conclusion}

% This paper focused on dealing with low-resource NMT scenarios, considering low and medium complexity Portuguese-English datasets (TED and Tatoeba). It experimentally evaluated the impact of transfer learning (pre-trained word embeddings), subword embeddings (BPE), and Back-Translation strategies (using the same and different domains data) over BLEU performance. In addition, this work presented a qualitative analysis conducted by a human translator over the outcomes of some best performing models, considering a specifically designed multidimensional evaluation criteria, for a sample constituted by a total of $40$ sentences, equally stratified in four CEFR levels. 
 
%  The BPE was the most effective technique for dealing with a low-resource setting, attaining the highest BLEU values and the lower error rates in six from eight error categories defined by the qualitative analysis. 
%  Same domain data augmentation has also led to exciting results when synthesising only a small portion of the original training set ($16.6\%$). 
  
%   Future works include evaluating models exploiting both BPE and BT and considering more sentences, as well as  CEFR levels, in the qualitative analysis,  possibly bringing a clearer view of error patterns and enlighting the practical effects of each strategy in objective and subjective translation quality aspects.
  
\end{document}
%% 
%%
%% End of file `example.tex'.
