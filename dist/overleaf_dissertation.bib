%
% This is file `coppe.bib'.
%
% Bibliographic references for the documentation.
%
% Copyright (C) 2011 CoppeTeX Project and any individual authors listed
% elsewhere in this file.
%
% This program is free software; you can redistribute it and/or modify
% it under the terms of the GNU General Public License version 3 as
% published by the Free Software Foundation.
%
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
% GNU General Public License version 3 for more details.
%
% You should have received a copy of the GNU General Public License
% version 3 along with this package (see COPYING file).
% If not, see <http://www.gnu.org/licenses/>.
%
% $URL: https://coppetex.svn.sourceforge.net/svnroot/coppetex/trunk/coppe.bib $
% $Id: coppe.bib 118 2008-10-18 14:17:06Z helano $
%
% Author(s): Vicente H. F. Batista
%            George O. Ainsworth Jr.
%
f
@article{ROUSSEEUW198753,
title = {Silhouettes: A graphical aid to the interpretation and validation of cluster analysis},
journal = {Journal of Computational and Applied Mathematics},
volume = {20},
pages = {53-65},
year = {1987},
issn = {0377-0427},
doi = {https://doi.org/10.1016/0377-0427(87)90125-7},
url = {https://www.sciencedirect.com/science/article/pii/0377042787901257},
author = {Peter J. Rousseeuw},
keywords = {Graphical display, cluster analysis, clustering validity, classification},
abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate’ number of clusters.}
}

@article{Greenacre1987TheGI,
  title={The Geometric Interpretation of Correspondence Analysis},
  author={Michael J. Greenacre and Trevor J. Hastie},
  journal={Journal of the American Statistical Association},
  year={1987},
  volume={82},
  pages={437-447}
}

@inproceedings{Hutchins1995MachineTA,
  title={Machine Translation: A Brief History},
  author={William J. Hutchins},
  year={1995}
}

@article{knight-1999-decoding,
    title = "Decoding complexity in word-replacement translation models",
    author = "Knight, Kevin",
    journal = "Computational Linguistics",
    volume = "25",
    number = "4",
    year = "1999",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J99-4005",
    pages = "607--615",
}

@INPROCEEDINGS{Loper02nltk:the,
    author = {Edward Loper and Steven Bird},
    title = {NLTK: The Natural Language Toolkit},
    booktitle = {Proc. of the ACL Workshop on Effective Tools for Teaching Natural Language Processing.},
    year = {2002}
}

@misc{philipp-koehn-europarl, 
    title={Europarl: A Parallel Corpus for Statistical Machine Translation}, 
    url={http://www.statmt.org/europarl/v10/}, 
    journal={Index of /europarl/v10}, 
    author={Philipp Koehn}
}

@inproceedings{Moseley2006OutOT,
  title={Out of the Tar Pit},
  author={Ben Moseley and Peter Marks},
  year={2006}
}

@book{multivariate-izenman,
author = {Izenman, Alan Julian},
title = {Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning},
year = {2008},
isbn = {0387781889},
publisher = {Springer Publishing Company, Incorporated},
url = {https://ce.aut.ac.ir/~shiry/lecture/Advanced%20Machine%20Learning/Manifold_Modern_Multivariate%20Statistical%20Techniques%20-%20Regres.pdf},
edition = {1},
abstract = {Remarkable advances in computation and data storage and the ready availability of huge data sets have been the keys to the growth of the new disciplines of data mining and machine learning, while the enormous success of the Human Genome Project has opened up the field of bioinformatics. These exciting developments, which led to the introduction of many innovative statistical tools for high-dimensional data analysis, are described here in detail. The author takes a broad perspective; for the first time in a book on multivariate analysis, nonlinear methods are discussed in detail as well as linear methods. Techniques covered range from traditional multivariate methods, such as multiple regression, principal components, canonical variates, linear discriminant analysis, factor analysis, clustering, multidimensional scaling, and correspondence analysis, to the newer methods of density estimation, projection pursuit, neural networks, multivariate reduced-rank regression, nonlinear manifold learning, bagging, boosting, random forests, independent component analysis, support vector machines, and classification and regression trees. Another unique feature of this book is the discussion of database management systems. This book is appropriate for advanced undergraduate students, graduate students, and researchers in statistics, computer science, artificial intelligence, psychology, cognitive sciences, business, medicine, bioinformatics, and engineering. Familiarity with multivariable calculus, linear algebra, and probability and statistics is required. The book presents a carefully-integrated mixture of theory and applications, and of classical and modern multivariate statistical techniques, including Bayesian methods. There are over 60 interesting data sets used as examples in the book, over 200 exercises, and many color illustrations and photographs. }
}

@book{TheodoridisKonstantinos2009,
  added-at = {2011-09-01T13:26:03.000+0200},
  author = {Theodoridis, Sergios and Koutroumbas, Konstantinos},
  biburl = {https://www.bibsonomy.org/bibtex/284b7e7aa970c54eebb504c44f3e29535/procomun},
  day = 24,
  howpublished = {Hardcover},
  interhash = {87b8fae1a542d79f5ef88ec74d573c18},
  intrahash = {84b7e7aa970c54eebb504c44f3e29535},
  isbn = {9781597492720},
  keywords = {pattern\_recognition},
  publisher = {{Academic Press}},
  timestamp = {2011-09-02T08:25:25.000+0200},
  title = {{Pattern Recognition, Fourth Edition}},
  year = 2009
}

@misc{gigaword,
author = {Parker, R. and Graff, D. and Kong, J. and Chen, K. and Maeda, K.},
title = {English Gigaword Fifth Edition},
eprint={LDC2011T07},
publisher = {Linguistic Data Consortium},
year = {2011},
month = {07},
format = {DVD},
}

@article{cettolo-ted-talks,
author = {Cettolo, Mauro and Girardi, C. and Federico, Marcello},
year = {2012},
month = {01},
pages = {261-268},
title = {Wit3: Web inventory of transcribed and translated talks},
journal = {Proceedings of EAMT}
}

@article{DBLP:journals/corr/abs-1211-3711,
  author    = {Alex Graves},
  title     = {Sequence Transduction with Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1211.3711},
  year      = {2012},
  url       = {http://arxiv.org/abs/1211.3711},
  eprinttype = {arXiv},
  eprint    = {1211.3711},
  timestamp = {Mon, 13 Aug 2018 16:48:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1211-3711.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{TIEDEMANN12.463,
  author = {Jörg Tiedemann},
  title = {Parallel Data, Tools and Interfaces in OPUS},
  booktitle = {Proc. of the Eight International Conf. on Language Resources and Evaluation (LREC'12)},
  year = {2012},
  month = {may},
  date = {23-25},
  address = {Istanbul, Turkey},
  publisher = {European Language Resources Assoc. (ELRA)},
  isbn = {978-2-9517408-7-7},
  language = {english}
 }

@misc{mikolov2013efficient,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{JMLR:v15:srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@article{DBLP:journals/corr/SutskeverVL14,
  author    = {Ilya Sutskever and
               Oriol Vinyals and
               Quoc V. Le},
  title     = {Sequence to Sequence Learning with Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1409.3215},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.3215},
  eprinttype = {arXiv},
  eprint    = {1409.3215},
  timestamp = {Mon, 13 Aug 2018 16:48:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SutskeverVL14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@article{DBLP:journals/corr/Rong14,
  author    = {Xin Rong},
  title     = {word2vec Parameter Learning Explained},
  journal   = {CoRR},
  volume    = {abs/1411.2738},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.2738},
  eprinttype = {arXiv},
  eprint    = {1411.2738},
  timestamp = {Mon, 13 Aug 2018 16:45:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Rong14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@article{DBLP:journals/corr/SennrichHB15,
  author    = {Rico Sennrich and
               Barry Haddow and
               Alexandra Birch},
  title     = {Neural Machine Translation of Rare Words with Subword Units},
  journal   = {CoRR},
  volume    = {abs/1508.07909},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.07909},
  archivePrefix = {arXiv},
  eprint    = {1508.07909},
  timestamp = {Mon, 13 Aug 2018 16:47:17 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SennrichHB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SennrichHB15a,
  author    = {Rico Sennrich and
               Barry Haddow and
               Alexandra Birch},
  title     = {Improving Neural Machine Translation Models with Monolingual Data},
  journal   = {CoRR},
  volume    = {abs/1511.06709},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06709},
  archivePrefix = {arXiv},
  eprint    = {1511.06709},
  timestamp = {Mon, 13 Aug 2018 16:47:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SennrichHB15a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{inproceedings,
author = {Curto, Pedro and Mamede, Nuno and Baptista, Jorge},
year = {2015},
month = {01},
pages = {36-44},
title = {Automatic Text Difficulty Classifier - Assisting the Selection Of Adequate Reading Materials For European Portuguese Teaching},
doi = {10.5220/0005428300360044}
}

@inproceedings{luong-etal-2015-effective,
    title = "Effective Approaches to Attention-based Neural Machine Translation",
    author = "Luong, Thang  and
      Pham, Hieu  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1166",
    doi = "10.18653/v1/D15-1166",
    pages = "1412--1421",
}

@inproceedings{ranzato-sequence,
author = {Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
year = {2016},
month = {01},
pages = {},
title = {Sequence Level Training with Recurrent Neural Networks}
}

@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{DBLP:journals/corr/LuongM16,
  author    = {Minh{-}Thang Luong and
               Christopher D. Manning},
  title     = {Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character
               Models},
  journal   = {CoRR},
  volume    = {abs/1604.00788},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.00788},
  archivePrefix = {arXiv},
  eprint    = {1604.00788},
  timestamp = {Mon, 13 Aug 2018 16:47:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LuongM16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/BojanowskiGJM16,
  author    = {Piotr Bojanowski and
               Edouard Grave and
               Armand Joulin and
               Tom{\'{a}}s Mikolov},
  title     = {Enriching Word Vectors with Subword Information},
  journal   = {CoRR},
  volume    = {abs/1607.04606},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.04606},
  archivePrefix = {arXiv},
  eprint    = {1607.04606},
  timestamp = {Mon, 28 Dec 2020 11:31:02 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/BojanowskiGJM16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{shen-etal-2016-minimum,
    title = "Minimum Risk Training for Neural Machine Translation",
    author = "Shen, Shiqi  and
      Cheng, Yong  and
      He, Zhongjun  and
      He, Wei  and
      Wu, Hua  and
      Sun, Maosong  and
      Liu, Yang",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1159",
    doi = "10.18653/v1/P16-1159",
    pages = "1683--1692",
}

@inproceedings{shi-etal-2016-string,
    title = "Does String-Based Neural {MT} Learn Source Syntax?",
    author = "Shi, Xing  and
      Padhi, Inkit  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1159",
    doi = "10.18653/v1/D16-1159",
    pages = "1526--1534",
}

@article{DBLP:journals/corr/WuSCLNMKCGMKSJL16,
  author    = {Yonghui Wu and
               Mike Schuster and
               Zhifeng Chen and
               Quoc V. Le and
               Mohammad Norouzi and
               Wolfgang Macherey and
               Maxim Krikun and
               Yuan Cao and
               Qin Gao and
               Klaus Macherey and
               Jeff Klingner and
               Apurva Shah and
               Melvin Johnson and
               Xiaobing Liu and
               Lukasz Kaiser and
               Stephan Gouws and
               Yoshikiyo Kato and
               Taku Kudo and
               Hideto Kazawa and
               Keith Stevens and
               George Kurian and
               Nishant Patil and
               Wei Wang and
               Cliff Young and
               Jason Smith and
               Jason Riesa and
               Alex Rudnick and
               Oriol Vinyals and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  title     = {Google's Neural Machine Translation System: Bridging the Gap between
               Human and Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1609.08144},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.08144},
  eprinttype = {arXiv},
  eprint    = {1609.08144},
  timestamp = {Thu, 14 Jan 2021 12:12:19 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/WuSCLNMKCGMKSJL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/JohnsonSLKWCTVW16,
  author    = {Melvin Johnson and
               Mike Schuster and
               Quoc V. Le and
               Maxim Krikun and
               Yonghui Wu and
               Zhifeng Chen and
               Nikhil Thorat and
               Fernanda B. Vi{\'{e}}gas and
               Martin Wattenberg and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  title     = {Google's Multilingual Neural Machine Translation System: Enabling
               Zero-Shot Translation},
  journal   = {CoRR},
  volume    = {abs/1611.04558},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.04558},
  eprinttype = {arXiv},
  eprint    = {1611.04558},
  timestamp = {Mon, 13 Aug 2018 16:46:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/JohnsonSLKWCTVW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zoph-etal-2016-transfer,
    title = "Transfer Learning for Low-Resource Neural Machine Translation",
    author = "Zoph, Barret  and
      Yuret, Deniz  and
      May, Jonathan  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1163",
    doi = "10.18653/v1/D16-1163",
    pages = "1568--1575",
}

@incollection{ROY2017361,
    title = {Chapter 11 - A novel graph clustering algorithm based on discrete-time quantum random walk},
    editor = {Siddhartha Bhattacharyya and Ujjwal Maulik and Paramartha Dutta},
    booktitle = {Quantum Inspired Computational Intelligence},
    publisher = {Morgan Kaufmann},
    address = {Boston},
    pages = {361-389},
    year = {2017},
    isbn = {978-0-12-804409-4},
    doi = {https://doi.org/10.1016/B978-0-12-804409-4.00011-5},
    url = {https://www.sciencedirect.com/science/article/pii/B9780128044094000115},
    author = {S.G. Roy and A. Chakrabarti},
    keywords = {Discrete-time quantum random walk, Continuous-time quantum random walk, Clustering, Quantum clustering, Quantum date, Quantum Circuit, -dimensional tree},
    abstract = {The clustering activity is an unsupervised learning observation which coalesces the data into segments. Grouping of data is done by identification of common characteristics that are labeled as similarities among data on the basis of their characteristics. Graph clustering is a tool needed in many computer applications, such as network routing, analysis of social networks, computer vision, and VLSI physical design. This chapter explains how quantum random walk helps in graph-based clustering, and we propose a new quantum clustering algorithm. The proposed quantum clustering algorithm is based on the discrete-time quantum random walk, which finds the clusters from a given adjacency matrix of a graph. We give a quantum circuit model and Quantum Computing Language-based simulation of our algorithm and illustrate its faster rate of convergence. Simulation results for experimental graphs illustrate that our proposed algorithm shows an exponential speedup over existing classical algorithms.}
}

@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{britz2017massive,
      title={Massive Exploration of Neural Machine Translation Architectures}, 
      author={Denny Britz and Anna Goldie and Minh-Thang Luong and Quoc Le},
      year={2017},
      eprint={1703.03906},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{gulcere-language-model-nmt,
author = {Gulcehre, Caglar and Firat, Orhan and Xu, Kelvin and Cho, Kyunghyun and Bengio, Y.},
year = {2017},
month = {03},
pages = {},
title = {On Integrating a Language Model Into Neural Machine Translation},
volume = {45},
journal = {Computer Speech and Language},
doi = {10.1016/j.csl.2017.01.014}
}

@misc{nmt_nlp_speech_draft_book, 
title={Speech and Language Processing (3rd ed. draft)}, 
author={Dan Jurafsky and James H. Martin},
howpublished={\url{https://web.stanford.edu/~jurafsky/slp3/}}, 
journal={Common European Framework of Reference for Languages (CEFR)},
note = {Accessed: 2021-10-14},
year = {2021}
}

@misc{cs224n_stanford_l2, 
title={Lecture 2: Word Vectors, Word Senses, and Neural Classifiers. CS224n: Natural Language Processing with Deep Learning}, 
author={Chris Manning},
howpublished={\url{http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture02-wordvecs2.pdf}}, 
journal={Stanford University},
year = {2021},
note = {Accessed: 2021-10-14}
}

@misc{cs224n_stanford_l7, 
title={Lecture 7: Machine Translation, Sequence-to-Sequence and Attention. CS224n: Natural Language Processing with Deep Learning}, 
author={Chris Manning},
howpublished={\url{http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture07-nmt.pdf}}, 
journal={Stanford University},
year = {2021},
note = {Accessed: 2021-11-25}
}

@inproceedings{calixto-etal-2017-doubly,
    title = "Doubly-Attentive Decoder for Multi-modal Neural Machine Translation",
    author = "Calixto, Iacer  and
      Liu, Qun  and
      Campbell, Nick",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1175",
    doi = "10.18653/v1/P17-1175",
    pages = "1913--1924",
    abstract = "We introduce a Multi-modal Neural Machine Translation model in which a doubly-attentive decoder naturally incorporates spatial visual features obtained using pre-trained convolutional neural networks, bridging the gap between image description and translation. Our decoder learns to attend to source-language words and parts of an image independently by means of two separate attention mechanisms as it generates words in the target language. We find that our model can efficiently exploit not just back-translated in-domain multi-modal data but also large general-domain text-only MT corpora. We also report state-of-the-art results on the Multi30k data set.",
}

@inproceedings{fares-etal-2017-word,
    title = "Word vectors, reuse, and replicability: Towards a community repository of large-text resources",
    author = "Fares, Murhaf  and
      Kutuzov, Andrey  and
      Oepen, Stephan  and
      Velldal, Erik",
    booktitle = "Proceedings of the 21st Nordic Conference on Computational Linguistics",
    month = may,
    year = "2017",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-0237",
    pages = "271--276",
}


@article{DBLP:journals/corr/abs-1708-06025,
  author    = {Nathan Hartmann and
               Erick R. Fonseca and
               Christopher Shulby and
               Marcos Vin{\'{\i}}cius Treviso and
               J{\'{e}}ssica S. Rodrigues and
               Sandra M. Alu{\'{\i}}sio},
  title     = {Portuguese Word Embeddings: Evaluating on Word Analogies and Natural
               Language Tasks},
  journal   = {CoRR},
  volume    = {abs/1708.06025},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.06025},
  archivePrefix = {arXiv},
  eprint    = {1708.06025},
  timestamp = {Sat, 28 Sep 2019 23:15:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-06025.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1812-08621,
  author    = {Miguel Domingo and
               Mercedes Garc{\'{\i}}a{-}Mart{\'{\i}}nez and
               Alexandre Helle and
               Francisco Casacuberta and
               Manuel Herranz},
  title     = {How Much Does Tokenization Affect Neural Machine Translation?},
  journal   = {CoRR},
  volume    = {abs/1812.08621},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.08621},
  archivePrefix = {arXiv},
  eprint    = {1812.08621},
  timestamp = {Wed, 02 Jan 2019 14:40:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-08621.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nguyen-chiang-2018-improving,
    title = "Improving Lexical Choice in Neural Machine Translation",
    author = "Nguyen, Toan  and
      Chiang, David",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1031",
    doi = "10.18653/v1/N18-1031",
    pages = "334--343",
    abstract = "We explore two solutions to the problem of mistranslating rare words in neural machine translation. First, we argue that the standard output layer, which computes the inner product of a vector representing the context with all possible output word embeddings, rewards frequent words disproportionately, and we propose to fix the norms of both vectors to a constant value. Second, we integrate a simple lexical module which is jointly trained with the rest of the model. We evaluate our approaches on eight language pairs with data sizes ranging from 100k to 8M words, and achieve improvements of up to +4.3 BLEU, surpassing phrase-based translation in nearly all settings.",
}

@inproceedings{chen-etal-2018-combining,
    title = "Combining Character and Word Information in Neural Machine Translation Using a Multi-Level Attention",
    author = "Chen, Huadong  and
      Huang, Shujian  and
      Chiang, David  and
      Dai, Xinyu  and
      Chen, Jiajun",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1116",
    doi = "10.18653/v1/N18-1116",
    pages = "1284--1293",
    abstract = "Natural language sentences, being hierarchical, can be represented at different levels of granularity, like words, subwords, or characters. But most neural machine translation systems require the sentence to be represented as a sequence at a single level of granularity. It can be difficult to determine which granularity is better for a particular translation task. In this paper, we improve the model by incorporating multiple levels of granularity. Specifically, we propose (1) an encoder with character attention which augments the (sub)word-level representation with character-level information; (2) a decoder with multiple attentions that enable the representations from different levels of granularity to control the translation cooperatively. Experiments on three translation tasks demonstrate that our proposed models outperform the standard word-based model, the subword-based model, and a strong character-based model.",
}

@article{DBLP:journals/corr/abs-1811-01064,
  author    = {Surafel Melaku Lakew and
               Aliia Erofeeva and
               Marcello Federico},
  title     = {Neural Machine Translation into Language Varieties},
  journal   = {CoRR},
  volume    = {abs/1811.01064},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.01064},
  archivePrefix = {arXiv},
  eprint    = {1811.01064},
  timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-01064.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1803-00047,
  author    = {Myle Ott and
               Michael Auli and
               David Grangier and
               Marc'Aurelio Ranzato},
  title     = {Analyzing Uncertainty in Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1803.00047},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.00047},
  archivePrefix = {arXiv},
  eprint    = {1803.00047},
  timestamp = {Mon, 13 Aug 2018 16:47:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-00047.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1804-06189,
  author    = {Alberto Poncelas and
               Dimitar Sht. Shterionov and
               Andy Way and
               Gideon Maillette de Buy Wenniger and
               Peyman Passban},
  title     = {Investigating Backtranslation in Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1804.06189},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.06189},
  archivePrefix = {arXiv},
  eprint    = {1804.06189},
  timestamp = {Fri, 25 Sep 2020 16:19:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-06189.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{post-2018-call,
  title = "A Call for Clarity in Reporting {BLEU} Scores",
  author = "Post, Matt",
  booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
  month = oct,
  year = "2018",
  address = "Belgium, Brussels",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/W18-6319",
  pages = "186--191",
}

@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wu-etal-2018-beyond,
    title = "Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter",
    author = "Wu, Lijun  and
      Tan, Xu  and
      He, Di  and
      Tian, Fei  and
      Qin, Tao  and
      Lai, Jianhuang  and
      Liu, Tie-Yan",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1396",
    doi = "10.18653/v1/D18-1396",
    pages = "3602--3611",
    abstract = "Neural machine translation usually adopts autoregressive models and suffers from exposure bias as well as the consequent error propagation problem. Many previous works have discussed the relationship between error propagation and the \textit{accuracy drop} (i.e., the left part of the translated sentence is often better than its right part in left-to-right decoding models) problem. In this paper, we conduct a series of analyses to deeply understand this problem and get several interesting findings. (1) The role of error propagation on accuracy drop is overstated in the literature, although it indeed contributes to the accuracy drop problem. (2) Characteristics of a language play a more important role in causing the accuracy drop: the left part of the translation result in a right-branching language (e.g., English) is more likely to be more accurate than its right part, while the right part is more accurate for a left-branching language (e.g., Japanese). Our discoveries are confirmed on different model structures including Transformer and RNN, and in other sequence generation tasks such as text summarization.",
}

@article{DBLP:journals/corr/abs-1808-03314,
  author    = {Alex Sherstinsky},
  title     = {Fundamentals of Recurrent Neural Network {(RNN)} and Long Short-Term
               Memory {(LSTM)} Network},
  journal   = {CoRR},
  volume    = {abs/1808.03314},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.03314},
  archivePrefix = {arXiv},
  eprint    = {1808.03314},
  timestamp = {Sun, 02 Sep 2018 15:01:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-03314.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1808-06226,
  author    = {Taku Kudo and
               John Richardson},
  title     = {SentencePiece: {A} simple and language independent subword tokenizer
               and detokenizer for Neural Text Processing},
  journal   = {CoRR},
  volume    = {abs/1808.06226},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.06226},
  archivePrefix = {arXiv},
  eprint    = {1808.06226},
  timestamp = {Sun, 02 Sep 2018 15:01:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-06226.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{qi-etal-2018-pre,
    title = "When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?",
    author = "Qi, Ye  and
      Sachan, Devendra  and
      Felix, Matthieu  and
      Padmanabhan, Sarguna  and
      Neubig, Graham",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-2084",
    doi = "10.18653/v1/N18-2084",
    pages = "529--535",
    abstract = "The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases {--} providing gains of up to 20 BLEU points in the most favorable setting.",
}

@article{DBLP:journals/corr/abs-1907-01752,
  author    = {Leshem Choshen and
               Lior Fox and
               Zohar Aizenbud and
               Omri Abend},
  title     = {On the Weaknesses of Reinforcement Learning for Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1907.01752},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.01752},
  archivePrefix = {arXiv},
  eprint    = {1907.01752},
  timestamp = {Mon, 08 Jul 2019 14:12:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-01752.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1905-09418,
  author    = {Elena Voita and
               David Talbot and
               Fedor Moiseev and
               Rico Sennrich and
               Ivan Titov},
  title     = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy
               Lifting, the Rest Can Be Pruned},
  journal   = {CoRR},
  volume    = {abs/1905.09418},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.09418},
  archivePrefix = {arXiv},
  eprint    = {1905.09418},
  timestamp = {Wed, 29 May 2019 11:27:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-09418.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hu2019texar,
  title={Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation},
  author={Hu, Zhiting and Shi, Haoran and Tan, Bowen and Wang, Wentao and Yang, Zichao and Zhao, Tiancheng and He, Junxian and Qin, Lianhui and Wang, Di and others},
  booktitle={ACL 2019, System Demonstrations},
  year={2019}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Proc. Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{DBLP:journals/corr/abs-1906-03785,
  author    = {Mengzhou Xia and
               Xiang Kong and
               Antonios Anastasopoulos and
               Graham Neubig},
  title     = {Generalized Data Augmentation for Low-Resource Translation},
  journal   = {CoRR},
  volume    = {abs/1906.03785},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.03785},
  archivePrefix = {arXiv},
  eprint    = {1906.03785},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-03785.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sennrich-zhang-2019-revisiting,
    title = "Revisiting Low-Resource Neural Machine Translation: A Case Study",
    author = "Sennrich, Rico  and
      Zhang, Biao",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1021",
    doi = "10.18653/v1/P19-1021",
    pages = "211--221",
    abstract = "It has been shown that the performance of neural machine translation (NMT) drops starkly in low-resource conditions, underperforming phrase-based statistical machine translation (PBSMT) and requiring large amounts of auxiliary data to achieve competitive results. In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. We discuss some pitfalls to be aware of when training low-resource NMT systems, and recent techniques that have shown to be especially helpful in low-resource settings, resulting in a set of best practices for low-resource NMT. In our experiments on German{--}English with different amounts of IWSLT14 training data, we show that, without the use of any auxiliary monolingual or multilingual data, an optimized NMT system can outperform PBSMT with far less data than previously claimed. We also apply these techniques to a low-resource Korean{--}English dataset, surpassing previously reported results by 4 BLEU.",
}

@article{DBLP:journals/corr/abs-1904-00639,
  author    = {Tosho Hirasawa and
               Hayahide Yamagishi and
               Yukio Matsumura and
               Mamoru Komachi},
  title     = {Multimodal Machine Translation with Embedding Prediction},
  journal   = {CoRR},
  volume    = {abs/1904.00639},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.00639},
  archivePrefix = {arXiv},
  eprint    = {1904.00639},
  timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-00639.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{eisenstein2019introduction,
  title={Introduction to Natural Language Processing},
  author={Eisenstein, J.},
  isbn={9780262042840},
  lccn={2018059552},
  series={Adaptive Computation and Machine Learning series},
  url={https://books.google.com.br/books?id=72yuDwAAQBAJ},
  address = "Florence, Italy",
  year={2019},
  publisher={MIT Press}
}


@article{DBLP:journals/corr/abs-1911-12385,
  author    = {Sachin Mehta and
               Rik Koncel{-}Kedziorski and
               Mohammad Rastegari and
               Hannaneh Hajishirzi},
  title     = {DeFINE: DEep Factorized INput Word Embeddings for Neural Sequence
               Modeling},
  journal   = {CoRR},
  volume    = {abs/1911.12385},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.12385},
  archivePrefix = {arXiv},
  eprint    = {1911.12385},
  timestamp = {Wed, 08 Jan 2020 15:28:22 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-12385.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cui-etal-2019-mixed,
    title = "Mixed Multi-Head Self-Attention for Neural Machine Translation",
    author = "Cui, Hongyi  and
      Iida, Shohei  and
      Hung, Po-Hsuan  and
      Utsuro, Takehito  and
      Nagata, Masaaki",
    booktitle = "Proceedings of the 3rd Workshop on Neural Generation and Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-5622",
    doi = "10.18653/v1/D19-5622",
    pages = "206--214",
    abstract = "Recently, the Transformer becomes a state-of-the-art architecture in the filed of neural machine translation (NMT). A key point of its high-performance is the multi-head self-attention which is supposed to allow the model to independently attend to information from different representation subspaces. However, there is no explicit mechanism to ensure that different attention heads indeed capture different features, and in practice, redundancy has occurred in multiple heads. In this paper, we argue that using the same global attention in multiple heads limits multi-head self-attention{'}s capacity for learning distinct features. In order to improve the expressiveness of multi-head self-attention, we propose a novel Mixed Multi-Head Self-Attention (MMA) which models not only global and local attention but also forward and backward attention in different attention heads. This enables the model to learn distinct representations explicitly among multiple heads. In our experiments on both WAT17 English-Japanese as well as IWSLT14 German-English translation task, we show that, without increasing the number of parameters, our models yield consistent and significant improvements (0.9 BLEU scores on average) over the strong Transformer baseline.",
}

@inproceedings{Caseli2020NMTAP,
  title={NMT and PBSMT Error Analyses in English to Brazilian Portuguese Automatic Translations},
  author={Helena de Medeiros Caseli and Marcio Lima In{\'a}cio},
  booktitle={LREC},
  year={2020}
}


@misc{quach_2020, 
    title={AI me to the Moon... Carbon footprint for 'training GPT-3' same as driving to our natural satellite and back}, 
    url={https://www.theregister.com/2020/11/04/gpt3_carbon_footprint_estimate/}, 
    journal={The Register® - Biting the hand that feeds IT}, 
    publisher={The Register}, 
    author={Quach, Katyanna}, 
    year={2020}, 
    note = {Accessed: 2022-02-07},
    month={Nov}
}

@article{DBLP:journals/corr/abs-2006-10270,
  author    = {Yang Fan and
               Shufang Xie and
               Yingce Xia and
               Lijun Wu and
               Tao Qin and
               Xiang{-}Yang Li and
               Tie{-}Yan Liu},
  title     = {Multi-branch Attentive Transformer},
  journal   = {CoRR},
  volume    = {abs/2006.10270},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.10270},
  archivePrefix = {arXiv},
  eprint    = {2006.10270},
  timestamp = {Wed, 02 Sep 2020 19:12:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-10270.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2010-06354,
  author    = {J{\"{o}}rg Tiedemann},
  title     = {The Tatoeba Translation Challenge - Realistic Data Sets for Low Resource
               and Multilingual {MT}},
  journal   = {CoRR},
  volume    = {abs/2010.06354},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.06354},
  archivePrefix = {arXiv},
  eprint    = {2010.06354},
  timestamp = {Tue, 20 Oct 2020 15:08:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-06354.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2006-15595,
  author    = {Guolin Ke and
               Di He and
               Tie{-}Yan Liu},
  title     = {Rethinking Positional Encoding in Language Pre-training},
  journal   = {CoRR},
  volume    = {abs/2006.15595},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.15595},
  eprinttype = {arXiv},
  eprint    = {2006.15595},
  timestamp = {Wed, 01 Jul 2020 15:21:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-15595.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1808-03314,
  author    = {Alex Sherstinsky},
  title     = {Fundamentals of Recurrent Neural Network {(RNN)} and Long Short-Term
               Memory {(LSTM)} Network},
  journal   = {CoRR},
  volume    = {abs/1808.03314},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.03314},
  eprinttype = {arXiv},
  eprint    = {1808.03314},
  timestamp = {Sun, 02 Sep 2018 15:01:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-03314.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bostrom-durrett-2020-byte,
    title = "Byte Pair Encoding is Suboptimal for Language Model Pretraining",
    author = "Bostrom, Kaj  and
      Durrett, Greg",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.414",
    doi = "10.18653/v1/2020.findings-emnlp.414",
    pages = "4617--4624",
    abstract = "The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. We analyze differences between BPE and unigram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE{'}s greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks and two languages (English and Japanese), we find that the unigram LM tokenization method matches or outperforms BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE.",
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{DBLP:journals/corr/abs-2012-15515,
  author    = {Zhixing Tan and
               Shuo Wang and
               Zonghan Yang and
               Gang Chen and
               Xuancheng Huang and
               Maosong Sun and
               Yang Liu},
  title     = {Neural Machine Translation: {A} Review of Methods, Resources, and
               Tools},
  journal   = {CoRR},
  volume    = {abs/2012.15515},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.15515},
  eprinttype = {arXiv},
  eprint    = {2012.15515},
  timestamp = {Tue, 18 May 2021 15:19:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-15515.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{mehta2021delight,
      title={DeLighT: Deep and Light-weight Transformer}, 
      author={Sachin Mehta and Marjan Ghazvininejad and Srinivasan Iyer and Luke Zettlemoyer and Hannaneh Hajishirzi},
      year={2021},
      eprint={2008.00623},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{google-ai-2020, 
title={Recent advances in google translate}, 
author={Isaac Caswell and Bowen Liang},
howpublished={\url{https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html}}, 
journal={Google AI Blog},
note = {Accessed: 2022-02-03},
year = {2020}
}

@misc{COE, 
title={The \mbox{CEFR} Levels - Council of Europe (COE)}, 
author={Europe Council},
howpublished={\url{https://tinyurl.com/cefrlcoe}}, 
journal={Common European Framework of Reference for Languages (CEFR)},
note = {Accessed: 2021-08-12},
year = {2021}
}
https://cloud.google.com/translate

@misc{gtranslate_api, 
title={Translation AI}, 
author={Google},
howpublished={\url{https://cloud.google.com/translate}}, 
note = {Accessed: 2022-06-02},
year = {2022}
}

@INPROCEEDINGS{portuguese-nmt,
    AUTHOR="Arthur Estrella and João Baptista de Oliveira e Souza Filho",
    TITLE="Tackling neural machine translation in low-resource settings: a Portuguese case study",
    BOOKTITLE="STIL 2021 () ",
    ADDRESS="",
    DAYS="29-3",
    MONTH="nov",
    YEAR="2021",
    ABSTRACT="Neural machine translation (NMT) is requiring an increasing amount of data and computational power, so succeeding in this task with limited data and using a single GPU might be challenging. Strategies such as pre-trained word embeddings, subword embeddings and data augmentation can potentially address some low-resource issues, but their impact on the quality of translations is unclear. This work evaluates such techniques on low-resource experiments beyond just reporting BLEU: errors are categorized on the Portuguese-English pair with the help of a translator, considering semantic and syntactic aspects. The BPE subword Transformer allows an BLEU increase of 59\% p.p. compared to the standard Transformer.",
    KEYWORDS="Machine Translation; Statistical and Corpus-Based Natural Language Processing",
    URL="https://sol.sbc.org.br/index.php/stil/article/view/17807"
}

@book{pml1murphy,
 author = "Kevin P. Murphy",
 title = "Probabilistic Machine Learning: An introduction",
 publisher = "MIT Press",
 year = 2022,
 url = "probml.ai"
}

@misc{ncss_hierarchical, 
    title={Hierarchical Clustering / Dendrograms}, 
    url={https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Hierarchical_Clustering-Dendrograms.pdf}, 
    journal={NCSS Statistical Software - Chapter 430}, 
    publisher={NCSS Statistical Software}, 
    author={NCSS}, 
    year={2022}, 
    note = {Accessed: 2022-03-23},
    month={Jan}
}

@misc{ncss_correspondence, 
    title={Correspondence Analysis}, 
    url={https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Correspondence_Analysis.pdf}, 
    journal={NCSS Statistical Software - Chapter 430}, 
    publisher={NCSS Statistical Software}, 
    author={NCSS}, 
    year={2022}, 
    note = {Accessed: 2022-03-07},
    month={Jan}
}

@misc{google-ai-2022, 
title={Google Research: Themes from 2021 and Beyond}, 
author={Jeff Dean},
howpublished={\url{https://ai.googleblog.com/2022/01/google-research-themes-from-2021-and.html?m=1#Trend2}}, 
journal={Google AI Blog},
note = {Accessed: 2022-02-21},
year = {2022}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% @BOOK{book-example,
%   author = "R. Abraham and J. E. Marsden and T. Ratiu",
%   title = "Manifolds, Tensor Analysis, and Applications",
%   edition = "2",
%   address = "New York",
%   publisher = "Springer-Verlag",
%   year = "1988",
% }

% @ARTICLE{article-example,
%   author = "D. Iesan",
%   title = {Existence Theorems in the Theory of Mixtures},
%   journal = {Journal of Elasticity},
%   volume = {42},
%   number = {2},
%   pages = {145--163},
%   month = feb,
%   year = 1996,
% }

% @TECHREPORT{techreport-exampleIn,
%   author = "D. A. Garret",
%   title = "The Microscopic Detection of Corrosion in Aluminum Aircraft Structures with Thermal Neutron Beams and Film Imaging Methods",
%   type = "In: Report",
%   number = "NBSIR 78-1434",
%   institution = "National Bureau of Standards",
%   address = "Washington, D.C.",
%   year = "1977",
% }

% @TECHREPORT{techreport-example,
%   author = "L. Maestrello",
%   title = "Two-Point Correlations of Sound Pressure in the Far Field of a Jet: Experiment",
%   type = "{NASA}",
%   number = "{TM}~{X}-72835",
%   year = 1976,
% }

% @INPROCEEDINGS{inproceedings-example,
%   author = "M. E. Gurtin",
%   title = "On the nonlinear theory of elasticity",
%   booktitle = "Proceedings of the International Symposium on Continuum Mechanics and Partial Differential Equations: Contemporary Developments in Continuum Mechanics and Partial Differential Equations",
%   pages = "237-253",
%   address = "Rio de Janeiro",
%   month = aug,
%   year = 1977,
% }


% @INCOLLECTION{incollection-example,
%   author = "S. C. Cowin",
%   title = "Adaptive Anisotropy: An Example in Living Bone",
%   booktitle = "Non-Classical Continuum Mechanics",
%   series = "London Mathematical Society Lecture Note Series",
%   volume = "122",
%   publisher = "Cambridge University Press",
%   pages = "174--186",
%   year = 1987,
% }

% @INBOOK{inbook-example,
%   author = "D. K. Edwards",
%   title = "Thermal Radiation Measurements",
%   address = "New York, USA",
%   booktitle = "Measurements in Heat Transfer",
%   publisher = "Hemisphere Publishing Corporation",
%   editor = "E. R. G. Eckert and R. J. Goldstein",
%   edition = 2,
%   year = "1976",
%   chapter = "10",
% }

% @MASTERSTHESIS{mastersthesis-example,
%   author = "A. Tuntomo",
%   title = "Transport Phenomena in a Small Particle with Internal Radiant Absorption",
%   school = "University of California at Berkeley",
%   type = "{Ph.D.} dissertation",
%   address = "Berkeley, California, USA",
%   year = 1990,
% }

% @PHDTHESIS{phdthesis-example,
%   author = "Paes~Junior, H. R.",
%   title = "Influ{\^e}ncia da Espessura da Camada Intr{\'i}nseca e Energia do Foton na Degrada{\c c}\~ao de C{\'e}lulas Solares de Sil{\'i}cio Amorfo Hidrogenado",
%   school = "COPPE/UFRJ",
%   address = "Rio de Janeiro, RJ, Brasil",
%   type = "Tese de {D.Sc.}",
%   year = 1994,
% }

